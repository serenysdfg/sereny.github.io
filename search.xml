<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[golang通过CMD来调用FFmpeg]]></title>
    <url>%2F2019%2F11%2F12%2F2019-11-12-golang_cmd_ffmpeg%2F</url>
    <content type="text"><![CDATA[概述在golang中调用FFmpeg，一种方法是使用cgo来使用FFmpeg，这样就可以在go程序里面编写转码语句，进行错误处理等， 但是这样的方法貌似有点复杂，或许得不偿失。 这里使用CMD来直接调用FFmpeg进行转码，在使用较多的复杂参数的情况下，可能反而简单许多。 使用记录调用： 123cmd := exec.Command(&quot;/bin/bash&quot;, &quot;-c&quot;, param)cmd.SysProcAttr = &amp;syscall.SysProcAttr&#123;Setpgid: true&#125;cmd.start() 这里的param就是整个FFmpeg转码语句，例如/usr/bin/ffmpeg -i 1.mp4 1.flv。 这里使用Setpgid来设置进程组pid，这样在后面kill时直接杀掉整个进程组，无残留。 另外cmd.start()表示非阻塞，转码开始后就放在后台（因为直播相关，所以会一直转码），等待结束命令。 转码日志记录： 12stdout, err := os.OpenFile(logFile, os.O_CREATE|os.O_WRONLY, 0644)cmd.Stderr = stdout 注意ffmpeg使用的日志输出是Stderr，err… 停止： 1err := syscall.Kill(-cmd.Process.Pid, syscall.SIGKILL) 加上一个负号，直接杀掉整个进程组。 完整代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344package mainimport ( "errors" "log" "os" "os/exec" "syscall")func Exec(param, logFile string) (*exec.Cmd, error) &#123; cmd := exec.Command("/bin/bash", "-c", param) cmd.SysProcAttr = &amp;syscall.SysProcAttr&#123;Setpgid: true&#125; stdout, err := os.OpenFile(logFile, os.O_CREATE|os.O_WRONLY, 0644) if err != nil &#123; return nil, err &#125; // ffmpeg 的日志输出用的是 stderr cmd.Stderr = stdout log.Info("Exce cmd: %s", param) if err := cmd.Start(); err != nil &#123; _ = stdout.Close() return nil, err &#125; return cmd, nil&#125;func KillCmd(cmd *exec.Cmd) error &#123; if cmd == nil || cmd.Process == nil || cmd.ProcessState == nil &#123; return errors.New("process not found or already stopped") &#125; log.Info("Kill cmd. pid: %d", cmd.Process.Pid) err := syscall.Kill(-cmd.Process.Pid, syscall.SIGKILL) if err != err &#123; return err &#125; return nil&#125; 参考Go语言中Kill子进程的正确姿势]]></content>
      <categories>
        <category>golang</category>
      </categories>
      <tags>
        <tag>live</tag>
        <tag>golang</tag>
        <tag>ffmpeg</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[srs的推拉流压测]]></title>
    <url>%2F2019%2F10%2F29%2F2019-10-29-srs_bench-v1%2F</url>
    <content type="text"><![CDATA[概述测试单个srs节点的同时推流或拉流的能力。 主机1： 云主机-上海 CPU：Intel(R) Xeon(R) Gold 6161 CPU @ 2.20GHz MEM：64G 主机2： 云主机-北京 CPU：Intel(R) Xeon(R) Gold 6161 CPU @ 2.20GHz MEM：64G 测试方法：使用srs自带的srs-bench进行测试。 带宽测试使用iperf3进行测试： 上海： 1234$ iperf3 -s -p 12345 -i 1-----------------------------------------------------------Server listening on 12345 北京： 123456789101112131415161718192021222324252627282930313233343536373839$ iperf3 -c xxx.xxx.xxx.219 -p12345 -i 1 -t 30 [ ID] Interval Transfer Bandwidth Retr Cwnd[ 4] 0.00-1.00 sec 16.0 MBytes 135 Mbits/sec 4174 652 KBytes [ 4] 1.00-2.00 sec 21.2 MBytes 178 Mbits/sec 3352 1011 KBytes [ 4] 2.00-3.00 sec 26.2 MBytes 220 Mbits/sec 1293 5.66 KBytes [ 4] 3.00-4.00 sec 45.0 MBytes 378 Mbits/sec 1368 2.62 MBytes [ 4] 4.00-5.00 sec 25.0 MBytes 210 Mbits/sec 2496 1.96 MBytes [ 4] 5.00-6.00 sec 42.5 MBytes 357 Mbits/sec 1203 2.60 MBytes [ 4] 6.00-7.00 sec 33.8 MBytes 283 Mbits/sec 2117 990 KBytes [ 4] 7.00-8.00 sec 33.8 MBytes 283 Mbits/sec 0 1.02 MBytes [ 4] 8.00-9.00 sec 35.0 MBytes 294 Mbits/sec 13 824 KBytes [ 4] 9.00-10.00 sec 27.5 MBytes 231 Mbits/sec 63 860 KBytes [ 4] 10.00-11.00 sec 31.2 MBytes 262 Mbits/sec 0 940 KBytes [ 4] 11.00-12.00 sec 31.2 MBytes 262 Mbits/sec 0 998 KBytes [ 4] 12.00-13.00 sec 35.0 MBytes 294 Mbits/sec 0 1.01 MBytes [ 4] 13.00-14.00 sec 35.0 MBytes 294 Mbits/sec 0 1.04 MBytes [ 4] 14.00-15.00 sec 35.0 MBytes 294 Mbits/sec 0 1.05 MBytes [ 4] 15.00-16.00 sec 36.2 MBytes 304 Mbits/sec 1 1.07 MBytes [ 4] 16.00-17.00 sec 37.5 MBytes 315 Mbits/sec 0 1.10 MBytes [ 4] 17.00-18.00 sec 38.8 MBytes 325 Mbits/sec 0 1.13 MBytes [ 4] 18.00-19.00 sec 30.0 MBytes 252 Mbits/sec 96 894 KBytes [ 4] 19.00-20.00 sec 30.0 MBytes 252 Mbits/sec 1 932 KBytes [ 4] 20.00-21.00 sec 31.2 MBytes 262 Mbits/sec 0 956 KBytes [ 4] 21.00-22.00 sec 32.5 MBytes 273 Mbits/sec 0 983 KBytes [ 4] 22.00-23.00 sec 32.5 MBytes 273 Mbits/sec 0 1008 KBytes [ 4] 23.00-24.00 sec 35.0 MBytes 294 Mbits/sec 0 1.01 MBytes [ 4] 24.00-25.00 sec 35.0 MBytes 294 Mbits/sec 0 1.03 MBytes [ 4] 25.00-26.00 sec 36.2 MBytes 304 Mbits/sec 0 1.06 MBytes [ 4] 26.00-27.00 sec 37.5 MBytes 315 Mbits/sec 0 1.15 MBytes [ 4] 27.00-28.00 sec 41.2 MBytes 346 Mbits/sec 0 1.27 MBytes [ 4] 28.00-29.00 sec 32.5 MBytes 273 Mbits/sec 226 1018 KBytes [ 4] 29.00-30.00 sec 36.2 MBytes 304 Mbits/sec 0 1.09 MBytes - - - - - - - - - - - - - - - - - - - - - - - - -[ ID] Interval Transfer Bandwidth Retr[ 4] 0.00-30.00 sec 996 MBytes 279 Mbits/sec 16403 sender[ 4] 0.00-30.00 sec 993 MBytes 278 Mbits/sec receiveriperf Done. 北京： 1234$ iperf3 -s -p 12345 -i 1-----------------------------------------------------------Server listening on 12345 上海： 123456789101112131415161718192021222324252627282930313233343536373839$ iperf3 -c xxx.xxx.xxx.103 -p12345 -i 1 -t 30[ ID] Interval Transfer Bandwidth Retr Cwnd[ 4] 0.00-1.00 sec 1.48 MBytes 12.4 Mbits/sec 1 56.6 KBytes [ 4] 1.00-2.00 sec 2.49 MBytes 20.9 Mbits/sec 1 80.6 KBytes [ 4] 2.00-3.00 sec 3.48 MBytes 29.2 Mbits/sec 1 106 KBytes [ 4] 3.00-4.00 sec 4.29 MBytes 36.0 Mbits/sec 1 130 KBytes [ 4] 4.00-5.00 sec 4.91 MBytes 41.2 Mbits/sec 0 157 KBytes [ 4] 5.00-6.00 sec 6.46 MBytes 54.2 Mbits/sec 0 184 KBytes [ 4] 6.00-7.00 sec 6.84 MBytes 57.3 Mbits/sec 0 212 KBytes [ 4] 7.00-8.00 sec 8.20 MBytes 68.8 Mbits/sec 0 238 KBytes [ 4] 8.00-9.00 sec 8.76 MBytes 73.5 Mbits/sec 0 266 KBytes [ 4] 9.00-10.00 sec 9.82 MBytes 82.4 Mbits/sec 0 293 KBytes [ 4] 10.00-11.00 sec 11.7 MBytes 98.0 Mbits/sec 0 356 KBytes [ 4] 11.00-12.00 sec 14.4 MBytes 120 Mbits/sec 0 467 KBytes [ 4] 12.00-13.00 sec 19.8 MBytes 166 Mbits/sec 0 602 KBytes [ 4] 13.00-14.00 sec 23.8 MBytes 199 Mbits/sec 0 772 KBytes [ 4] 14.00-15.00 sec 30.0 MBytes 252 Mbits/sec 0 977 KBytes [ 4] 15.00-16.00 sec 37.5 MBytes 315 Mbits/sec 1 1.06 MBytes [ 4] 16.00-17.00 sec 38.8 MBytes 325 Mbits/sec 0 1.09 MBytes [ 4] 17.00-18.00 sec 40.0 MBytes 336 Mbits/sec 0 1.11 MBytes [ 4] 18.00-19.00 sec 36.2 MBytes 304 Mbits/sec 152 846 KBytes [ 4] 19.00-20.00 sec 31.2 MBytes 262 Mbits/sec 4 911 KBytes [ 4] 20.00-21.00 sec 31.2 MBytes 262 Mbits/sec 0 938 KBytes [ 4] 21.00-22.00 sec 33.8 MBytes 283 Mbits/sec 0 964 KBytes [ 4] 22.00-23.00 sec 33.8 MBytes 283 Mbits/sec 0 991 KBytes [ 4] 23.00-24.00 sec 35.0 MBytes 294 Mbits/sec 0 1018 KBytes [ 4] 24.00-25.00 sec 37.5 MBytes 315 Mbits/sec 0 1.02 MBytes [ 4] 25.00-26.00 sec 36.2 MBytes 304 Mbits/sec 0 1.05 MBytes [ 4] 26.00-27.00 sec 32.5 MBytes 273 Mbits/sec 82 813 KBytes [ 4] 27.00-28.00 sec 30.0 MBytes 252 Mbits/sec 0 905 KBytes [ 4] 28.00-29.00 sec 33.8 MBytes 283 Mbits/sec 0 973 KBytes [ 4] 29.00-30.00 sec 33.8 MBytes 283 Mbits/sec 0 1022 KBytes - - - - - - - - - - - - - - - - - - - - - - - - -[ ID] Interval Transfer Bandwidth Retr[ 4] 0.00-30.00 sec 678 MBytes 189 Mbits/sec 243 sender[ 4] 0.00-30.00 sec 676 MBytes 189 Mbits/sec receiveriperf Done. 可以看到TCP稳定时，大概是300M的带宽。 rtmp推流测试在北京起一个srs服务器，从上海使用srs-bench推流过去。 这里srs服务器和srs-bench都是单核运行的，为了保证srs-bench不会先达到cpu上限，使用两个srs-bench进程来推流。 先推750路流： 1234567891011$ ./objs/sb_rtmp_publish -i doc/source.200kbps.768x320.flv -c 750 -s 10 -r rtmp://xxx.xxx.xxx.103:2019/live/livestream_&#123;i&#125;?vhost=long.test.com[2019-10-27 15:58:55.938] [report] [16398] threads:750 alive:750 duration:30 tduration:0 nread:0.79 nwrite:137.69 tasks:750 etasks:0 stasks:0 estasks:0[2019-10-27 15:59:25.938] [report] [16398] threads:750 alive:750 duration:60 tduration:0 nread:0.39 nwrite:163.65 tasks:750 etasks:0 stasks:0 estasks:0[2019-10-27 15:59:55.938] [report] [16398] threads:750 alive:750 duration:90 tduration:0 nread:0.26 nwrite:164.61 tasks:750 etasks:0 stasks:0 estasks:0[2019-10-27 16:00:25.938] [report] [16398] threads:750 alive:750 duration:120 tduration:0 nread:0.20 nwrite:172.49 tasks:750 etasks:0 stasks:0 estasks:0[2019-10-27 16:00:55.938] [report] [16398] threads:750 alive:750 duration:150 tduration:0 nread:0.16 nwrite:174.96 tasks:750 etasks:0 stasks:0 estasks:0[2019-10-27 16:01:25.938] [report] [16398] threads:750 alive:750 duration:180 tduration:0 nread:0.13 nwrite:179.18 tasks:750 etasks:0 stasks:0 estasks:0[2019-10-27 16:01:55.938] [report] [16398] threads:750 alive:750 duration:210 tduration:0 nread:0.11 nwrite:184.73 tasks:750 etasks:0 stasks:0 estasks:0[2019-10-27 16:02:25.938] [report] [16398] threads:750 alive:750 duration:240 tduration:0 nread:0.10 nwrite:182.79 tasks:750 etasks:0 stasks:0 estasks:0[2019-10-27 16:02:55.939] [report] [16398] threads:750 alive:750 duration:270 tduration:0 nread:0.09 nwrite:183.50 tasks:750 etasks:0 stasks:0 estasks:0 观察服务器cpu占用： 1234567891011121314151617$ toptop - 15:57:51 up 243 days, 1:29, 4 users, load average: 0.35, 0.16, 0.10Tasks: 212 total, 2 running, 210 sleeping, 0 stopped, 0 zombie%Cpu(s): 2.2 us, 0.5 sy, 0.0 ni, 96.7 id, 0.0 wa, 0.0 hi, 0.6 si, 0.0 stKiB Mem : 65807864 total, 2352216 free, 1588392 used, 61867256 buff/cacheKiB Swap: 4194300 total, 4192468 free, 1832 used. 61573296 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 9548 root 20 0 639468 318264 1924 R 50.5 0.5 22:03.87 srs 9554 root 20 0 437456 221308 1928 S 15.6 0.3 11:34.34 srs 10 root 20 0 0 0 0 S 0.3 0.0 236:57.63 rcu_sched 1 root 20 0 199248 3912 2436 S 0.0 0.0 68:38.96 systemd 2 root 20 0 0 0 0 S 0.0 0.0 0:00.01 kthreadd 3 root 20 0 0 0 0 S 0.0 0.0 1:33.02 ksoftirqd/0 5 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 kworker/0:0H 6 root 20 0 0 0 0 S 0.0 0.0 0:03.29 kworker/u32:0 观察服务器IO占用： 12345678910111213141516$ dstat----total-cpu-usage---- -dsk/total- -net/total- ---paging-- ---system--usr sys idl wai hiq siq| read writ| recv send| in out | int csw 2 0 97 0 0 1| 0 0 | 24M 730k| 0 0 | 12k 7223 2 0 97 0 0 1| 0 0 | 23M 715k| 0 0 | 11k 7190 2 1 97 0 0 1| 0 112k| 20M 675k| 0 0 | 11k 7399 2 0 97 0 0 1| 0 0 | 21M 684k| 0 0 | 11k 6847 2 0 97 0 0 1| 0 0 | 23M 695k| 0 0 | 11k 7115 2 0 97 0 0 1| 0 0 | 23M 710k| 0 0 | 11k 7168 2 0 97 0 0 1| 0 0 | 24M 714k| 0 0 | 11k 7377 2 1 97 0 0 1| 0 0 | 27M 743k| 0 0 | 12k 6799 2 0 97 0 0 1| 0 24k| 30M 783k| 0 0 | 13k 7247 2 0 97 0 0 1| 0 0 | 33M 813k| 0 0 | 13k 7354 2 1 97 0 0 1| 0 0 | 36M 833k| 0 0 | 14k 8435 2 0 97 0 0 1| 0 0 | 37M 861k| 0 0 | 14k 7925 再推250路流，增加到1000路流： 12345$ ./objs/sb_rtmp_publish -i doc/source.200kbps.768x320.flv -c 250 -s 10 -r rtmp://xxx.xxx.xxx.103:2019/live/livestream1_&#123;i&#125;?vhost=long.test.com[2019-10-27 16:28:10.008] [report] [16481] threads:250 alive:250 duration:1110 tduration:0 nread:0.01 nwrite:62.53 tasks:262 etasks:12 stasks:0 estasks:0[2019-10-27 16:28:40.008] [report] [16481] threads:250 alive:250 duration:1140 tduration:0 nread:0.01 nwrite:62.37 tasks:262 etasks:12 stasks:0 estasks:0[2019-10-27 16:29:10.008] [report] [16481] threads:250 alive:250 duration:1170 tduration:0 nread:0.01 nwrite:62.39 tasks:262 etasks:12 stasks:0 estasks:0 观察服务器cpu占用： 123456789101112top - 16:26:27 up 243 days, 1:57, 4 users, load average: 0.84, 0.73, 0.61Tasks: 212 total, 1 running, 211 sleeping, 0 stopped, 0 zombie%Cpu(s): 3.0 us, 0.5 sy, 0.0 ni, 95.7 id, 0.0 wa, 0.0 hi, 0.8 si, 0.0 stKiB Mem : 65807864 total, 2342924 free, 1593668 used, 61871272 buff/cacheKiB Swap: 4194300 total, 4192468 free, 1832 used. 61567380 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 9548 root 20 0 639468 318264 1924 S 69.4 0.5 39:42.44 srs 15063 root 20 0 155140 5848 4500 S 1.0 0.0 0:00.03 sshd 15065 root 20 0 116496 3088 1648 S 0.7 0.0 0:00.02 bash 10 root 20 0 0 0 0 S 0.3 0.0 237:00.91 rcu_sched 80 root 20 0 0 0 0 S 0.3 0.0 560:22.55 ksoftirqd/14 观察服务器IO占用： 12345678910111213----total-cpu-usage---- -dsk/total- -net/total- ---paging-- ---system--usr sys idl wai hiq siq| read writ| recv send| in out | int csw 3 0 96 0 0 1| 0 0 | 32M 990k| 0 0 | 14k 5548 3 0 96 0 0 1| 0 20k| 31M 994k| 0 0 | 14k 5763 3 0 96 0 0 1| 0 0 | 32M 986k| 0 0 | 14k 5636 3 0 96 0 0 1| 0 0 | 32M 979k| 0 0 | 14k 5777 3 0 96 0 0 1| 0 8192B| 31M 981k| 0 0 | 14k 5762 4 0 95 0 0 1| 0 0 | 32M 989k| 0 0 | 14k 5615 4 1 95 0 0 1| 0 3008k| 33M 1011k| 0 0 | 15k 5571 3 1 96 0 0 1| 0 0 | 35M 1051k| 0 0 | 15k 5471 3 0 96 0 0 1| 0 0 | 37M 1066k| 0 0 | 15k 5289 3 1 95 0 0 1| 44k 4096B| 38M 1079k| 0 0 | 15k 5712 3 0 96 0 0 1| 0 0 | 39M 1086k| 0 0 | 15k 5389 增加到1200路流： 12345$ ./objs/sb_rtmp_publish -i doc/source.200kbps.768x320.flv -c 200 -s 10 -r rtmp://xxx.xxx.xxx.103:2019/live/livestream2_&#123;i&#125;?vhost=long.test.com[2019-10-27 16:33:21.150] [report] [16689] threads:200 alive:199 duration:60 tduration:0 nread:0.11 nwrite:39.88 tasks:208 etasks:9 stasks:0 estasks:0[2019-10-27 16:33:51.150] [report] [16689] threads:200 alive:200 duration:90 tduration:0 nread:0.08 nwrite:41.05 tasks:219 etasks:19 stasks:0 estasks:0[2019-10-27 16:34:21.150] [report] [16689] threads:200 alive:200 duration:120 tduration:0 nread:0.06 nwrite:42.49 tasks:219 etasks:19 stasks:0 estasks:0 观察服务器cpu占用： 123456789101112top - 16:32:01 up 243 days, 2:03, 4 users, load average: 0.98, 0.85, 0.68Tasks: 213 total, 2 running, 211 sleeping, 0 stopped, 0 zombie%Cpu(s): 3.5 us, 0.5 sy, 0.0 ni, 95.1 id, 0.0 wa, 0.0 hi, 0.9 si, 0.0 stKiB Mem : 65807864 total, 2326480 free, 1607988 used, 61873396 buff/cacheKiB Swap: 4194300 total, 4192468 free, 1832 used. 61551888 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 9548 root 20 0 675468 321256 1924 R 88.4 0.5 44:02.00 srs 10 root 20 0 0 0 0 S 0.3 0.0 237:01.58 rcu_sched 1060 root 20 0 20.1g 421956 13468 S 0.3 0.6 308:47.03 java 5359 root 20 0 112800 4328 3300 S 0.3 0.0 9:57.35 sshd 6052 root 20 0 13220 796 592 S 0.3 0.0 238:27.05 rngd 观察服务器IO占用： 1234567891011121314----total-cpu-usage---- -dsk/total- -net/total- ---paging-- ---system--usr sys idl wai hiq siq| read writ| recv send| in out | int csw 3 1 95 0 0 1| 0 0 | 35M 1218k| 0 0 | 16k 3206 3 0 95 0 0 1| 0 0 | 35M 1235k| 0 0 | 17k 3217 3 1 95 0 0 1| 0 0 | 35M 1224k| 0 0 | 17k 3194 4 0 95 0 0 1| 0 0 | 35M 1216k| 0 0 | 17k 3381 4 1 95 0 0 1| 0 52k| 35M 1223k| 0 0 | 17k 3177 4 1 95 0 0 1| 0 0 | 35M 1247k| 0 0 | 17k 3289 4 0 95 0 0 1| 0 0 | 35M 1218k| 0 0 | 17k 3346 3 0 95 0 0 1| 0 0 | 35M 1229k| 0 0 | 17k 3093 4 1 95 0 0 1| 0 0 | 35M 1207k| 0 0 | 17k 3226 3 0 95 0 0 1| 0 60k| 35M 1238k| 0 0 | 17k 3097 4 0 95 0 0 1| 0 8192B| 35M 1209k| 0 0 | 17k 3612 3 1 95 0 0 1| 0 0 | 35M 1219k| 0 0 | 16k 3216 结论：在推送200kbps的流的情况下，大概在1000路推流时，带宽首先被占满，CPU在70%左右。 rtmp拉流测试先推一路流上去： 1$ ./objs/sb_rtmp_publish -i doc/source.200kbps.768x320.flv -c 1 -s 10 -r rtmp://xxx.xxx.xxx.103:2019/live/livestream?vhost=long.test.com 直接拉1000路流：： 1234$ ./objs/sb_rtmp_load -c 1000 -s 10 -r rtmp://xxx.xxx.xxx.103:2019/live/livestream?vhost=long.test.com[2019-10-27 16:42:00.428] [report] [16756] threads:1000 alive:1000 duration:30 tduration:0 nread:172.40 nwrite:0.93 tasks:1000 etasks:0 stasks:0 estasks:0[2019-10-27 16:42:30.428] [report] [16756] threads:1000 alive:1000 duration:60 tduration:0 nread:212.69 nwrite:0.46 tasks:1000 etasks:0 stasks:0 estasks:0 观察服务器cpu占用： 12345678910111213top - 16:40:27 up 243 days, 2:11, 4 users, load average: 0.14, 0.42, 0.56Tasks: 213 total, 2 running, 211 sleeping, 0 stopped, 0 zombie%Cpu(s): 0.5 us, 0.3 sy, 0.0 ni, 98.5 id, 0.0 wa, 0.0 hi, 0.7 si, 0.0 stKiB Mem : 65807864 total, 2276144 free, 1648396 used, 61883324 buff/cacheKiB Swap: 4194300 total, 4192468 free, 1832 used. 61505084 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 9548 root 20 0 675468 321276 1944 R 13.6 0.5 47:13.14 srs 6572 root 20 0 1949000 44236 14768 S 1.7 0.1 955:10.49 containerd 4921 root 20 0 24576 8708 2380 S 0.3 0.0 85:26.68 srs 6052 root 20 0 13220 796 592 S 0.3 0.0 238:28.16 rngd 1 root 20 0 199248 3912 2436 S 0.0 0.0 68:39.47 systemd 2 root 20 0 0 0 0 S 0.0 0.0 0:00.01 kthreadd 观察服务器IO占用： 123456789101112131415----total-cpu-usage---- -dsk/total- -net/total- ---paging-- ---system--usr sys idl wai hiq siq| read writ| recv send| in out | int csw 0 0 99 0 0 1| 0 0 |1309k 46M| 0 0 | 14k 880 1 0 98 0 0 1| 0 0 |1312k 45M| 0 0 | 14k 913 0 0 99 0 0 1| 0 0 |1167k 44M| 0 0 | 13k 897 1 0 99 0 0 1| 0 0 |1236k 45M| 0 0 | 12k 810 1 0 99 0 0 1| 0 148k|1309k 47M| 0 0 | 13k 911 0 0 99 0 0 1| 0 0 |1296k 45M| 0 0 | 13k 817 0 0 99 0 0 1| 0 0 |1269k 42M| 0 0 | 13k 843 0 0 99 0 0 1| 0 0 |1195k 44M| 0 0 | 12k 894 0 0 99 0 0 1| 0 0 |1192k 42M| 0 0 | 12k 812 1 0 99 0 0 1| 0 20k|1271k 49M| 0 0 | 12k 795 0 0 99 0 0 1| 0 0 |1243k 41M| 0 0 | 13k 949 1 0 98 0 0 1| 0 0 |1320k 48M| 0 0 | 13k 909 0 0 99 0 0 1| 0 0 |1254k 45M| 0 0 | 13k 858 结论：在拉取200kbps的流的情况下，大概在1000路拉流时，带宽45MB左右，CPU在13%左右，上限决定于带宽 flv拉流测试先推一路流上去： 1$ ./objs/sb_rtmp_publish -i doc/source.200kbps.768x320.flv -c 1 -s 10 -r rtmp://xxx.xxx.xxx.103:2019/live/livestream?vhost=long.test.com 直接拉1000路流：： 12345$ ./objs/sb_http_load -c 1000 -s 10 -r http://long.test.com:3019/live/livestream.flv[2019-10-27 18:05:31.713] [report] [17375] threads:1000 alive:1000 duration:240 tduration:0 nread:238.23 nwrite:0.00 tasks:1000 etasks:0 stasks:0 estasks:0[2019-10-27 18:06:01.713] [report] [17375] threads:1000 alive:1000 duration:270 tduration:0 nread:242.30 nwrite:0.00 tasks:1000 etasks:0 stasks:0 estasks:0[2019-10-27 18:06:31.713] [report] [17375] threads:1000 alive:1000 duration:300 tduration:0 nread:244.52 nwrite:0.00 tasks:1000 etasks:0 stasks:0 estasks:0 观察服务器cpu占用： 1234567891011121314top - 18:04:10 up 243 days, 3:35, 4 users, load average: 0.10, 0.17, 0.13Tasks: 211 total, 2 running, 209 sleeping, 0 stopped, 0 zombie%Cpu(s): 1.2 us, 0.9 sy, 0.0 ni, 97.1 id, 0.0 wa, 0.0 hi, 0.8 si, 0.0 stKiB Mem : 65807864 total, 2559192 free, 1223956 used, 62024716 buff/cacheKiB Swap: 4194300 total, 4192468 free, 1832 used. 61930044 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 18700 root 20 0 340136 107528 2000 R 41.5 0.2 3:15.25 srs 6572 root 20 0 1949000 43996 14768 S 1.7 0.1 955:25.07 containerd 966 root 20 0 122040 1492 888 S 0.3 0.0 174:11.47 wrapper 9695 root 20 0 161976 2372 1600 R 0.3 0.0 0:29.07 top 1 root 20 0 199248 3912 2436 S 0.0 0.0 68:40.50 systemd 2 root 20 0 0 0 0 S 0.0 0.0 0:00.01 kthreadd 3 root 20 0 0 0 0 S 0.0 0.0 1:33.02 ksoftirqd/0 观察服务器IO占用： 1234567891011121314----total-cpu-usage---- -dsk/total- -net/total- ---paging-- ---system--usr sys idl wai hiq siq| read writ| recv send| in out | int csw 1 1 97 0 0 1| 0 0 |1138k 42M| 0 0 | 13k 1447 1 1 97 0 0 1| 0 0 |1121k 42M| 0 0 | 13k 1417 1 1 97 0 0 1| 0 0 |1108k 41M| 0 0 | 13k 1436 1 1 97 0 0 1| 0 28k|1110k 44M| 0 0 | 13k 1381 1 1 97 0 0 1| 0 0 |1150k 44M| 0 0 | 13k 1300 1 1 97 0 0 1| 0 0 |1190k 42M| 0 0 | 13k 1306 1 1 98 0 0 1| 0 0 |1086k 42M| 0 0 | 13k 1320 1 1 98 0 0 1| 0 0 |1186k 44M| 0 0 | 13k 1298 1 1 98 0 0 1| 0 68k|1181k 41M| 0 0 | 14k 1348 1 1 98 0 0 1| 0 0 | 853k 33M| 0 0 | 11k 1255 1 1 98 0 0 0| 0 0 | 812k 37M| 0 0 | 10k 1149 3 1 96 0 0 0| 0 0 | 876k 41M| 0 0 | 11k 1308 结论：与rtmp类似，在拉取200kbps的流的情况下，大概在1000路拉流时，带宽43MB左右，CPU在41%左右，上限决定于带宽， 但是这里的CPU使用率显然比rtmp高得多。 最后总结无论是推流还是拉流，实际上的瓶颈都在于带宽。并且在测试中srs实际上只使用了一个CPU核心，实际上还剩余了大量的CPU资源。 由于srs-bench不支持flv推流测试，所以暂时没有对flv推流进行测试。 ——————————————————-2019-11-1补充：局域网测试上一次测试限于带宽只有300M，导致测试效果是带宽为瓶颈，这里在局域网中进行测试，有更大的带宽。 VM1：192.168.90.40 VM2：192.168.90.45 主机CPU为i7-8700，频率3.2GHz。 带宽测试VM1: 12345$ iperf3 -s -p 12345 -i 1-----------------------------------------------------------Server listening on 12345----------------------------------------------------------- VM2: 1234567891011121314$ iperf3 -c 192.168.90.40 -p12345 -i 1 -t 30Connecting to host 192.168.90.40, port 12345[ 4] local 192.168.90.43 port 50380 connected to 192.168.90.40 port 12345[ ID] Interval Transfer Bandwidth Retr Cwnd[ 4] 0.00-1.00 sec 111 MBytes 932 Mbits/sec 3 402 KBytes [ 4] 1.00-2.00 sec 113 MBytes 948 Mbits/sec 0 581 KBytes [ 4] 2.00-3.00 sec 111 MBytes 934 Mbits/sec 0 713 KBytes [ 4] 3.00-4.00 sec 111 MBytes 933 Mbits/sec 0 826 KBytes [ 4] 4.00-5.00 sec 111 MBytes 933 Mbits/sec 0 923 KBytes [ 4] 5.00-6.00 sec 111 MBytes 933 Mbits/sec 0 1014 KBytes [ 4] 6.00-7.00 sec 111 MBytes 933 Mbits/sec 0 1.07 MBytes [ 4] 7.00-8.00 sec 112 MBytes 944 Mbits/sec 0 1.14 MBytes [ 4] 8.00-9.00 sec 111 MBytes 933 Mbits/sec 0 1.21 MBytes 可以看到带宽接近千兆。 推拉流测试结果CPU为i7-8700，频率3.2GHz;带宽933 Mbits/sec。 测试项目 单路流大小 推拉流数 CPU使用率 带宽占用 rtmp推流测试 200kbps 4000 100% 114M rtmp拉流测试 200kbps 4000 20%~40% 114M flv拉流测试 200kbps 4000 60%~80% 114M rtmp推流测试 1000kbps 500 40%~50% 60M~80M rtmp推流测试 1000kbps 1000 75%~80% 114M rtmp拉流测试 1000kbps 500 13%~15% 60M~80M rtmp拉流测试 1000kbps 1000 15%~17% 114M flv拉流测试 1000kbps 500 18%~20% 60M~80M flv拉流测试 1000kbps 1000 45%~50% 114M]]></content>
      <categories>
        <category>live</category>
      </categories>
      <tags>
        <tag>live</tag>
        <tag>srs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[srs中ffmpeg配置转码参数的一些介绍]]></title>
    <url>%2F2019%2F10%2F29%2F2019-10-29-srs_ffmpeg_transcode_params%2F</url>
    <content type="text"><![CDATA[srs 所有转码参数介绍srs版本v2 参考来自srs官方FFMPEG教程 首先是原官方文档： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113listen 1935;vhost __defaultVhost__ &#123; # the streaming transcode configs. transcode &#123; # whether the transcode enabled. # if off, donot transcode. # default: off. enabled on; # the ffmpeg ffmpeg ./objs/ffmpeg/bin/ffmpeg; # the transcode engine for matched stream. # all matched stream will transcoded to the following stream. # the transcode set name(ie. hd) is optional and not used. engine example &#123; # whether the engine is enabled # default: off. enabled on; # input format, can be: # off, do not specifies the format, ffmpeg will guess it. # flv, for flv or RTMP stream. # other format, for example, mp4/aac whatever. # default: flv iformat flv; # ffmpeg filters, follows the main input. vfilter &#123; # the logo input file. i ./doc/ffmpeg-logo.png; # the ffmpeg complex filter. # for filters, @see: http://ffmpeg.org/ffmpeg-filters.html filter_complex &apos;overlay=10:10&apos;; &#125; # video encoder name. can be: # libx264: use h.264(libx264) video encoder. # copy: donot encoder the video stream, copy it. # vn: disable video output. vcodec libx264; # video bitrate, in kbps # @remark 0 to use source video bitrate. # default: 0 vbitrate 1500; # video framerate. # @remark 0 to use source video fps. # default: 0 vfps 25; # video width, must be even numbers. # @remark 0 to use source video width. # default: 0 vwidth 768; # video height, must be even numbers. # @remark 0 to use source video height. # default: 0 vheight 320; # the max threads for ffmpeg to used. # default: 1 vthreads 12; # x264 profile, @see x264 -help, can be: # high,main,baseline vprofile main; # x264 preset, @see x264 -help, can be: # ultrafast,superfast,veryfast,faster,fast # medium,slow,slower,veryslow,placebo vpreset medium; # other x264 or ffmpeg video params vparams &#123; # ffmpeg options, @see: http://ffmpeg.org/ffmpeg.html t 100; # 264 params, @see: http://ffmpeg.org/ffmpeg-codecs.html#libx264 coder 1; b_strategy 2; bf 3; refs 10; &#125; # audio encoder name. can be: # libfdk_aac: use aac(libfdk_aac) audio encoder. # copy: donot encoder the audio stream, copy it. # an: disable audio output. acodec libfdk_aac; # audio bitrate, in kbps. [16, 72] for libfdk_aac. # @remark 0 to use source audio bitrate. # default: 0 abitrate 70; # audio sample rate. for flv/rtmp, it must be: # 44100,22050,11025,5512 # @remark 0 to use source audio sample rate. # default: 0 asample_rate 44100; # audio channel, 1 for mono, 2 for stereo. # @remark 0 to use source audio channels. # default: 0 achannels 2; # other ffmpeg audio params aparams &#123; # audio params, @see: http://ffmpeg.org/ffmpeg-codecs.html#Audio-Encoders # @remark SRS supported aac profile for HLS is: aac_low, aac_he, aac_he_v2 profile:a aac_low; bsf:a aac_adtstoasc; &#125; # output format, can be: # off, do not specifies the format, ffmpeg will guess it. # flv, for flv or RTMP stream. # other format, for example, mp4/aac whatever. # default: flv oformat flv; # output stream. variables: # [vhost] the input stream vhost. # [port] the intput stream port. # [app] the input stream app. # [stream] the input stream name. # [engine] the tanscode engine name. output rtmp://127.0.0.1:[port]/[app]?vhost=[vhost]/[stream]_[engine]; &#125; &#125;&#125; 转码engine配置一个 transcode 中可以配置多个 engine 来进行转码： 12345678910111213vhost www.test.com &#123; transcode &#123; engine 360p &#123; ... &#125; engine 480p &#123; ... &#125; engine 720p &#123; ... &#125; &#125;&#125; srs 中可使用的转码参数 整体参数123456789101112131415161718192021222324252627282930313233343536373839# 输入格式，其中flv表示输入格式为flv或者是RTMP。iformat flv;# ffmpeg 滤镜vfilter &#123;&#125;# 视频相关参数vcodec libx264;vbitrate 1500;vfps 25;vwidth 768;vheight 320;vthreads 12;vprofile main;vpreset medium;vparams &#123; t 100; coder 1; b_strategy 2; bf 3; refs 10;&#125;# 音频相关参数acodec libfdk_aac;abitrate 70;asample_rate 44100;achannels 2;aparams &#123; profile:a aac_low; bsf:a aac_adtstoasc;&#125;# 输出格式，其中flv表示输出格式为flv或者是RTMP。oformat flv;# 转码后的输出流output rtmp://127.0.0.1:[port]/[app]?vhost=[vhost]/[stream]_[engine]; 视频相关参数vcodec： 视频编码器名称，通常使用 libx264。 vbitrate： 视频编码所使用的码率，单位kbps。这里所设置的应该是视频的平均码率。 vfps： 视频帧率。 vwidth： 视频宽度。 vheight： 视频高度。 vthreads： 使用多少个线程来进行转码。开启多个线程需要codec（编解码器）支持才行。 vprofile： 可以简单理解为H264的版本，可以选择high,main,baseline。 暂时知道：压缩率 high &gt; main &gt; baseline。 暂时不知道它与画质有没有直接关系。 H264 Profile对比分析 vpreset： 用于调节编码速度和质量的平衡。 可以设置为：ultrafast,superfast,veryfast,faster,fast,medium,slow,slower,veryslow,placebo vparams： 可以添加一些x264或者视频相关的参数。 t：限制从输入文件读取的时长 coder：x264使用，熵编码相关参数。 b_strategy：x264使用，自适应的添加B帧数目，Use only on first-pass。 bf：max number of B frames。 refs：Set reference frames to consider for motion compensation。 实际上可以在这个参数中添加各种参数，而这些参数都会被添加在-preset后面。 视频滤镜 vfilteri： 表示输入，例如加水印时，就需要额外的图片作为输入。 可以有多个输入，并且输入也可以是流的形式，意思是可以支持多路流合并成为一路流。 filter_complex： 也就是FFmpeg的filter_complex，一个filtergraph，支持多个输入流。 使用它可以做各种事情，后面使用举例说明。 详细使用方法可以参考官方文档:4.1 Filtergraph syntax 音频相关参数acodec： 音频编码器，通常使用 libfdk_aac 。 abitrate： 音频码率，单位kbps，[16, 72] for libfdk_aac。 asample_rate： 音频采样率，对于 flv和rtmp，只能设置为 44100,22050,11025,5512。 achannels： 声道，1表示单声道，2表示双声道。 aparams： 可以在里面填一些与音频相关的转码参数，同样的，这些参数会接在achannels(-ac)的后面。 profile:a 与h264编码中的profile类似，这里支持 aac_low, aac_he, aac_he_v2 这几种参数的设置。 bsf:a 表示Bitstream Filters，详细参考18 Bitstream Filters。 转码配置举例 改变视频分辨率方法一： 使用vwidth和vheight参数。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950vhost www.transcode.com &#123; transcode &#123; enabled on; ffmpeg ./objs/ffmpeg/bin/ffmpeg; engine 1080_60 &#123; enabled on; vfilter &#123; &#125; vcodec libx264; vbitrate 5000; vfps 60; vwidth 1920; vheight 1080; vthreads 12; vprofile main; vpreset fast; vparams &#123; &#125; acodec libfdk_aac; abitrate 50; asample_rate 44100; achannels 2; aparams &#123; &#125; output rtmp://192.168.90.229:2019/persist?vhost=lrm.test.com/long_1080_60; &#125; engine 720_60 &#123; enabled on; vfilter &#123; &#125; vcodec libx264; vbitrate 3500; vfps 60; vwidth 1280; vheight 720; vthreads 12; vprofile main; vpreset fast; vparams &#123; &#125; acodec libfdk_aac; abitrate 50; asample_rate 44100; achannels 2; aparams &#123; &#125; output rtmp://192.168.90.229:2019/persist?vhost=lrm.test.com/long_720_60; &#125; &#125;&#125; 上面转码将视频转码成1080P+60fps和720P+60fps。 方法二： 使用filter_complex，在其中使用scale滤镜来缩减分辨率，并将vwidth和vheight设置为0。 1234567891011121314151617181920212223242526272829vhost www.transcode.com &#123; transcode &#123; enabled on; ffmpeg ./objs/ffmpeg/bin/ffmpeg; engine 720_60 &#123; enabled on; vfilter &#123; filter_complex &apos;scale=1280:-1&apos;; &#125; vcodec libx264; vbitrate 3500; vfps 60; vwidth 0; vheight 0; vthreads 12; vprofile main; vpreset fast; vparams &#123; &#125; acodec libfdk_aac; abitrate 50; asample_rate 44100; achannels 2; aparams &#123; &#125; output rtmp://192.168.90.229:2019/persist?vhost=lrm.test.com/long_720_60; &#125; &#125;&#125; 结论：推荐使用方法二。 如果输入视频不是16:9分辨率时，使用方法一转码视频造成画面拉伸，因为它的vwidth和vheight参数反映到FFmpeg中是-s vwidth x vheight， 使用-1时则会报错。而使用方法二则可以使用-1来维持原画面比例。 添加水印官方例子： 123456789101112131415161718192021222324252627282930vhost __defaultVhost__ &#123; transcode &#123; enabled on; ffmpeg ./objs/ffmpeg/bin/ffmpeg; engine ff &#123; enabled on; vfilter &#123; i ./doc/ffmpeg-logo.png; filter_complex &apos;overlay=10:10&apos;; &#125; vcodec libx264; vbitrate 300; vfps 20; vwidth 768; vheight 320; vthreads 2; vprofile baseline; vpreset superfast; vparams &#123; &#125; acodec libfdk_aac; abitrate 45; asample_rate 44100; achannels 2; aparams &#123; &#125; output rtmp://127.0.0.1:[port]/[app]?vhost=[vhost]/[stream]_[engine]; &#125; &#125;&#125; 实现方法，使用vfilter，输入图片并使用滤镜overlay来完成。 CBR恒定编码实现先来看一下使用bitrate来控制码率的情况： 123456789101112131415161718192021222324252627282930vhost lrm.test.com &#123; transcode &#123; enabled on; ffmpeg ./objs/ffmpeg/bin/ffmpeg; engine ff &#123; enabled on; vfilter &#123; i ./doc/ffmpeg-logo.png; filter_complex &apos;[1][0]scale2ref=w=iw/8:h=ow/mdar[scaled][0-out]; [0-out][scaled]overlay=x=main_w/10-w/2:y=main_h/10-h/2[over]; [over]scale=1280:-1&apos;; &#125; vcodec libx264; vbitrate 3500; vfps 60; vwidth 0; vheight 0; vthreads 12; vprofile main; vpreset fast; vparams &#123; &#125; acodec libfdk_aac; abitrate 50; asample_rate 44100; achannels 2; aparams &#123; &#125; output rtmp://127.0.0.1:[port]/[app]?vhost=[vhost]/[stream]_[engine]; &#125; &#125;&#125; 这里将vbitrate设置在3500，也就是我们希望码率可以控制在3500kbps，使用VLC播放器可以看到大概的统计效果： 参数中虽然可以使用bitrate来指定码率，但是实际上这只是平均码率，但是实际码率还是会上下波动，并且范围也不小。 可以使用下面方法来实现CBR，也是FFmpeg文档中所述的： 123456789101112131415161718192021222324252627282930313233vhost lrm.test.com &#123; transcode &#123; enabled on; ffmpeg ./objs/ffmpeg/bin/ffmpeg; engine ff &#123; enabled on; vfilter &#123; i ./doc/ffmpeg-logo.png; filter_complex &apos;[1][0]scale2ref=w=iw/8:h=ow/mdar[scaled][0-out]; [0-out][scaled]overlay=x=main_w/10-w/2:y=main_h/10-h/2[over]; [over]scale=1280:-1&apos;; &#125; vcodec libx264; vbitrate 3500; vfps 60; vwidth 0; vheight 0; vthreads 12; vprofile main; vpreset fast; vparams &#123; maxrate:v 3500k; minrate:v 3500k; bufsize:v 3500k; &#125; acodec libfdk_aac; abitrate 50; asample_rate 44100; achannels 2; aparams &#123; &#125; output rtmp://127.0.0.1:[port]/[app]?vhost=[vhost]/[stream]_[engine]; &#125; &#125;&#125; 可以看到上面配置中，增加设置maxrate，minrate，bufsize来配合控制视频的码率。 实际效果并不能固定码率，码率仍然在上下波动，但很明显能达到更小的波动范围。]]></content>
      <categories>
        <category>live</category>
      </categories>
      <tags>
        <tag>live</tag>
        <tag>ffmpeg</tag>
        <tag>srs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[srs中ffmpeg的按需转码实现方法]]></title>
    <url>%2F2019%2F10%2F29%2F2019-10-29-srs_ffmpeg_transcode_as_needed%2F</url>
    <content type="text"><![CDATA[概述使用srs来进行直播流转码时，如果直接配置了转码，则无论有无客户端在拉取转码流，转码都会进行。 需要注意到，转码是一件非常消耗CPU资源的事情，所以实现按需转码（只有在转码流被拉取时，才进行转码）是一件非常值得的事情。 srs的FFmpeg转码时的推拉流逻辑在srs的vhost中，可以配置transcode来使用FFmpeg对流进行转码。 例如下面配置： 12345678910111213listen 1935;vhost www.transcode.com &#123; transcode &#123; enabled on; ffmpeg ./objs/ffmpeg/bin/ffmpeg; engine ff &#123; enabled on; vcodec copy; acodec copy; output rtmp://127.0.0.1:[port]/[app]?vhost=[vhost]/[stream]_[engine]; &#125; &#125;&#125; 假设一路流推送到rtmp://www.transcode.com/testapp/teststream，上面所配置的ffmpeg立即就会对这路流进行转码（即使没有人拉流）， 因为它转码的逻辑是拉流到FFmpeg-&gt;转码-&gt;推流到output所定义的地方。 那么上面配置中的写法就表示： 从 rtmp://127.0.0.1:1935/testapp?vhost=www.transcode.com/teststream 拉流。 转码。 推流到 rtmp://127.0.0.1:1935/testapp?vhost=www.transcode.com/teststream_ff 也就是转码后反推了一路流到 www.transcode.com这个vhost下面（由于output可以自定义，所以实际上想推到哪里都可以）。 所以转码一旦开始，就会不断拉流，转码，推流，与有没有人在拉流没有关系，它会持续占用CPU资源。 如何实现srs可控的FFmpeg转码首先，如果在一个vhost下同时配置 remote 和 transcode，则会出来不先拉原始流就无法拉取转码流的问题： 123456789101112131415161718192021222324252627282930vhost lrm.test.com &#123; mode remote; origin 192.168.90.229:2019; transcode &#123; enabled on; ffmpeg ./objs/ffmpeg/bin/ffmpeg; engine ff &#123; enabled on; vfilter &#123; &#125; vcodec libx264; vbitrate 3500; vfps 60; vwidth 1920; vheight 1080; vthreads 2; vprofile baseline; vpreset superfast; vparams &#123; &#125; acodec libfdk_aac; abitrate 45; asample_rate 44100; achannels 2; aparams &#123; &#125; output rtmp://127.0.0.1:[port]/[app]?vhost=[vhost]/[stream]_[engine]; &#125; &#125;&#125; 如上面的配置，如果不先拉取rtmp://lrm.test.com/app/streamname的未转码流，则无法拉取rtmp://lrm.test.com/app/streamname_ff的转码流， 并且只要转码一旦开始，就不会停止。 所以，如果想要实现可控转码，则只能通过需要转码时，再下发转码配置来实现，只通过srs本身来控制貌似不行。 按需转码实现思路： 如何知道有用户拉流：用户拉流触发srs的HTTPCallBack -&gt; 开启转码。 如何开启转码：在用于转码的srs节点上下发ingest配置从origin拉取原始流 -&gt; 进行转码后将转码流推回origin。 所以这里的关键在于两点：使用HTTPCallBack来获知拉流信息;使用ingest来拉取原流进行转码，转码操作不会影响srs集群的任何配置。 ingest配置一： 1234567891011121314151617181920212223242526272829303132333435vhost lrm.test.com &#123; ingest livestream &#123; enabled on; input &#123; type stream; url rtmp://192.168.90.229:2019/persist?vhost=lrm.test.com/long; &#125; ffmpeg ./objs/ffmpeg/bin/ffmpeg; engine 720_60 &#123; enabled on; vfilter &#123; &#125; vcodec libx264; vbitrate 3500; vfps 60; vwidth 0; vheight 0; vthreads 12; vprofile main; vpreset fast; vparams &#123; maxrate:v 3500k; minrate:v 3500k; bufsize:v 3500k; &#125; acodec libfdk_aac; abitrate 50; asample_rate 44100; achannels 2; aparams &#123; &#125; output rtmp://192.168.90.229:2019/persist?vhost=lrm.test.com/long_720_60; &#125; &#125;&#125; ingest配置二： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384vhost lrm.test.com &#123; ingest livestream &#123; enabled on; input &#123; type stream; url rtmp://192.168.90.229:2019/persist?vhost=lrm.test.com/long; &#125; ffmpeg ./objs/ffmpeg/bin/ffmpeg; engine 1 &#123; enabled off; output rtmp://127.0.0.1:2019/persist?vhost=www.transcode.com/long; &#125; engine 2 &#123; enabled off; output rtmp://127.0.0.1:2019/persist?vhost=www.transcode2.com/long; &#125; &#125;&#125;vhost www.transcode.com &#123; transcode &#123; enabled off; ffmpeg ./objs/ffmpeg/bin/ffmpeg; engine 720_60 &#123; enabled on; vfilter &#123; &#125; vcodec libx264; vbitrate 3500; vfps 60; vwidth 0; vheight 0; vthreads 12; vprofile main; vpreset fast; vparams &#123; maxrate:v 3500k; minrate:v 3500k; bufsize:v 3500k; &#125; acodec libfdk_aac; abitrate 50; asample_rate 44100; achannels 2; aparams &#123; &#125; output rtmp://192.168.90.229:2019/persist?vhost=lrm.test.com/long_720_60; &#125; &#125;&#125;vhost www.transcode2.com &#123; transcode &#123; enabled on; ffmpeg ./objs/ffmpeg/bin/ffmpeg; engine 720_60_1 &#123; enabled on; vfilter &#123; filter_complex &apos;scale=1280:-1&apos;; &#125; vcodec libx264; vbitrate 3500; vfps 60; vwidth 0; vheight 0; vthreads 12; vprofile main; vpreset fast; vparams &#123; maxrate:v 3500k; minrate:v 3500k; bufsize:v 3500k; &#125; acodec libfdk_aac; abitrate 50; asample_rate 44100; achannels 2; aparams &#123; &#125; output rtmp://192.168.90.229:2019/persist?vhost=lrm.test.com/long_720_60_1; &#125; &#125;&#125; 虽然两种方式其实可以达到一样的效果（方式一中也可以添加多个engine），哪种方便用哪一种。]]></content>
      <categories>
        <category>live</category>
      </categories>
      <tags>
        <tag>live</tag>
        <tag>srs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[golang中unsafe.Pointer的入门使用]]></title>
    <url>%2F2019%2F09%2F30%2F2019-09-30-golang-unsafe_pointer%2F</url>
    <content type="text"><![CDATA[概述在做316. Remove Duplicate Letters遇到了[]byte转string的问题， 发现可以通过unsafe.Pointer来完成转换，由此产生了对golang中指针操作的兴趣。 []byte转string首先不通过指针直接转： 12345func main() &#123; var b []byte = []byte&#123;'a', 'b', 'c'&#125; var str string = string(b) fmt.Println(str)&#125; abc 可以看到直接转是可以的，但是直接转整个[]byte会被复制一遍再给到string中，不是最高效率的做法。 通过指针来转： 注意如果直接转指针，go语言是不允许的： 12var b []byte = []byte&#123;'a', 'b', 'c'&#125;var str *string = (*string)(&amp;b) Cannot convert expression of type []byte to type string 要想进行指针转换，需要使用unsafe.Pointer来中继： 12345func main() &#123; var b []byte = []byte&#123;'a', 'b', 'c'&#125; var str string = *(*string)(unsafe.Pointer(&amp;b)) fmt.Println(str)&#125; abc 上面的代码先将[]byte的指针转化为unsafe包中的ArbitraryType类型，然后就可以转化为任意别的类型的指针， 这里就将它转化为*string类型的指针。 因为string和[]byte的底层的c语言结构为： 123456789101112struct String&#123; byte* str; intgo len;&#125;;struct Slice&#123; byte* array; uintgo len; uintgo cap;&#125;; 所以可以直接从切片[]byte转到string也毫无违和感。 这样string与切片[]byte共用一片内存，本来go的string是不可变的，这里竟然可以使得string内容可变： 12345678910111213141516171819func main() &#123; var b []byte = make([]byte, 3, 10) b[0] = 'a' b[1] = 'b' b[2] = 'c' var str *string = (*string)(unsafe.Pointer(&amp;b)) fmt.Println(*str) //// 直接append // b = append(b, 'd') // 先修改len，再赋值 var bp *int = (*int)(unsafe.Pointer(uintptr(unsafe.Pointer(&amp;b)) + unsafe.Sizeof(0))) *bp = 4 b[3] = 'd' fmt.Println(*str)&#125; abcabcd string转[]byte同理可以直接转，也可以通过指针转： 12345678func main() &#123; var str string = "abc" var b []byte = ([]byte)(str) fmt.Println(b) var buf []byte = *(*[]byte)(unsafe.Pointer(&amp;str)) fmt.Println(buf)&#125; [97 98 99][97 98 99] 注意到通过指针转化得到的[]byte切片是不能进行值得修的，因为str是不可修改的。 这里的buf的cap值是不确定的，因为string结构中只有两个属性，没有cap，有一种骚操作来转换： 1234567func main() &#123; var str string = "abc" xx := *(*[2]uintptr)(unsafe.Pointer(&amp;str)) var buf []byte = *(*[]byte)(unsafe.Pointer(&amp;[3]uintptr&#123;xx[0], xx[1], xx[1]&#125;)) fmt.Println(buf) fmt.Printf("%p %d %d\n", buf, len(buf), cap(buf))&#125; [97 98 99]0x4c66e4 3 3 这里把string当作一个[2]uintptr，通过它新建一个[3]uintptr，这样就得到了一个len和cap都为3的[]byte切片。 struct赋值例子如下： 12345678910111213141516type V struct &#123; i int32 j int64&#125;func main() &#123; var v *V = new(V) var i *int32 = (*int32)(unsafe.Pointer(v)) *i = 1 var j *int64 = (*int64)(unsafe.Pointer(uintptr(unsafe.Pointer(v)) + unsafe.Offsetof(v.j))) *j = 2 fmt.Println(*v)&#125; {1 2} 需要注意到go的struct与c语言一样有对齐机制，对于结构体V，它明显是按照int64也就是8字节对齐， 所以上面的代码取j的位置换一种写法也是一样的： 1var j *int64 = (*int64)(unsafe.Pointer(uintptr(unsafe.Pointer(v)) + uintptr(8))) 另外struct使用的是一块连续内存，可以看到下面的操作： 12345678910111213141516type V struct &#123; i int32 j int64&#125;func main() &#123; var v *V = new(V) var i *int32 = (*int32)(unsafe.Pointer(v)) *i = 1 var k *int64 = (*int64)(unsafe.Pointer(uintptr(unsafe.Pointer(v)) + uintptr(4))) *k = 3 &lt;&lt; 32 fmt.Println(*v)&#125; {1 3} 这里的 i，j，k 在内存中的位置是这样的： 所以*k = 3 &lt;&lt; 32，这句话相当于把第9个字节变成了1100 0000，需要注意到这里是小端序（数据的低字节保存在内存的低地址）。 所以j的值就变成了3。]]></content>
      <categories>
        <category>golang</category>
      </categories>
      <tags>
        <tag>golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rsyslog + Logrotate进行日志记录并切割压缩]]></title>
    <url>%2F2019%2F09%2F26%2F2019-09-26-rsyslog-logrotate%2F</url>
    <content type="text"><![CDATA[目标描述Golang程序将日志写入到rsyslog的LOCAL0中。 所以这里要使用rsyslog记录来自LOCAL0的日志，几点需求： 日志存储在/xxx/程序名/程序名.log下。 要对日志进行切割，压缩。 这里主要需要研究的功能就是： rsyslog的日志动态路径。 Logrotate的定期日志切割。 Logrotate的日志压缩。 另外： 这里系统为Centos7，不涉及远程日志记录。 关于rsyslog和Logrotate的介绍网上有很多： Linux rsyslog服务 Rsyslog日志系统 日志切割之Logrotate 高效的log工具：Logrotate rsyslog的配置 基本配置rsyslog的配置文件位于/etc/rsyslog.conf， 12345678910111213141516...# Log all the mail messages in one place.mail.* -/var/log/maillog# Log cron stuffcron.* /var/log/cron# Everybody gets emergency messages*.emerg :omusrmsg:*# Save news errors of level crit and higher in a special file.uucp,news.crit /var/log/spooler# Save boot messages also to boot.loglocal7.* /var/log/boot.log... 上面的配置中，所使用的*就表示通配，例如mail.*就表示来自mail的所有级别日志都记录到 -/var/log/maillog中。*.emerg则表示所有emerg级别以上的日志（其实emerg已经是最高级别）都发送给所有正在登陆的用户。 rsyslog有三种配置格式basic、advanced和obsolete legacy，它们可以在同一个配置文件中混用，但是官方建议尽量避免使用obsolete legacy配置格式， 因为这个不健康，官方原话：Do not use obsolete legacy format. It will make your life miserable. 123456789# basicmail.info /var/log/mail.log# advancedmail.info action(type="omfile" File="/var/log/mail.log")# legacy（下面例子的含义和上面不一样，它定义一个Template）# 总之一般带着 $ 的语句格式都是 legacy$template DynFile,"/var/log/%HOSTNAME%/%programname%.log" 这里如果要让自己的程序使用rsyslog来记录日志，那么就可以使用LOCAL0~6这些facility来进行我们的自定义日志记录， 配置日志动态路径例如一条在rsyslog接收到一条日志之后，希望它能将根据日志附带的tag信息， 将日志打印到 /logdir/tag/tag.log 文件，这就是动态日志路径。 要达到这个目的，使用rsyslog配置中的Templates语法结构即可。 Templates 这里可以使用Templates来定义一段字符串，字符串中可以带有变量，这样就可以达到日志记录位置随日志的tag信息而变化的目的。 修改 /etc/rsyslog.conf : 123template(name="MyDynFile" type="string" string="/var/log/%programname%/%programname%.log")local0.* action(type="omfile" dynaFile="MyDynFile") 上面的配置中，首先定义了一个string类型的Template，然后将local0的所有日志输入到这个路径下的日志文件中， 这里的%programname%即表示： the “static” part of the tag, as defined by BSD syslogd. For example, when TAG is “named[12345]”, programname is “named”. 也就是说这里将日志保存到/var/log/%programname%/%programname%.log路径之下，更多的字段可以参考rsyslog Properties。 测试配置： 首先使用命令看配置是否有语法错误： 1$ /usr/sbin/rsyslogd -f /etc/rsyslog.conf -N1 重启rsyslog服务： 1$ sudo systemctl restart rsyslog 测试配置是否达到预期： 1234$ logger -t test -p local0.info "hello world"$ cat /var/log/test/test.logSep 23 11:38:32 node1 test: hello world 可以看到这里动态日志路径配置成功。 Logrotate 配置上面完成了使用rsyslog来进行日志记录，但是如果不对日志进行切割压缩，日志的大小就会无限增长，不仅将来不好查询，而且占存储空间， 这里可以使用Logrotate来进行日志的切割与压缩。 Logrotate并不是一个一直运行的linux程序，它的自动运行是linux的计划任务cron来实现的，位于/etc/cron.daily/logrotate， 它每天执行一次。 而Logrotate的配置文件则位于/etc/logrotate.conf，这个配置文件里面又包含了目录/etc/logrotate.d/下的所有配置文件， 所有通常在/etc/logrotate.d/目录下来添加自定义的配置文件。 切割日志： 方法一：将原日志文件重命名，重新创建新的日志文件，通知使用此日志的进程使用新的日志文件。对应Logrotate中的create。 方法二：先将原日志文件复制，然后截断原文件，这样不需要通知使用此日志的进程，但两个操作之间有短暂的时间间隙，可能会丢失日志。对应Logrotate中的copytruncate。 方法三：只复制原日志文件。对应Logrotate中的copy。 这里之所以有三种切割日志的方法，原因在于当一个程序获取到一个文件句柄并向里面写入数据时，即使此时文件名发生了变化， 也不会影响之前的文件句柄的使用，程序仍然可以通过这个文件句柄写入数据到此文件中，如果不对原程序发出通知，让其重新获取文件句柄， 那么这个日志文件的大小就会继续增长。 压缩日志：经过日志切割，原日志文件已经重命名，已经没有进程再继续使用它，这时便可以进行任意的操作，想要压缩就可以直接进行压缩。 rotate：保留日志文件的数量（轮转数量）。例如rotate为3，切割出来日志文件为log.1、log.2和log.3，则下一次再进行切割时， 会将log.3删除，log.2重命名为log.3，log.1重命名为log.2，新切割出来的日志文件命名为log.1。 Logrotate的具体配置。这里添加一个新的配置文件到/etc/logrotate.d/目录下，内容如下： 123456789101112131415/var/log/*/*.log &#123; create 0644 root root // 新创建日志文件的权限 daily // 每天执行一次 rotate 65535 // 轮转数量 65535，基本等于存储所有日志 size 1M // 超过1M才进行切割 dateext // 使用日期作为后缀 dateformat -%Y%m%d.%s // 定义日期后缀的格式 missingok // 没有找到日志文件也OK notifempty // not ifempty 如过日志文件为空，则不切割 compress // 切割后进行压缩 sharedscripts // 脚本只执行一次 postrotate // 脚本，用于通知rsyslog使用新的日志文件 /bin/kill -HUP `cat /var/run/syslogd.pid 2&gt; /dev/null` 2&gt; /dev/null || true endscript&#125; 这里首先配置的路径为/var/log/*/*.log，因为上面rsyslog将LOCAL0的日志记录到了/var/log/%programname%/%programname%.log， 这样便能够通配到所有LOCAL0的日志文件。 注意到这里的最后几句配置： 123postrotate // 脚本，用于通知rsyslog使用新的日志文件 /bin/kill -HUP `cat /var/run/syslogd.pid 2&gt; /dev/null` 2&gt; /dev/null || trueendscript 其中夹在postrotate和endscript中间的就是脚本，postrotate表示脚本在rotate之后运行，也就是此时已经将原日志文件重命名， 并且创建了新日志文件，但是还没有对原日志文件进行压缩。脚本中使用kill -HUP来通知rsyslog使用新日志文件，脚本执行完毕后， Logrotate将对原日志文件使用gzip进行压缩。 这里切割出来的日志文件将保存在与原日志文件同一目录下，如果想要将切割出来的日志文件保存到别的目录，可以添加下面的配置： 1olddir /var/log/old 这样切割下来的日志文件就会存储在/var/log/old目录之下（old目录需要手动创建）。 Logrotate 日志压缩默认情况下，Logrotate使用gzip进行压缩，当然也可以配置其它的压缩工具。 如果要使用bzip2来进行压缩，则可以使用下面的配置： 1234compresscompresscmd /usr/bin/bzip2compressext .bz2compressoptions -9 那么在压缩日志时，就会使用/usr/bin/bzip2 -9来进行压缩。 使用bzip2可以达到更大的压缩比，但是在压缩过程中也会消耗更多的CPU。 同理也可以配置成其它的压缩工具。 Logrotate 定期执行Logrotate通过cron来定期运行，默认配置在/etc/cron.daily/logrotate，也就是默认每天执行一次。 使用下面命令可以让Logrotate立即执行某一配置，而不用等待计划任务： 12345678// 立即执行syslog配置$ logrotate /etc/logrotate.d/syslog// 使用debug模式执行，并不改变和生成任何日志文件$ logrotate -d /etc/logrotate.d/syslog// 强制执行，忽略size参数$ logrotate -f /etc/logrotate.d/syslog 如果想要更灵活的执行时间配置，就可以在cron中来添加，例如想要Logrotate每分钟执行一次，则可以在/etc/crontab中进行配置： 123456789101112131415161718$ sudo vim /etc/crontab$ cat /etc/crontabSHELL=/bin/bashPATH=/sbin:/bin:/usr/sbin:/usr/binMAILTO=root# For details see man 4 crontabs# Example of job definition:# .---------------- minute (0 - 59)# | .------------- hour (0 - 23)# | | .---------- day of month (1 - 31)# | | | .------- month (1 - 12) OR jan,feb,mar,apr ...# | | | | .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat# | | | | |# * * * * * user-name command to be executed*/1 * * * * root logrotate /etc/logrotate.d/syslog 这样就可以使得Logrotate每分钟执行一次。 配置总结rsyslog： 123$umask 0000template(name="MyDynFile" type="string" string="/data1/log/%programname%/%programname%.log")local0.* action(type="omfile" dynaFile="MyDynFile" dirCreateMode="0755" fileCreateMode="0644" ioBufferSize="64K") 这里的umask配合dirCreateMode和fileCreateMode来使用，这样便可指定创建出来的文件夹与文件的默认权限。 Logrotate： 123456789101112131415/data1/log/*/*.log &#123; create 0644 root root daily rotate 65535 size 1M dateext dateformat -%Y%m%d.%s missingok notifempty compress sharedscripts postrotate /bin/kill -HUP `cat /var/run/syslogd.pid 2&gt; /dev/null` 2&gt; /dev/null || true endscript&#125; 补充：docker内使用rsyslog + Logrotate今天要在docker中配置rsyslog + Logrotate，发现centos7的docker中没有自带rsyslog和cron，并且没有systemd，所以不能通过systemctl来操作服务。 首先需要安装rsyslog和cron： 1# yum install -y rsyslog cronie 非常重要的一点在于：rsyslog 默认通过 journal 读取日志信息，但CentOS镜像默认并未安装systemd和journald。 首先kill掉正在运行的rsyslog进程，然后修改配置： 注释$ModLoad imjournal 注释$IMJournalStateFile imjournal.state 将$OmitLocalLogging on改为$OmitLocalLogging off 将journal的配置删除：rm -rf /etc/rsyslog.d/listen.conf 启动rsyslog： 1# rsyslogd 启动cron： 1# crond 补充：修改rsyslog的日志记录格式首先可以看到默认的rsyslog的格式： 1Sep 30 17:21:49 localhost test: Hello World!!! 它所对应的配置格式是： 123template(name="FileFormat" type="string" string= "%TIMESTAMP% %HOSTNAME% %syslogtag%%msg:::sp-if-no-1st-sp%%msg:::drop-last-lf%\n" ) 可以看到其实有点儿丑，为了把它变得好看一点，需要新建一个template来定义它的格式。 官方文档中这部分定义的字段都可以使用: rsyslog Properties The Property Replacer 这里定义的template如下： 123456789101112131415161718192021222324252627282930template(name="MyOutFmt" type="list") &#123; constant(value="[") property(name="syslogseverity-text") constant(value="]") constant(value="[") property(name="timereported" dateformat="year") constant(value="-") property(name="timereported" dateformat="month") constant(value="-") property(name="timereported" dateformat="day") constant(value=" ") property(name="timereported" dateformat="hour") constant(value=":") property(name="timereported" dateformat="minute") constant(value=":") property(name="timereported" dateformat="second") constant(value=" ") property(name="timereported" dateformat="tzoffsdirection") property(name="timereported" dateformat="tzoffshour") property(name="timereported" dateformat="tzoffsmin") constant(value="]") constant(value="[") property(name="programname" position.from="1" position.to="32") constant(value="]") constant(value=": ") property(name="msg") constant(value="\n") &#125; 配置输出文件的template为MyOutFmt： 1local0.* action(type="omfile" dynaFile="MyDynFile" template="MyOutFmt" dirCreateMode="0755" fileCreateMode="0644" ioBufferSize="64K") 测试效果： 123456789$ logger -t test -p local0.info 'Hello World!!!'$ logger -t test -p local0.warning 'Hello World!!!'$ logger -t test -p local0.err 'Hello World!!!'$ tail -n3 /data1/log/test/test.log[warning][2019-09-30 18:13:25 +0800][test]: Hello World!!![err][2019-09-30 18:13:31 +0800][test]: Hello World!!![err][2019-09-30 18:14:19 +0800][test]: Hello World!!! ok，顺眼多了。 参考The rocket-fast Syslog Server logrotate(8) - Linux man page docker容器中使用rsyslogd]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>centos</tag>
        <tag>rsyslog</tag>
        <tag>logrotate</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[srs带ffmpeg的安装]]></title>
    <url>%2F2019%2F09%2F18%2F2019-09-18-srs-ffmpeg%2F</url>
    <content type="text"><![CDATA[概述主要记录一下如何成功在centos7下面编译安装带ffmpeg的srs，其中有一些小坑。 这里srs版本为2.0.263。 下载编译srs直接看github，https://github.com/ossrs/srs 很简单的几步： 123$ git clone https://github.com/ossrs/srs$ cd srs/trunk$ ./configure &amp;&amp; make 那么上面就完成了srs的编译，但是没有带ffmpeg。 带ffmpegsrs自带了ffmpeg，但是在编译的时候需要加上--with-ffmpeg，也就是： 1./configure --with-ffmpeg &amp;&amp; make 其中可能会报几个错，这里说一下我遇到的。 问题一： Found no assembler，提示缺少nasm。 去下一个安装就行，NASM-2.13.03 123456$ wget http://www.nasm.us/pub/nasm/releasebuilds/2.13.03/nasm-2.13.03.tar.xz$ tar -xf nasm-2.13.03.tar.xz$ mv nasm-2.13.03 nasm$ cd nasm$ ./configure --prefix=/usr &amp;&amp; make$ sudo make install 问题二： speex not found using pkg-config 提示找不到speex。 但是其实speex就在srs/trunk/objs/ffmpeg.src/speex-1.2rc1/下面，可以看到其中有speex.pc就没错。那么设置一下路径即可： 1$ export PKG_CONFIG_PATH=/xxx/srs/trunk/objs/ffmpeg.src/speex-1.2rc1/ 然后这个问题就解决了。 问题三： bzlib requested but not found 提示找不到bzlib，那么安装一下就行： 1$ sudo yum install -y bzip2-devel.x86_64 解决上面三个问题，应该就能成功编译带ffmpeg的srs了。]]></content>
      <categories>
        <category>live</category>
      </categories>
      <tags>
        <tag>live</tag>
        <tag>srs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang基础学习]]></title>
    <url>%2F2019%2F08%2F29%2F2019-08-29-golang-learn-1%2F</url>
    <content type="text"><![CDATA[前言众所周知Java里面是没有指针的，但是有自动垃圾回收机制。C++是有指针的，但是没有自动垃圾回收。 Golang是一种新的语言，很多地方的设计借鉴了以前的语言，例如它是有指针的，并且有自动垃圾回收机制。 （主要代表着我需要进行指针的学习…） Go语言基础学习这方面去看一些教程就好： 首先菜鸟教程，内容不多，比较简单和基础： Go 语言教程 | 菜鸟教程 然后可以看看知乎上是怎么推荐学习Go语言的： 系统学习GO，推荐几本靠谱的书? 然后还有官方的教程，有英文版的，也有中文版的，非常详细： Documentation - The Go Programming Language 文档 - Go 编程语言 当然看上面的东西估计很快就烦了，那么不想看上面的就直接看官方提供的这个，真是个有意思的东西: Go 语言之旅 牛顿法（练习：循环与函数）在教程练习：循环与函数中，要求我们实现一个求解平方根的函数， 方法就是使用牛顿法，代码如下： 123456789101112131415161718package mainimport ( "fmt")func Sqrt(x float64) float64 &#123; z := 1.0 for i := 0; i &lt; 10; i++ &#123; z -= (z*z - x) / (2 * z) fmt.Println(z) &#125; return z&#125;func main() &#123; fmt.Println(Sqrt(1123415))&#125; 输出为： 1234567891011561708280854.99999910983140429.4999906535470218.7499205564235117.3743550358217574.6823226651838819.3023364871264473.3418637848772362.2386879692661418.90543345168861418.9054334516886 可以看到通过牛顿法，基本在10次迭代以内就可以计算出十分精确的平方根。 牛顿法原理： 牛顿法可以用于求解方程的根，当一个方程的根不好直接求解时，就可以使用牛顿法来进行迭代求解。 对于一个方程，需要求解 $x$ 的值： f(x) = 0那么首先利用泰勒展开来对 $f(x)$ 进行展开，这里从 $x_0$ 进行展开，并且只展开到一阶： f(x) \approx f(x_0) + (x - x_0)f'(x_0)那么就可以使用这个近似式来求解原方程的近似解： f(x_0) + (x - x_0)f'(x_0) = 0 f(x_0) + xf'(x_0) - x_0 f'(x_0) = 0 xf'(x_0) = x_0f'(x_0) - f(x_0) x = x_0 - \frac{f(x_0)}{f'(x_0)}注意到 $f’(x_0)$ 就是原函数 $f(x)$ 在 $x_0$ 这点的斜率，那么其实在这一点上的切线方程就可以写为： y - f(x_0) = f'(x_0)(x - x_0)那么对于 $x = x_0 - \frac{f(x_0)}{f’(x_0)}$ 这一点，将它代入切线方程，求出它在切线上所对应的 $y$ ： y - f(x_0) = f'(x_0)(x_0 - \frac{f(x_0)}{f'(x_0)} - x_0) y - f(x_0) = -f(x_0) y = 0也就是对应切线上的点 $(x_0 - \frac{f(x_0)}{f’(x_0)}, 0)$ ，画出图可以看出对应如下： 从图中可以看到观察到，这里所需要求解的点为曲线与x轴所相交的那个点 $x$ （y为0，也就是 $f(x)=0$ ）。 虽然图中的 $x_{n+1}$ 并不是所需要求得点 $x$ ，但是它比 $x_n$ 更加接近 $x$ 。 所以这里就可以通过迭代来求解原方程的根，这一次从 $x_{n+1}$ 进行泰勒展开来求得下一个点。 回到练习题： 求解平方根，即方程 $x^2 = y$ 的解，即 $f(x) = x^2 - y$ ，代入上面的式子 $x = x_0 - \frac{f(x_0)}{f’(x_0)}$ ， 可以推出： x_{n+1} = x_n - \frac{x_n^2 - y}{2x_n} x_{n+1} = \frac{x_n^2 + y}{2x_n}题目中用的上面的形式，使用下面的形式其实也可以求解，但是两种方法的结果有细微的差别。 估计是考虑到溢出的问题吧，减法在这里可以保证更好的精度。 最大公约数与最小公倍数（练习题：最小众倍数）牛客-最小众倍数 最小众倍数：即是三个数的最小公倍数。 题目给了 a，b，c，d，e 一共5个数，求它们的最小众倍数中最小的那一个。 方法一：暴力搜索。复杂度等于搜索下限减去上限： 搜索下限：第3大的数。 搜索上限：前3小的数的乘积。 方法二：寻找所有三个数的组合的最小众倍数，取其中最小的那个。复杂度等于O(n^3)，n为输入数的个数，本题 n = 5。 如何求3个数a,b,c的最小众倍数？ 第一种求法： 求出b*c,a*c的最大公约数A; 求出a*c,a*b的最大公约数B; 求出A,B的最大公约数k; 使用(a*b*c) / k即得到结果。 这个想法就是，对于 $C = abc$ ： (bc)a=C \\ (ac)b=C \\ (ab)c=C假设最小众倍数为 $D$ ，那么 $C$ 一定是 $D$ 的整数倍 $k$，即 $C = kD$ ： (bc)a=kD \\ (ac)b=kD \\ (ab)c=kD也就是： \frac{(bc)}{k}a=D \\ \frac{(ac)}{k}b=D \\ \frac{(ab)}{k}c=D那么 $k$ 就是能同时整除 $bc,ac,ab$ 的最大的数，也就是它们的最大众约数， 也就可以通过 $bc,ac$ 的最大公约数 $A$ ， $ac,ab$ 的最大公约数B，再求 $A,B$ 的最大公约数 $k$ 即可： D = \frac{abc}{GCD(GCD(bc,ac), GCD(ac,ab))}两个数的最大公约数求法在之前的博客中有写LeetCode 149 Max Points on a Line 第二种求法： 求出a，b的最小公倍数A； 求出A，c的最小公倍数D； D就是a，b，c的最小众倍数。 首先，对于a，b的最小公倍数A： x_1 a = A \\ x_2 b = A然后对于A，c的最小公倍数D： x_3 A = D \\ x_4 c = D上面合在一起，即： x_1 x_3 a = D \\ x_2 x_3 b = D \\ x_4 c = D注意到由于最小公倍数的性质，$x_1$ 与 $x_2$ 互质，$x_3$ 与 $x_4$ 互质。 如果 $D$ 是a，b，c的最小众倍数，则 $x_1 x_3$ ，$x_2 x_3$，$x_4$ 的最大众约数为 1 。 这里使用反证，假设 $x_1 x_3$ ，$x_2 x_3$，$x_4$ 的最大众约数不为 1 。 由于$x_3$ 与 $x_4$ 互质，那么只能 $x_1$ 、 $x_2$ 与 $x_4$ 存在不为 1 的最大公约数 $S$ ： x_1 = k_1 S \\ x_2 = k_2 S \\ x_4 = k_3 S也就是 $x_1$ 与 $x_2$ 存在最大公约数 $S$ ，与原条件 $x_1$ 与 $x_2$ 互质相矛盾。 所以 $x_1 x_3$ ，$x_2 x_3$，$x_4$ 的最大众约数为 1 ，即 $D$ 是a，b，c的最小众倍数。 D = LCM(LCM(a, b), c)同理，使用公约数的解法可以改写为： D = \frac{abc}{GCD(GCD(ab,bc), ac)}对于两个数的最小公倍数： 对于a，b的最小公倍数A ： k_1 a = A \\ k_2 b = A注意 $k_1$ 与 $k_2$ 互质，假如a，b的最大公约数为 $S$ ： k_1 (h_1 S) = A \\ k_2 (h_2 S) = A注意到 $h_1$ 与 $h_2$ 互质，那么这里就可以推出： k_1 (h_1 S) = k_2 (h_2 S) \\ k_1 h_1 = k_2 h_2 \\ k_1= \frac{h_2}{h_1} k_2注意 $k_1$ 与 $k_2$ 互质，$h_1$ 与 $h_2$ 互质，所以 $\frac{h_2}{h_1}$ 必然等于 $\frac{k_1}{k_2}$ ，所以： k_1 = h_2 \\ k_2 = h_1所以： k_1 a = h_2 (h_1 S) = A \\ \frac{(h_1 S) (h_2 S)}{S} = A \\ \frac{ab}{S} = A \\ \frac{ab}{GCD(a, b)} = A即a，b的最小公倍数等于它们的乘积除以它们的最大公约数。 12345678910111213func LCM(a, b int) int &#123; return (a*b) / GCD(a, b)&#125;func GCD(a, b int) int &#123; var c int for b != 0 &#123; c = a % b a = b b = c &#125; return a&#125; 代码，求一个数组的最小众倍数： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package mainconst MinInt = int(^uint(0) &gt;&gt; 1)func zbsUseGCD(seq *[]int) int &#123; min := MinInt n := len(*seq) for i := 0; i &lt; n; i++ &#123; for j := i+1; j &lt; n; j++ &#123; for k := j+1; k &lt; n; k++ &#123; zbsTmp := findZBSUseGCD((*seq)[i], (*seq)[j], (*seq)[k]) if zbsTmp &lt; min &#123; min = zbsTmp &#125; &#125; &#125; &#125; return min&#125;func zbsUseLCM(seq *[]int) int &#123; min := MinInt n := len(*seq) for i := 0; i &lt; n; i++ &#123; for j := i+1; j &lt; n; j++ &#123; for k := j+1; k &lt; n; k++ &#123; zbsTmp := findZBSUseLCM((*seq)[i], (*seq)[j], (*seq)[k]) if zbsTmp &lt; min &#123; min = zbsTmp &#125; &#125; &#125; &#125; return min&#125;func findZBSUseGCD(a, b, c int) int &#123; return (a*b*c) / GCD(GCD(a*b, b*c), a*c)&#125;func findZBSUseLCM(a, b, c int) int &#123; return LCM(a, LCM(b, c))&#125;func LCM(a, b int) int &#123; return (a*b) / GCD(a, b)&#125;func GCD(a, b int) int &#123; var c int for b != 0 &#123; c = a % b a = b b = c &#125; return a&#125; 练习：Web 爬虫 修改 Crawl 函数来并行地抓取 URL，并且保证不重复。 使用map和互斥锁来保证URL的唯一抓取。 使用管道或者sync.WaitGroup来实现主线程等待子线程结束。 代码一： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109package mainimport ( "fmt" "sync" "time")type Fetcher interface &#123; // Fetch 返回 URL 的 body 内容，并且将在这个页面上找到的 URL 放到一个 slice 中。 Fetch(url string) (body string, urls []string, err error)&#125;type SafeCounter struct &#123; v map[string]int mux sync.Mutex&#125;// Inc 增加给定 key 的计数器的值。func (c *SafeCounter) set(key string) bool &#123; c.mux.Lock() defer c.mux.Unlock() // Lock 之后同一时刻只有一个 goroutine 能访问 c.v if c.v[key] == 0 &#123; c.v[key]++ return true &#125; else &#123; return false &#125;&#125;// Crawl 使用 fetcher 从某个 URL 开始递归的爬取页面，直到达到最大深度。func Crawl(url string, sc *SafeCounter, fetcher Fetcher, ch chan int) &#123; if !sc.set(url) &#123; ch &lt;- 1 return &#125; body, urls, err := fetcher.Fetch(url) if err != nil &#123; fmt.Println(err) ch &lt;- 1 return &#125; fmt.Printf("found: %s %q\n", url, body) for _, u := range urls &#123; go Crawl(u, sc, fetcher, ch) &#125; for range urls &#123; &lt;-ch &#125; ch &lt;- 1&#125;func main() &#123; for i := 0; i &lt; 100; i++ &#123; fmt.Println("-------------------------") Crawl("https://golang.org/", &amp;SafeCounter&#123;v: make(map[string]int)&#125;, fetcher, make(chan int)) time.Sleep(time.Millisecond * 50) &#125;&#125;// fakeFetcher 是返回若干结果的 Fetcher。type fakeFetcher map[string]*fakeResulttype fakeResult struct &#123; body string urls []string&#125;func (f fakeFetcher) Fetch(url string) (string, []string, error) &#123; if res, ok := f[url]; ok &#123; return res.body, res.urls, nil &#125; return "", nil, fmt.Errorf("not found: %s", url)&#125;// fetcher 是填充后的 fakeFetcher。var fetcher = fakeFetcher&#123; "https://golang.org/": &amp;fakeResult&#123; "The Go Programming Language", []string&#123; "https://golang.org/pkg/", "https://golang.org/cmd/", &#125;, &#125;, "https://golang.org/pkg/": &amp;fakeResult&#123; "Packages", []string&#123; "https://golang.org/", "https://golang.org/cmd/", "https://golang.org/pkg/fmt/", "https://golang.org/pkg/os/", &#125;, &#125;, "https://golang.org/pkg/fmt/": &amp;fakeResult&#123; "Package fmt", []string&#123; "https://golang.org/", "https://golang.org/pkg/", &#125;, &#125;, "https://golang.org/pkg/os/": &amp;fakeResult&#123; "Package os", []string&#123; "https://golang.org/", "https://golang.org/pkg/", &#125;, &#125;,&#125; 这里实现了SafeCounter结构体来实现线程安全的map，并在set函数中使用了defer来将解锁放到return之后。 Crawl函数使用了管道来让主线程等待子线程结束，但是可以看到这样写十分复杂，每个return之前都需要记得向管道写入。 可以使用下面方法简化： 12345678910111213141516171819202122func WriteToChan(ch chan int) &#123; ch &lt;- 1&#125;func Crawl(url string, sc *SafeCounter, fetcher Fetcher, ch chan int) &#123; defer WriteToChan(ch) if !sc.set(url) &#123; return &#125; body, urls, err := fetcher.Fetch(url) if err != nil &#123; fmt.Println(err) return &#125; fmt.Printf("found: %s %q\n", url, body) for _, u := range urls &#123; go Crawl(u, sc, fetcher, ch) &#125; for range urls &#123; &lt;-ch &#125;&#125; 这里编写函数WriteToChan，然后使用defer来实现函数结束时往管道写入。 当然也可以使用sync.WaitGroup： 123456789101112131415161718func Crawl(url string, sc *SafeCounter, fetcher Fetcher, ch chan int, wg *sync.WaitGroup) &#123; defer wg.Done() if !sc.set(url) &#123; return &#125; body, urls, err := fetcher.Fetch(url) if err != nil &#123; fmt.Println(err) return &#125; fmt.Printf("found: %s %q\n", url, body) var wgNext sync.WaitGroup wgNext.Add(len(urls)) for _, u := range urls &#123; go Crawl(u, sc, fetcher, ch, &amp;wgNext) &#125; wgNext.Wait()&#125; 素数求解素数或者质数：大于1，并且只能被1和自身整除的数，例如：2，3，5，7… 判断一个数 $n$ 是不是素数，可以按照定义： 方法一：从2开始判断是否能够整除，直到 $\sqrt{n}$ 。 方法二：如果比 $n$ 小的数所构成的合数中没有 $n$，那么 $n$ 是素数。 问题一：如何快速求解[0,n]范围内的素数。 如果n较大的时候，使用方法一复杂度也较大，这时使用方法二会更快： 1234567891011121314151617181920212223242526272829303132package mainimport "fmt"const MAX = 500000var primes [MAX]intvar isNotPrime [MAX]boolvar idx int = 0func findPrime(n int) &#123; isNotPrime[0] = true isNotPrime[1] = true idx = 0 for i := 2; i &lt;= n; i++ &#123; if !isNotPrime[i] &#123; primes[idx] = i idx++ &#125; // 对于每个数 i ，将 i*2, i*3, i*4 ... 排除 for j := i*2; j &lt;= n; j+=i &#123; isNotPrime[j] = true &#125; &#125;&#125;func main() &#123; findPrime(20) for i := 0; i &lt; idx; i++ &#123; fmt.Println(primes[i]) &#125;&#125; 当然，这样的写法会有很多重复筛选，例如2*3，3*2都会被统计，但是其实没有必要。 另外，还有重复类似 4*2, 4*3, 4*4 ... 其实就是 2*4, 2*6, 2*8 ...，也就是说对于不是素数的数， 筛选它们的倍数是重复的。更进一步，这里只需要筛选素数的倍数就可以了。 按照上面所述，可以改写如下： 1234567891011121314func findPrime(n int) &#123; isNotPrime[0] = true isNotPrime[1] = true idx = 0 for i := 2; i &lt;= n; i++ &#123; if !isNotPrime[i] &#123; primes[idx] = i idx++ &#125; for j, bound := 0, n/i + 1; j &lt; idx &amp;&amp; primes[j] &lt; bound; j++ &#123; isNotPrime[primes[j]*i] = true &#125; &#125;&#125; 相当于这里只统计： 2*2，2*3，2*4 … 3*3，3*4，3*5 … 5*5，5*6，5*7 … 当然这里还是会出现重复，例如2*9和3*6，但是已经少了很多重复的筛选了。 更进一步，快速筛选法： 1234567891011121314151617func findPrime(n int) &#123; isNotPrime[0] = true isNotPrime[1] = true idx = 0 for i := 2; i &lt;= n; i++ &#123; if !isNotPrime[i] &#123; primes[idx] = i idx++ &#125; for j, bound := 0, n/i + 1; j &lt; idx &amp;&amp; primes[j] &lt; bound; j++ &#123; isNotPrime[primes[j]*i] = true if i%primes[j] == 0 &#123; break &#125; &#125; &#125;&#125; 这样在筛选过程中，可以不出现任何一个重复。 那么为什么这样可以不出现任何一个重复，且筛选出所有合数呢？ 首先每一个合数都可以以唯一形式被写成质数的乘积，即分解质因数，那么只要是不同的质数组合，得到的肯定是不同的合数， 例如合数 $A_1 = a \times b \times c$ ，$A_2 = d \times e \times f$，假设 $a \leq b \leq c ; d \leq e \leq f$， 那么只有在 a == d &amp;&amp; b == e &amp;&amp; c == f 时， $A_1$ 等于 $A_2$。 那么在给定一个素数集合 {a, b, ..., n} ，就可以使用下面的方法来组合出所有这个集合所能表示的合数，且不发生重复： 图中的集合为{2, 3, 5, 7}，左边是包含2的所有合数的组合方法，右边是其中 2,3,3 这条路线的样子。 可以看到，第一层生成 {2,2},{2,3},{2,5},{2,7}，对于后面的三个集合 {2,3},{2,5},{2,7} 来说，它们向下的路线无法再经过 2 ， 所有它们所生成的集合中，永远不可能包含子集 {2,2}，所以 {2,2} 与 {2,3},{2,5},{2,7} 向下所得到的集合永远不会相同， 同理可以知道 {2,2},{2,3},{2,5},{2,7} 互相之间向下不可能生成相同集合，也就保证了不会重复。 至于覆盖所有合数，则是显然的，因为任何合数都可以表示为图中一条路线。 当然，另一种组合顺序也是一样的，也是快速筛选法中所用的： 也就是刚才是限制向左，这里是限制向右，不过效果都是一样的。 为什么说算法中使用的就是这种组合方式呢？我们来跑100以内素数，打印一下输出就知道了： 1234567891011121314151617181920i = 2primes = 2 i = 3primes = 2 3 i = 4 // 2 x 2primes = 2 i = 5primes = 2 3 5 i = 6 // 3 x 2primes = 2 i = 7primes = 2 3 5 7 i = 8 // 2 x 2 x 2primes = 2 i = 9 // 3 x 3primes = 2 3 i = 10 // 5 x 2primes = 2 ... 可以看到，输出完全符合预期（因为本来就是按这个组合方法写的…）： 当i = 2，那么由于限制往右，那么向下只能再组合出 {2,2}； 当i = 3，那么向下只能再组合出 {2,2}，{3,2}； 当i = 6，其实就是组合 {3,2}，相当于图中的 3 走到 2，那么再向下也只有 2 能选择了，所以只能组合出 {3,2,2}； … 所以这里的 i%primes[j] == 0 其实就是在判断 primes[j] 是不是 i 的最小因数，一旦判断为 true，就相当于到达了右边界， 所以进行 break。 所以这样的方法一定不会出现任何重复的筛选。 至于包含所有合数，那也是明显的，对于合数 $A = a \times b \times … \times m \times n$ ，假设 $ a \geq b \geq … \geq m \geq n$， 那么一定会先遍历到 $i = a \times b \times … \times m$ ，又因为 $i$ 向下组合的右边界为 $m$ ，且 $m \geq n$ ，所以，一定会由 $i$ 筛选出 $A$， 所以这里一定不会漏掉任何一个合数。 证明完毕。]]></content>
      <categories>
        <category>golang</category>
      </categories>
      <tags>
        <tag>golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[云改讲坛和部门业务培训记录]]></title>
    <url>%2F2019%2F08%2F23%2F2019-08-23-cloud-workgroup%2F</url>
    <content type="text"><![CDATA[云下面围绕三个部分进行记录: 云是什么？ 云的当前市场状态。 云的未来变化趋势。 云是什么？云起源于虚拟化技术。虚拟化技术实现了把一台物理机虚拟成多台服务器的能力，由于这个能力，那么就可以在一台物理机上虚拟出多个虚拟机来同时提供给多个用户使用，它们共享物理机的资源。既然这样，就可以将多台物理机统一管理起来，这样就可以在同一时间提供出更多的虚拟机来给用户使用。更进一步，可以为用户提供配套的管理系统，方便用户使用，这样就形成了一朵云。 从内部来看，云是一个个服务器组成的。从外部来看，云是一个具有强大的计算能力、存储能力和网络能力的整体。 云的类别划分 一般业界通过云的共享程度来对云的类别进行划分，通常划分为公有云、专属云和私有云。从图中可以看到它们的共享程度逐渐降低，公有云几乎全部都是与他人共享的，而私有云几乎全都是独享的。 混合云，从名字就可以看出，它是几种云的混合，也许某个用户一部分业务需要部署到私有云，而另一部分业务放在公有云就可以（毕竟私有云要比公有云贵），那么混合云就可以帮他解决这个问题。 不同的云的类别总是适合不同的用户，不同的类别也就相当于不同的产品类型。 三类服务形态谈云的时候，常常会说I层、P层和S层，即Iaas、PaaS和SaaS，这也就是云的三类服务形态。 IaaS：基础设施即服务。服务的功能就类似于用户自己租了一台或者多台物理服务器，至于这些服务器用户想要怎么操作和管理，这里都不关心。当然硬件的管理不需要用户操心。 PaaS：平台即服务。在I层的基础上，提供了操作系统、中间件以及运行库等，也就是说其它的东西都准备好了，用户直接往上面部署应用就行。 SaaS：软件即服务。在PaaS的基础上，甚至连软件都以及部署好了，用户相当于买了一个已经正常运行了的软件，直接使用就行。 云的商业模式云既然是一件商品，那么它就需要吸引到客户，需要能够赚钱。 吸引客户的原因： 快捷，低门槛。对一个小公司或初创公司，相比于直接买几台云主机，维护一个机房费时也费力。 按需，低成本。需要增加计算能力的时候，多买几台云主机或者扩容就行，不需要的时候再退掉，不仅节约钱，而且方便。 能够赚钱的条件和原因： 到达一定规模是前提。第一，每台机器的平均管理成本就会降低。第二，能够更好的复用和超售，同一台机器，不同的时间可以提供给不同的客户使用，同一个服务，也可以卖给更多的用户。 交叉补贴和价值填充。在卖A服务的同时，将服务B一同卖出，或是卖服务B时，将服务A一同卖出，也就是将商品绑在一起卖。 当然，实际上到现在为止，各家的公有云和私有云都不赚钱或者不怎么赚钱，对于公有云来说，赚钱的关键在于规模；对于私有云，它的商业模式就决定了大部分操作留给了用户，而且还不能超售，所以难以赚钱。 云的当前市场状态 云计算的市场最开始是百家争鸣，但是到今年为止，市场格局逐渐清晰。正如上面所说，云要获得盈利的一个重要前提在于规模，那么规模正是一个此消彼长的东西。在图2-1中可以看到，阿里云占领了整个市场的近半壁江山，而紧随其后的就是腾讯云和天翼云。 这里引用《中国公有云服务市场（2018下半年）跟踪》中所述，当前的市场状态如下： 云产品迭代创新快：2018年公有云产品发布主要围绕高性能计算、异构计算、容器和无服务器、机器学习和机器人应用、IoT平台、自研云原生数据库、混合云七大关键词展开。 云与智能的强融合：智能云、云智能已然成为主要公有云服务商的未来战略，不仅体现在组织结构调整上，还体现在产品和服务研发上。基于云上的一体化的、使用便捷的AI服务能力成为公有云服务商比拼的重要方向。 混合云成发展常态：企业用户的多样化需求、公有云服务商的多元化发展，使得混合云部署、管理和运维等逐渐发展为一个初具规模的专业化市场，并对越来越多传统的信息技术服务商、增值开发商等体现出强烈的吸引力。 行业探索逐渐落地：虽然中国互联网行业的云计算应用已经相对成熟，但广大的非互联网行业对公有云的探索才开始不久。阿里、腾讯、华为、百度、浪潮等纷纷结合自身优势，明确了重点行业纵深发展战略，并借力合作生态布局不断拓展细分行业市场。 云的未来变化趋势首先，云本身从关注基础设施层逐渐过渡到开发部署运维，首先随着云越来越成熟，关注点逐渐从下层向上转移是一件很自然的事，其次正如上面所说的“交叉补贴和价值填充”，更多的增值服务才能赚到钱。 其次，现在迅速发展的IoT，以及马上就要广泛使用的5G，它们的结合正好需要“边缘计算”的能力。目前各家云厂商都已经开始了这方面的布局。 混合云、多云管理将成为主流。预计到2021年，使用云的组织中的75%将部署多云或混合云模型，所以一定要注意这方面产品的规划与发展。 云与大数据的结合成为新常态。云能够解决传统大数据的一些痛点，例如汇聚难、整合难和标记难等，云的数据分析能力和AI能力正好能够解决这些问题。 ABC的全面融合。ABC即AI、大数据和云计算，现在其实ABC已经不是一个割裂的不同概念了，通常将它们三个放在一起来讨论，由大数据和云计算所支撑起来的AI服务会是一个未来的卖点。 部门业务培训我所在的组：媒体存储组。 部门目前的主要业务：存储业务（也包含视频点播转码等）。 所包含的能力： 12345678---- 存储能力（Ceph集群）---- 文件上传（S3，CephFS，iSCSI） ---- 普通文件上传 ---- 视频文件上传 ---- 视频转码（FFmpeg）---- 文件下载（S3，CephFS，iSCSI） ---- 普通文件下载 ---- 视频文件点播（nginx） 下面从底向上来记录培训的内容： Linux相关。 Ceph相关。 视频转码与点播。 TCP/IP中的TCP部分。 所有得培训记录都在下面（我竟然写了12篇博客…）： linux基础练习-1 linux基础练习-2 centos7上的ceph指定版本安装方法 ceph-13.2.5删除osd，新建osd ceph-13.2.5添加mon，删除mon（ceph-deploy） centos7中lvm分区的创建与扩展 ceph-mimic-13.2.5中s3的初步使用 ceph-mimic-13.2.5中CephFS的初步使用 ceph+samba的简单使用 ceph+iscsi的简单使用 练习使用FFmpeg将视频转码为hls，并添加水印 TCP/IP详解卷1：协议 第19、20、21章笔记]]></content>
      <categories>
        <category>company</category>
      </categories>
      <tags>
        <tag>report</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线段树（Segment Tree）进阶使用记录（HDU3397）]]></title>
    <url>%2F2019%2F08%2F18%2F2019-08-21-segment-tree%2F</url>
    <content type="text"><![CDATA[线段树的进阶在前一篇博客中线段树（Segment Tree）使用记录，介绍了基础的线段树形式。 这一篇讨论以下进阶的使用，主要针对lazy标志。 上一篇博客中对于一个区间的修改只是单点修改，但是有时候会碰到区间修改的情况，这时基本的线段树可能就不适用了。 这时就需要针对区间修改来对区间树的更新方式进行变化。 下面先假设问题为，初始给定一个数组： 修改操作包括：区间加上一个数，或者区间减去一个数。 查询操作包括：区间的求和。 建树上一篇博客中使用的是至底向上的建树方式，这样的建树方法可以最大化空间利用率（2n空间即可），但是这样会造成一些处理上的困难，如图： 图中点1、2其实在整颗树的最右边，而3、4、5、6却在左边，这样的节点排列方式对于区间修改是不利的（会造成逻辑上的混乱）。 所以可以使用至顶向下的建树方式，也就是递归建树： 12345678910private void buildTree(int l, int r, int node, int[] arr) &#123; if(l == r) &#123; table[node] = arr[l]; return; &#125; int mid = (l + r) &gt;&gt; 1; buildTree(l, mid, left(node), arr); buildTree(mid+1, r, right(node), arr); pushUp(node);&#125; 将区间的左边放到左子树，右边放到右子树来进行递归建树，注意在建立完毕左右子树之后更新本节点信息（pushUp(node)）。 可以看到上面就是至顶向下所建立的树的结构，节点序号从左向右排列，但是这样带来的问题就是增加了空间占用（上图中数组大小14）。 12345678910public SegmentTreeLazySum(int n, int[] arr) &#123; this.n = n; int i = 1; while (i &lt; n) &#123; i = i &lt;&lt; 1; &#125; table = new int[2*i]; lazy = new int[2*i]; buildTree(0, n-1, 1, arr);&#125; 可以使用上面的方式来确定所需数组大小，也就是假如最后一层需要能放下n个元素，最小的i使得$2^i &gt; n$。 lazy标记在进行区间修改时，我们不可能像单点修改一样，将所有节点的值都修改，因为在查询时，可能只需要上层节点的信息就可以完成查询。 例如将整个数组所有点都增加1，然后询问整个数组的求和，这时我们只需要在根节点上之前所记录的求和加上整个数组的长度即可。 这样的思想就是为了降低算法复杂度，对于一个区间的修改，我们先欠着，当必要的时候才进行修改。 update方法的代码如下： 123456789101112131415161718// [L, R]为原始更新区间，[x, y]为当前节点node所包含的区间// t为操作类型，c为操作数public void update(int t, int L, int R, int c, int x, int y, int node) &#123; if ( y &lt; L || x &gt; R ) &#123; return; &#125; if (L &lt;= x &amp;&amp; y &lt;= R) &#123; c = t == 0 ? c : -c; table[node] += (y - x + 1) * c; lazy[node] += c; return; &#125; lazyDown(node, y - x + 1); int mid = (x + y) &gt;&gt; 1; update(t, L, R, c, x, mid, left(node)); update(t, L, R, c, mid+1, y, right(node)); pushUp(node);&#125; 这里lazy标记就代表需要加上的数（使用正负来代表原始的加减）。 可以看到： 当发现整个子区间都包含在更新区间中时，就可以停止更新下传，更新节点值与lazy标记即可。 当子区间部分包含在更新区间中时，就需要下传更新，那么此时就需要先将之前的lazy标记给下传了。 完成左右儿子的更新后，记得更新本节点（pushUp(node)）。 那么这里就涉及到了lazyDown函数，这个函数根据不同的情况会有很大的变化，这里因为只涉及加减法，所以比较简单： 1234567891011private void lazyDown(int node, int len) &#123; if (lazy[node] == 0) &#123; return; &#125; int l = left(node), r = right(node); lazy[l] += lazy[node]; table[l] += lazy[node] * (len - (len &gt;&gt; 1)); lazy[r] += lazy[node]; table[r] += lazy[node] * (len &gt;&gt; 1); lazy[node] = 0;&#125; 因为只是加减法，所以子节点的lazy标记单纯加上父节点的lazy标记即可，当然同时也要记得更新子节点的值。 查询操作查询操作就递归向下即可，当然记得需要下传lazy标记： 12345678910111213public int query(int L, int R, int x, int y, int node) &#123; if ( y &lt; L || x &gt; R ) &#123; return 0; &#125; if (L &lt;= x &amp;&amp; y &lt;= R) &#123; return table[node]; &#125; lazyDown(node, y - x + 1); int res = 0, mid = (x + y) &gt;&gt; 1; res += query(L, R, x, mid, left(node)); res += query(L, R,mid+1, y, right(node)); return res;&#125; 完整代码注意这里所针对的问题，初始给定一个数组： 修改操作包括：区间加上一个数，或者区间减去一个数。 查询操作包括：区间的求和。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125public class SegmentTreeLazySum &#123; private int[] table; private int[] lazy; private int n; public SegmentTreeLazySum(int n, int[] arr) &#123; this.n = n; int i = 1; while (i &lt; n) &#123; i = i &lt;&lt; 1; &#125; table = new int[2*i]; lazy = new int[2*i]; buildTree(0, n-1, 1, arr); &#125; private void buildTree(int l, int r, int node, int[] arr) &#123; if(l == r) &#123; table[node] = arr[l]; return; &#125; int mid = (l + r) &gt;&gt; 1; buildTree(l, mid, left(node), arr); buildTree(mid+1, r, right(node), arr); pushUp(node); &#125; private void pushUp(int node) &#123; int l = left(node), r = right(node); table[node] = table[l] + table[r]; &#125; private void lazyDown(int node, int len) &#123; if (lazy[node] == 0) &#123; return; &#125; int l = left(node), r = right(node); lazy[l] += lazy[node]; table[l] += lazy[node] * (len - (len &gt;&gt; 1)); lazy[r] += lazy[node]; table[r] += lazy[node] * (len &gt;&gt; 1); lazy[node] = 0; &#125; public void update(int t, int L, int R, int c, int x, int y, int node) &#123; if ( y &lt; L || x &gt; R ) &#123; return; &#125; if (L &lt;= x &amp;&amp; y &lt;= R) &#123; c = t == 0 ? c : -c; table[node] += (y - x + 1) * c; lazy[node] += c; return; &#125; lazyDown(node, y - x + 1); int mid = (x + y) &gt;&gt; 1; update(t, L, R, c, x, mid, left(node)); update(t, L, R, c, mid+1, y, right(node)); pushUp(node); &#125; public int query(int L, int R, int x, int y, int node) &#123; if ( y &lt; L || x &gt; R ) &#123; return 0; &#125; if (L &lt;= x &amp;&amp; y &lt;= R) &#123; return table[node]; &#125; lazyDown(node, y - x + 1); int res = 0, mid = (x + y) &gt;&gt; 1; res += query(L, R, x, mid, left(node)); res += query(L, R,mid+1, y, right(node)); return res; &#125; private int left(int idx) &#123; return idx &lt;&lt; 1; &#125; private int right(int idx) &#123; return (idx &lt;&lt; 1) + 1; &#125; // 测试用例： // 输入： // 1 // 5 5 // 1 1 1 1 1 // 2 2 4 7 // 1 1 3 4 // 0 0 4 2 // 1 1 4 8 // 2 2 4 3 // // 输出： // 3 // -23 public static void main(String[] args) &#123; Scanner sc = new Scanner(System.in); int T = sc.nextInt(); for (int k = 0; k &lt; T; k++) &#123; int n, m, t, x, y, c; n = sc.nextInt(); m = sc.nextInt(); int[] arr = new int[n]; for (int i = 0; i &lt; n; i++) &#123; arr[i] = sc.nextInt(); &#125; SegmentTreeLazySum hdu = new SegmentTreeLazySum(n, arr); for (int i = 0; i &lt; m; i++) &#123; t = sc.nextInt(); x = sc.nextInt(); y = sc.nextInt(); c = sc.nextInt(); if (t == 2) &#123; System.out.println(hdu.query(x, y, 0, n - 1, 1)); &#125; else &#123; hdu.update(t, x, y, c,0, n - 1, 1); &#125; &#125; &#125; &#125;&#125; HDU3397这个题目是一个比较典型的线段树题，初始给定一个数组，操作包括： 0：将区间[x, y]全部置为0； 1：将区间[x, y]全部置为1； 2：将区间[x, y]中的1变为0，0变为1； 3：查询区间[x, y]中1的数量； 4：查询区间[x, y]中连续出现1的最多的次数。 这道题的难点一在于查询4，因为在一个节点上，我们需要通过它的两个子节点的信息来得到连续1的数量。 对于这个问题，可以考虑这样来解决，在一个节点上，我们保存如下信息： 贴着区间左边的连续1的数量LLen。 贴着区间右边的连续1的数量RLen。 区间中的最大连续1的数量MLen。 那么对于一个节点，它的相关信息可以这样计算得到： LLen：等于左儿子的LLen。但是需要注意，如果左儿子的LLen等于整个区间的长度，那么就为左儿子的LLen加上右儿子的LLen。 RLen：同上。 MLen：等于 左儿子的MLen，右儿子的MLen，左儿子的RLen加上右儿子的LLen 的最大值。 这道题的难点二在于操作2，如何在一个节点上完成信息的更新，主要是LLen、RLen、MLen信息的变化？ 为了完成这个件事，这里对称的将连续0的数量保存下来ZLLen、ZRLen、ZMLen，这样在进行操作2时， 就可以将LLen、RLen、MLen和ZLLen、ZRLen、ZMLen的信息交换即可。 整个代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213import java.util.Arrays;import java.util.Scanner;public class HDU3397 &#123; private int[] table; private int[] Ztable; private int[] MLen; private int[] LLen; private int[] RLen; private int[] ZLLen; private int[] ZRLen; private int[] ZMLen; private int[] lazy; private int n; public HDU3397(int n, int[] arr) &#123; this.n = n; int i = 1; while (i &lt; n) &#123; i = i &lt;&lt; 1; &#125; table = new int[2*i]; Ztable = new int[2*i]; lazy = new int[2*i]; MLen = new int[2*i]; LLen = new int[2*i]; RLen = new int[2*i]; ZLLen = new int[2*i]; ZRLen = new int[2*i]; ZMLen = new int[2*i]; Arrays.fill(lazy, -1); buildTree(0, n-1, 1, arr); &#125; private void buildTree(int l, int r, int node, int[] arr) &#123; if(l == r) &#123; table[node] = MLen[node] = LLen[node] = RLen[node] = arr[l]; Ztable[node] = ZLLen[node] = ZRLen[node] = ZMLen[node] = 1 - arr[l]; return; &#125; int mid = (l + r) &gt;&gt; 1; buildTree(l, mid, left(node), arr); buildTree(mid+1, r, right(node), arr); pushUp(node, mid - l + 1, r - mid); &#125; private void pushUp(int node, int lenL, int lenR) &#123; int l = left(node), r = right(node); table[node] = table[l] + table[r]; Ztable[node] = Ztable[l] + Ztable[r]; LLen[node] = LLen[l] == lenL ? LLen[l] + LLen[r] : LLen[l]; RLen[node] = RLen[r] == lenR ? RLen[r] + RLen[l] : RLen[r]; ZLLen[node] = ZLLen[l] == lenL ? ZLLen[l] + ZLLen[r] : ZLLen[l]; ZRLen[node] = ZRLen[r] == lenR ? ZRLen[r] + ZRLen[l] : ZRLen[r]; MLen[node] = Math.max(MLen[l], MLen[r]); MLen[node] = Math.max(MLen[node], RLen[l] + LLen[r]); ZMLen[node] = Math.max(ZMLen[l], ZMLen[r]); ZMLen[node] = Math.max(ZMLen[node], ZRLen[l] + ZLLen[r]); &#125; private void lazyDown(int node, int len) &#123; if (len &lt;= 1 || lazy[node] == -1) &#123; return; &#125; int l = left(node), r = right(node); lazyHelper2(lazy[node], l, len - (len &gt;&gt; 1)); lazyHelper2(lazy[node], r, len &gt;&gt; 1); lazy[node] = -1; &#125; private void lazyHelper(int node, int len) &#123; if (lazy[node] == 0) &#123; table[node] = LLen[node] = RLen[node] = MLen[node] = 0; Ztable[node] = ZLLen[node] = ZRLen[node] = ZMLen[node] = len; &#125; else if (lazy[node] == 1) &#123; table[node] = LLen[node] = RLen[node] = MLen[node] = len; Ztable[node] = ZLLen[node] = ZRLen[node] = ZMLen[node] = 0; &#125; else if (lazy[node] == 2) &#123; int tmp0 = table[node], tmp1 = LLen[node], tmp2 = RLen[node], tmp3 = MLen[node]; table[node] = Ztable[node]; LLen[node] = ZLLen[node]; RLen[node] = ZRLen[node]; MLen[node] = ZMLen[node]; Ztable[node] = tmp0; ZLLen[node] = tmp1; ZRLen[node] = tmp2; ZMLen[node] = tmp3; &#125; &#125; private void lazyHelper2(int t, int node, int len) &#123; if (t == 2) &#123; if (lazy[node] == -1) &#123; lazy[node] = t; lazyHelper(node, len); &#125; else if (lazy[node] == 2) &#123; lazyHelper(node, len); lazy[node] = -1; &#125; else &#123; lazy[node] = 1 - lazy[node]; lazyHelper(node, len); &#125; &#125; else &#123; lazy[node] = t; lazyHelper(node, len); &#125; &#125; public void update(int t, int L, int R, int x, int y, int node) &#123; if ( y &lt; L || x &gt; R ) &#123; return; &#125; if (L &lt;= x &amp;&amp; y &lt;= R) &#123; lazyHelper2(t, node, y - x + 1); return; &#125; lazyDown(node, y - x + 1); int mid = (x + y) &gt;&gt; 1; update(t, L, R, x, mid, left(node)); update(t, L, R, mid+1, y, right(node)); pushUp(node, mid - x + 1, y - mid); &#125; public int query3(int L, int R, int x, int y, int node) &#123; if ( y &lt; L || x &gt; R ) &#123; return 0; &#125; if (L &lt;= x &amp;&amp; y &lt;= R) &#123; return table[node]; &#125; lazyDown(node, y - x + 1); int res = 0, mid = (x + y) &gt;&gt; 1; res += query3(L, R, x, mid, left(node)); res += query3(L, R,mid+1, y, right(node)); return res; &#125; // 在进行查询4的时候，由于左右儿子都可能只有一部分与查询区间相交， // 所有需要一个结构体来存储相交部分的相关信息。 private class Ans &#123; public int LLen = 0; public int RLen = 0; public int MLen = 0; public Ans() &#123;&#125; public Ans(int LLen, int RLen, int MLen) &#123; this.LLen = LLen; this.RLen = RLen; this.MLen = MLen; &#125; &#125; public int query4(int L, int R, int x, int y, int node) &#123; return query4Helper(L, R, x, y, node).MLen; &#125; private Ans query4Helper(int L, int R, int x, int y, int node) &#123; if ( y &lt; L || x &gt; R ) &#123; return new Ans(); &#125; if (L &lt;= x &amp;&amp; y &lt;= R) &#123; return new Ans(LLen[node], RLen[node], MLen[node]); &#125; lazyDown(node, y - x + 1); int res1 = 0, res2 = 0, res3 = 0, mid = (x + y) &gt;&gt; 1; Ans ans1 = query4Helper(L, R, x, mid, left(node)); Ans ans2 = query4Helper(L, R, mid+1, y, right(node)); res1 = ans1.LLen == (mid - x + 1) ? ans1.LLen + ans2.LLen : ans1.LLen; res2 = ans2.RLen == (y - mid) ? ans2.RLen + ans1.RLen : ans2.RLen; res3 = Math.max(ans1.MLen, ans2.MLen); res3 = Math.max(res3, ans1.RLen + ans2.LLen); return new Ans(res1, res2, res3); &#125; private int left(int idx) &#123; return idx &lt;&lt; 1; &#125; private int right(int idx) &#123; return (idx &lt;&lt; 1) + 1; &#125; public static void main(String[] args) &#123; Scanner sc = new Scanner(System.in); int T = sc.nextInt(); for (int k = 0; k &lt; T; k++) &#123; int n, m, t, x, y; n = sc.nextInt(); m = sc.nextInt(); int[] arr = new int[n]; for (int i = 0; i &lt; n; i++) &#123; arr[i] = sc.nextInt(); &#125; HDU3397 hdu = new HDU3397(n, arr); for (int i = 0; i &lt; m; i++) &#123; t = sc.nextInt(); x = sc.nextInt(); y = sc.nextInt(); if (t == 3) &#123; System.out.println(hdu.query3(x, y, 0, n - 1, 1)); &#125; else if (t == 4) &#123; System.out.println(hdu.query4(x, y, 0, n - 1, 1)); &#125; else &#123; hdu.update(t, x, y, 0, n - 1, 1); &#125; &#125; &#125; &#125;&#125; 代码比较基础，速度与使用空间都不怎么样，但是至少AC了： 总结线段树有很多不同的形式，而且很多时候根据题目的不同会有很多的小变化。 最重要的是代码一般较长，逻辑一般较乱，所有很容易出BUG，建议保持好心态。]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线段树（Segment Tree）使用记录]]></title>
    <url>%2F2019%2F08%2F16%2F2019-08-16-segment-tree%2F</url>
    <content type="text"><![CDATA[线段树代码形式12345678910111213141516class SegmentTree &#123; private int[] table; public SegmentTree(int[] arr) &#123; &#125; public void update(int idx, int num) &#123; &#125; public int query(int left, int right) &#123; &#125;&#125; 可以看到，通常线段树输入一个数组，同时维护自身的一个table数组。 提供query方法来完成对一个区间$[left, right]$的查询操作（最大值、最小值、求和等）， 提供update方法来完成对输入数组某一位置的修改。 比如对于子数组最大值的查询，假设数组长度为$N$，查询$M$次，那么暴力方法查询的复杂度就为$O(MN)$， 而使用线段树则可以将每次查询的复杂度降到$log(n)$，那么总的负责度就能降到$O(Mlog(N))$。 所以在某些时候，线段树是一个十分有用的数据结构。 线段树结构形式 树的性质线段树其实就是一颗满二叉树，注意到这是一个十分重要的性质（使得可以使用数组快速建树）， 它可以推出“假设叶节点个数为$n$，那么非叶节点的个数一定是 $n - 1$ ”， 那么同时也就表明总的节点数为 $n + (n - 1) = 2n - 1$ ，总的节点个数一定是奇数。 （这里使用的满二叉树定义为：除了叶子结点之外的每一个结点都有两个孩子结点。） 完美二叉树, 完全二叉树和完满二叉树 下面可以简单的证明一下推论“假设叶节点个数为$n$，那么非叶节点的个数一定是 $n - 1$ ”： 假设总结点树为$n$，非叶节点数为$n_1$，叶节点数为$n_2$，那么$n = n_1 + n_2$。 由二叉树的性质，分支数（边数）为$n - 1$。 由非叶节点都有两个儿子，分支数（边数）也可以计算为$2n_1$。 那么： n - 1 = 2n_1 \\ n = 2n_1 - 1可以推出： n = n_1 + n_2 \\ 2n_1 - 1 = n_1 + n_2 \\ n_1 - 1 = n_2推论得证。 线段树数据结构线段树通常使用一个数组T来进行存储，根节点在T[1]的位置（T[0]不使用）， 一个节点T[i]的左儿子为T[2i]，右儿子为T[2i+1]，父节点为T[i/2]。 对于最大值来说，每个节点维护以这个节点为根的子树的最大值，所有输入的数据都存放在叶节点。 建树： 复杂度：$O(n)$ 123456789101112131415161718class SegmentTreeMax &#123; private int[] table; private int n; public SegmentTreeMax(int[] arr) &#123; this.n = arr.length; table = new int[n*2]; for (int i = n, j = 0; i &lt; table.length; i++, j++) &#123; table[i] = arr[j]; &#125; for (int i = n-1; i &gt; 0; i--) &#123; table[i] = Math.max(table[left(i)], table[right(i)]); &#125; &#125;&#125; 建树时就使用了上面的性质：假设叶节点个数为$n$，那么非叶节点的个数一定是 $n - 1$ 。所以这里直接申请一个2n大小的数组， 然后从1到n-1为作为非叶节点，n到2n-1作为叶节点。 对于[1, 2, 3, 4, 5, 6]来说，它建立的数组为[0, 6, 6, 2, 4, 6, 1, 2, 3, 4, 5, 6]（注意索引0不使用），形状为： 更新： 复杂度：$O(log(n))$ 更新很简单，更新叶节点后再迭代更新父节点即可： 123456789public void update(int idx, int num) &#123; idx += n; table[idx] = num; idx = parent(idx); while (idx &gt; 0) &#123; table[idx] = Math.max(table[left(idx)], table[right(idx)]); idx = parent(idx); &#125;&#125; 线段树的查询复杂度：$O(log(n))$ 线段树的查询才是线段树的精髓所在，其实二叉树这种分治的思想并不是一个难以想到的方法， 但是分治后的子问题结果的合并才是这里比较重要的地方。 线段树的结构已经将问题划分到了一个个子树之上，但是在进行区间查询时，区间可能跨越多颗子树： 例如上图，它查询原数组中[3, 5]区间（树中的节点9、10、11）中的最大元素，很明显的，9号节点单独在一颗子树之中。 首先需要注意到上面线段树数据结构中所说“一个节点T[i]的左儿子为T[2i]，右儿子为T[2i+1]”， 那么所有左儿子的节点序号都是偶数，而右儿子的节点序号则都是奇数。 那么对于一个查询区间[L, R]（L不等于R）： 对于区间的左边界L，如果它是偶数，那么它是父节点的左儿子，那么它的兄弟节点L+1（父节点的右儿子）也属于这个区间之内，那么对于最大值，应该直接向上询问它的父节点。 对于区间的左边界L，如果它是奇数，那么它是父节点的右儿子，那么它的兄弟节点L-1（父节点的左儿子）肯定不属于这个区间之内，那么对于最大值，直接询问节点l，不能向上询问它的父节点。 对应以上结论： 当左边界L为偶数时：L = parent(L)。 当左边界L为奇数时：max = Math.max(max, L)。这里完成了对L这个点的查询，那么就可以对区间进行缩小，即：L = L + 1（注意到L变成了偶数）。 右边界的处理同理左边界。 可以看到这是一个不断收缩区间左右边界的过程，并从叶节点逐渐向上走，实际代码如下： 123456789101112131415161718public int query(int l, int r) &#123; l += n; r += n; int max = 0; while ( l &lt;= r ) &#123; if ( (l &amp; 1) == 1 ) &#123; max = Math.max(max, table[l]); l++; &#125; if ( (r &amp; 1) != 1 ) &#123; max = Math.max(max, table[r]); r--; &#125; l &gt;&gt;= 1; r &gt;&gt;= 1; &#125; return max;&#125; 代码在不断缩小区间的过程中，并且对于更新后的L&#39;、R&#39;，能够保证 $[L’, R’] \in [L, R]$ ，也不会遗漏任何区间内的元素。 线段树代码区间最大值： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758class SegmentTreeMax &#123; private int[] table; private int n; public SegmentTreeMax(int[] arr) &#123; this.n = arr.length; table = new int[n*2]; for (int i = n, j = 0; i &lt; table.length; i++, j++) &#123; table[i] = arr[j]; &#125; for (int i = n-1; i &gt; 0; i--) &#123; table[i] = Math.max(table[left(i)], table[right(i)]); &#125; &#125; public void update(int idx, int num) &#123; idx += n; table[idx] = num; idx = parent(idx); while (idx &gt; 0) &#123; table[idx] = Math.max(table[left(idx)], table[right(idx)]); idx = parent(idx); &#125; &#125; public int query(int l, int r) &#123; l += n; r += n; int max = 0; while ( l &lt;= r ) &#123; if ( (l &amp; 1) == 1 ) &#123; max = Math.max(max, table[l]); l++; &#125; if ( (r &amp; 1) != 1 ) &#123; max = Math.max(max, table[r]); r--; &#125; l &gt;&gt;= 1; r &gt;&gt;= 1; &#125; return max; &#125; private int left(int idx) &#123; return idx &lt;&lt; 1; &#125; private int right(int idx) &#123; return (idx &lt;&lt; 1) + 1; &#125; private int parent(int idx) &#123; return idx &gt;&gt; 1; &#125;&#125; 上面是基本的代码形式，可以依据它来修改为各种不同的用途。 Leetcode 1157Leetcode 1157 题目大意是给定一个数组，提供以下功能： 查询一个区间内的majority-element，也就是这个数的出现次数大于给定的threshold，注意threshold一定大于这个区间的一半大小， 这样即能保证一个区间里面最多只存在一个majority-element。 注意到这个题目最关键的思想点在于： 一个区间（区间大小N）中的一个数A，它在区间内的出现次数大于N/2，那么无论将这个区间切分为几个小区间， 这些小区间中，必定至少存在一个小区间t（区间大小n），A在小区间t中的出现次数大于n/2。 这个证明使用反证即可： 假设区间$T$，大小为$N$，其中数$A$出现次数$A_T$大于$\frac{N}{2}$。 现在将区间$T$划分为小区间$\{ T_1, T_2, T_3, … , T_m \}$，区间大小为$\{ n_1, n_2, n_3, … , n_m \}$，$A$的出现次数为： \{ A_{T_1}, A_{T_2}, ..., A_{T_m} \}如果所有的小区间中，A的出现都不到区间的一半： A_{T_i} \leqslant \frac{n_i}{2}，对于任意 \ i \in \{1,2,...,m\}那么： A_{T_1} + A_{T_2} + ... + A_{T_m} \leqslant \frac{n_1}{2} + \frac{n_2}{2} + ... + \frac{n_m}{2} \\ A_T \leqslant \frac{N}{2}显然与假设中的 $A_T$大于$\frac{N}{2}$ 相矛盾。 一旦想通了这个Punchline，就可以开始使用线段树来做这个题了，线段树中每个节点存储以它为根的子树中最多的元素， 那么一个区间内的最多的元素，就类似于求这个区间内的最大元素，只不过最大元素比的是大小，而这里比的是在子区间中出现的次数多少。 注意到一个查询区间可能由多个子树组成，这就类似与多个子区间，那么这个majority-element一定会出现在某课子树的根节点上。 当然这里还有另一个重点：如何快速查询一个数在一个子区间内出现多少次？如果使用遍历，那么是$O(n)$的复杂度。 一个巧妙的方法是将这个数的所有索引存下来形成一个List，通过二分查找来查询子区间的左右边界在List中出现的位置， 相减即可知道子区间中这个数的数量，复杂度$O(log(n))$。 最后代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677class MajorityChecker &#123; private int[] table; private Map&lt;Integer, List&lt;Integer&gt;&gt; numIdxs = new HashMap&lt;&gt;(); private int n; public MajorityChecker(int[] arr) &#123; this.n = arr.length; table = new int[n*2]; for (int i = n, j = 0; i &lt; table.length; i++, j++) &#123; table[i] = arr[j]; List&lt;Integer&gt; idxs = numIdxs.get(arr[j]); if ( idxs == null ) &#123; idxs = new ArrayList&lt;&gt;(); numIdxs.put(arr[j], idxs); &#125; idxs.add(i); &#125; for (int i = n-1; i &gt; 0; i--) &#123; int l = left(i), r = right(i); table[i] = countRange(l, r, table[l]) &gt; countRange(l, r, table[r]) ? table[l] : table[r]; &#125; &#125; private int countRange(int l, int r, int num) &#123; List&lt;Integer&gt; idxs = numIdxs.get(num); int idx1 = Collections.binarySearch(idxs, l); int idx2 = Collections.binarySearch(idxs, r); if (idx1 &lt; 0) &#123; idx1 = -(idx1 + 1); &#125; if (idx2 &lt; 0) &#123; idx2 = -(idx2 + 1) - 1; &#125; return idx2 - idx1 + 1; &#125; public int query(int l, int r, int threshold) &#123; l += n; r += n; int ll = l, rr = r; int res = 0; int max = 0; while ( l &lt;= r ) &#123; if ( (l &amp; 1) == 1 ) &#123; int tmp = countRange(ll, rr, table[l]); if (tmp &gt; max) &#123; max = tmp; res = table[l]; &#125; l++; &#125; if ( (r &amp; 1) != 1 ) &#123; int tmp = countRange(ll, rr, table[r]); if (tmp &gt; max) &#123; max = tmp; res = table[r]; &#125; r--; &#125; l &gt;&gt;= 1; r &gt;&gt;= 1; &#125; return max &gt;= threshold ? res : -1; &#125; private int left(int idx) &#123; return idx &lt;&lt; 1; &#125; private int right(int idx) &#123; return (idx &lt;&lt; 1) + 1; &#125; &#125;]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP/IP详解卷1：协议 第19、20、21章笔记]]></title>
    <url>%2F2019%2F08%2F14%2F2019-08-14-TCP-IP-19-20-21%2F</url>
    <content type="text"><![CDATA[第19章 TCP的交互数据流首先，上面叫交互数据流，原文解释： 块数据：如FTP、电子邮件和Usenet新闻。 交互数据：如Telnet和Rlogin。 那么交互数据流就是客户端与服务端需要频繁交互数据的那种数据流。 这一章主要以Rlogin举例，也就是远程登录。 信息交互过程 在Rlogin的交互中，需要进行图上的信息传递： 客户端告知服务端按键信息。 服务端的对按键信息的确认。 服务器告知客户端的按键显示信息（回显）。 客户端对按键显示信息的确认。 在我们平常使用ssh的过程中可能感觉不到，这里每次只传送一个按键信息。 经受时延的确认考虑到上面的交互过程，2和3步服务器有两个信息要传递给客户端，一个ACK信息，一个数据信息。 那么十分自然的想法，这两个信息如果用两个报文来传递有点浪费资源，因为它们可以不冲突的放在以一个报文里面。 （即数据捎带ACK） 如果要做到这一点，就要想到，服务器在收到按键信息后，需要经过一定时间的处理，然后才会发出回显信息， 那么很明显的，按键确认信息要比回显信息先构建出来，它们之间差多少时间无法确定，所以就有了这里经受时延的确认。 经受时延的确认： 通常TCP在接收到数据时并不立即发送ACK；相反，它推迟发送，以便将ACK与需要沿该方向发送的数据一起发送（有时 称这种现象为数据捎带ACK）。绝大多数实现采用的时延为200ms，也就是说，TCP将以最大200ms的时延等待是否有数据一起发送。 注意到这里说的是最大200ms时延，也就是这里有一个200ms定时器，但是TCP使用了一个200 ms的定时器，该定时器以相对于内核引导的200 ms固定时间溢出， 所以这个定时器不是在ACK开始等待的时候触发的，而是一定有的，ACK在等待的时候，就可能会在1~200ms的任意时刻碰到定时器溢出。 看书上给出的例子： ACK等待时间为：123.5、65.6、109.0、132.2、42.0、140.3和195.8ms。 ACK发出时间为：139.9、539.3、940.1、1339.9、1739.9、1940.1和2140.1ms。 ACK发出时间相差均为200的整数倍，证明了这个200ms的定时器与ACK开始等待的时间无关，而是在200ms固定时间间隔溢出。 Nagle算法正如上面说的，Rlogin交互过程中每次只传送一个按键信息，比如你一瞬间打（粘贴）了个abcdefg， 结果它竟然是发送7个报文，分别报告这几个字母，如果网络资源并不是那么丰富的情况下，就有点太浪费了。 那么我们自然就想一次多发送几个字母过去，也就是Nagle算法做的事情。 Nagle算法： 该算法要求一个TCP连接上最多只能有一个未被确认的未完成的小分组，在该分组的确认到达之前不能发送其他的小分组。 相反，TCP收集这些少量的分组，并在确认到来时以一个分组的方式发出去。 也就是当你按下字母a时，报文被发送，再按下bcdefg时，它们不会被发送，直到收到报文a的ACK， 这时就将bcdefg放在一个报文里面发送出去。 该算法的优越之处在于它是自适应的：确认到达得越快，数据也就发送得越快。 Nagle算法的目的在于减少网络中小分组的数目，但是实际上会增加整个发送过程的时延， 在延迟比较敏感的场景下，禁用它能提升体验。 窗口大小通告 图中的win 4096、win 8192等就是窗口大小的通告，代表的是自己的接收窗口还有多少剩余空间。 注意上图的14~18报文： 客户端通过14、15报文一共发送了4个字节（14:18(4)）的数据到服务器。 服务器回复ACK 18，表示收到了这4个字节的数据，但是只回显了3个字节56:59(3)，所以还留有1个字节数据未处理，此时窗口大小为$8191 = 8192 - 1$。 客户端确认回显（ACK 59），并继续发送3字节（18:21(3)）数据，但是窗口大小$4093 = 4096 - 3$，说明收到的3字节回显还没有处理。 服务器回复ACK 21，并发送回显1个字节59:60(1)，那么18:21(3)这3字节数据都还没处理，所以窗口大小为$8189 = 8192 - 3$。 wireshark抓包测试首先先建立一个ssh连接： 开启wireshark抓包： 敲一个a看什么效果： 可以看到一共发送了三个报文，和书上所述一致。 这里直接粘贴一个字符串TCP进去，结果如下： 可以看到这里显然同时为T、C、P三个字符分别发送了报文，可以看出，这里显然关闭了Nagle算法。 另外.233发送了三个回显报文，但是.150只发送了2个ACK，这显然是经受时延的确认带来的效果。 TCP的成块数据流在上一节说的是有关于交互数据流的一些东西，这一节介绍了关于成块数据流上的一些东西。 这一章介绍了滑动窗口协议、PUSH标志、慢启动和紧急方式，这些概念。 滑动窗口协议在滑动窗口之前，书上已经介绍了停止等待协议（停等协议），这个协议十分简单： 发送方发送一个报文之后，必须等到这个报文的ACK到来，才能发送下一个报文。 在成块数据传输时（例如传一个文件），如果使用停等协议来进行传输，那么速度就太慢了，相当于每个RTT时间只能传输一个报文， 传输效率基本只与RTT时间有关，链路容量再大也是白给。所以这时候就需要同时传输多个报文，也就形成了滑动窗口协议。 如书中所给的图，1、2、3…10、11…等就表示报文，数字就是它们的序号。方框就表示当前时刻窗口的位置， 可以看到窗口外的左边是已发送并且已确认的报文，窗口外的右边是还未发送也不能发送的报文， 窗口内的左右两边则是已发送但还未确认、未发送但可以发送的报文。 显然之所以叫做滑动窗口，就是因为它会随着报文的发送确认而不断的向右滑动。 书中使用三个术语来描述窗口左右边沿的运动： 称窗口左边沿向右边沿靠近为窗口合拢。这种现象发生在数据被发送和确认时。 当窗口右边沿向右移动时将允许发送更多的数据，我们称之为窗口张开。这种现象发生在另一端的接收进程读取已经确认的数据并释放了T C P的接收缓存时。 当右边沿向左移动时，我们称之为窗口收缩。Host Requirements RFC强烈建议不要使用这种方式。但TCP必须能够在某一端产生这种情况时进行处理。 另外注意到，一个ACK是对它之前所有报文的确认。 窗口大小： 需要注意到的是，滑动窗口存在于发送方，但是它的窗口的大小是由接收方提供的。 观察书中给出的例子： 在第二个报文（SYN）中，接收方通报窗口大小为6144，mss大小为1024。 由于窗口大小为6144，发送方直接发送了6个1024大小的报文。 接收方发出ACK 6145，通报窗口大小为2048。 发送方收到ACK 6145，刚才窗口内的6个报文全部被确认，窗口左边直接移动到6145字节位置，由于通报窗口大小为2048，窗口右边沿移动到8192位置。 发送方将现在窗口里面的两个报文发出。 接收方不断发出ACK，确认报文接收以及通报窗口大小，可惜发送方已经没有更多的数据发送了。 PUSH标志可以看到上面的例子中，一些报文被设置了PSH标志，直接引用书中所述，描述了PUSH标志的作用： 发送方使用该标志通知接收方将所收到的数据全部提交给接收进程。 这里的数据包括与PUSH一起传送的数据以及接收方TCP已经为接收进程收到的其他数据。 这里描述了PUSH标志的用途： 通过允许客户应用程序通知其TCP设置PUSH标志， 客户进程通知TCP在向服务器发送一个报文段时不要因等待额外数据而使已提交数据在缓存中滞留。 类似地，当服务器的TCP接收到一个设置了PUSH标志的报文段时， 它需要立即将这些数据递交给服务器进程而不能等待判断是否还会有额外的数据到达。 也就是说，在没有设置PUSH的情况下，TCP收到数据之后，可能并不会立即上传给应用，也许它想要等到更多的数据到来再一起上传给应用， 以提高效率。 但是例如Rlogin这种交互式应用，每次就发送一个字节的数据过去，如果TCP想要等待更多的数据到来，则可能会增大延迟， 这时就需要PUSH标志来告知TCP不要进行等待，即使只收到一个字节数据，也要立即上传给应用。 但是书中又说了: 然而，目前大多数的API没有向应用程序提供通知其TCP设置PUSH标志的方法。的确， 许多实现程序认为PUSH标志已经过时，一个好的TCP实现能够自行决定何时设置这个标志。 需要注意到不同的TCP实现方式处理PUSH标志时可能也有所不同， 例如由于源于伯克利的实现一般从不将接收到的数据推迟交付给应用程序，因此它们忽略所接收的PUSH标志。 PUSH标志通常在以下情形被设置： 应用的一次数据写入完成。 发送缓冲区没有更多的数据等待发送。 当然，还是有些时候还是挺玄学，具体还是要看使用TCP版本的实现。 慢启动在滑窗协议那里所举的例子中，发送方首先就直接发送了6个报文，看起来这样可以提高发送效率， 但是实际中需要考虑整个路由路径上的拥塞情况，如果每个人都没有节制的发送自己的报文， 那么结果就是路径堵死，谁都别想成功发送报文。 为了避免这种情况，就需要每个人限制自己的发送速度，那么如何知道自己能以多快的速率发送呢？ 既然全靠自觉，那么就只能一点点去试，逐渐增加自己的发送速度，直到丢包，这就是慢启动。 慢启动为发送方的TCP增加了另一个窗口：拥塞窗口(congestion window)，记为cwnd。 当与另一个网络的主机建立TCP连接时，拥塞窗口被初始化为1个报文段（即另一端通告的报文段大小）。 每收到一个ACK，拥塞窗口就增加一个报文段（cwnd以字节为单位，但是慢启动以报文段大小为单位进行增加）。 需要注意到发送方取拥塞窗口与通告窗口中的最小值作为发送上限。 拥塞窗口是发送方使用的流量控制，而通告窗口则是接收方使用的流量控制。 注意到虽然名字是慢启动，但是其实启动速度是指数增长的，如书上的例子（先不考虑通告窗口的大小）： 初始化拥塞窗口大小为1，发送一个报文。 收到ACK 513，拥塞窗口大小增加为2，窗口右移，发送两个报文。 收到ACK 1025，拥塞窗口大小增加为3，窗口右移，发送两个报文。 收到ACK 1537，拥塞窗口大小增加为4，窗口右移，发送两个报文。 收到ACK 2049、ACK 2561，拥塞窗口大小增加为6，窗口右移，发送最后一个报文。 在这一步时，一共发送了8个报文，收到了5个ACK，还有3个报文没有收到ACK，当这三个ACK都收到时， 拥塞窗口大小将变为9。 如果发出的报文ACK立刻返回，那么拥塞窗口的大小变化规律就是： 1 -&gt; 2 2 -&gt; 4 4 -&gt; 8 … 也就是一个RTT时间之后，窗口大小就会翻倍，所以说是指数增长。 成块数据的吞吐量 通常发送一个分组的时间取决于两个因素： 传播时延（由光的有限速率、传输设备的等待时间等引起） 和一个取决于媒体速率（即媒体每秒可传输的比特数）的发送时延。 对于一个给定的两个接点之间的通路，传播时延一般是固定的，而发送时延则取决于分组的大小。 在速率较慢的情况下发送时延起主要作用，而在千兆比特速率下传播时延则占主要地位。 如书中所举的例子： 在例子中的最后时刻，发送方不断发送报文和接收ACK，已经填满了整个路径， 这时窗口的增加已经无法增加发送速率了，也就是连接的理想稳定状态。 带宽时延乘积： 这里给出了通道容量的计算公式，注意到它并不是吞吐量。 正如上面的例子中的图，如果一个报文完全写入通道需要时间A，RTT大小为B， 相当于发送方不间断的写入报文时，从第一个报文开始写入，直到第一个ACK到达发送方时， 发送方一共发送了$\frac{B}{A}$个报文。 假设一个报文的长度为C，那么通道容量为： capacity(bit)=\frac{B}{A} * C = B * \frac{C}{A}注意到B就是RTT时间，$\frac{C}{A}$就是链路带宽，即： capacity(bit)=bandwidth(b/s) \times "round \ trip \ time(s)"计算这个值的意义在于指导TCP的缓存大小，如果发送或者接收缓冲小于通道容量， 那么传输速度就一定达不到理论上的最大值，因为当发送缓冲塞满时，就会停止发送报文。 拥塞： 当数据到达一个大的管道（如一个快速局域网）并向一个较小的管道（如一个较慢的广域网）发送时便会发生拥塞。 当多个输入流到达一个路由器，而路由器的输出流小于这些输入流的总和时也会发生拥塞。 正如书中的例子： 整个链路的链路带宽由最小的那部分决定。 紧急方式 TCP提供了紧急方式(urgent mode)，它使一端可以告诉另一端有些具有某种方式的“紧急数据”已经放置在普通的数据流中。 另一端被通知这个紧急数据已被放置在普通数据流中，由接收方决定如何处理。 这个东西目前还不好理解，它是一种在已有的TCP连接上传输紧急数据的方式。 具体参考： TCP带外数据 带外数据：TCP紧急模式分析 TCP的超时与重传TCP提供可靠的运输层，也就是说，使用TCP传输数据，它保证接收端能收到发送端所发给它的所有数据。 达到可靠的阻碍在于报文在传输过程中会发生丢包，而且你还无法知道这个丢包的发生， 所以就需要使用超时来判断是否丢包，使用重传来重新传输丢掉的数据。 往返时间测量 TCP超时与重传中最重要的部分就是对一个给定连接的往返时间（RTT）的测量。 假设RTT时间为A秒，正常情况下，当一个报文发出后的A秒，发送方就应该接收到ACK， 那如果没有收到ACK，我们就有理由怀疑报文丢失了，考虑到网络的抖动，我们可能再等待一段时间， 如果这时还没有收到ACK，那么就可以考虑开始重传了。 所以，超时时间的计算是基于RTT时间的，在一个局域网里面，超时时间相比与一个广域网来说，肯定是要短一点的。 首先TCP必须测量在发送一个带有特别序号的字节和接收到包含该字节的确认之间的RTT。 注意到发送报文与ACK并不是一一对应的，一个报文2可以通过ACK 5来确认，所以这里书中表述为接收到包含该字节的确认。 由于这个问题和网络抖动会造成测量的RTT时间在一定范围内变化，那么这里就需要多次对RTT的测量来计算得到一个相对准确的值， 一种简单的平滑方式如下： R \leftarrow \alpha R + (1 - \alpha ) M其中$R$表示平滑得到的RTT，$M$表示测量得到的RTT，$\alpha$为平滑因子，推荐值0.9。 在RFC 793中推荐的重传超时时间RTO（Retransmission Time Out）的值应该设置为： RTO=R \beta这里的$\beta$为时延离散因子，推荐值为2。也就是说超时时间为2倍的RTT。 Jacobson认为这种方式太简单，容易更不上网络的变化，他认为方差也是一个需要考虑的因素， 所以它提出了下面的RTO估计方法： Err = M - A \\ A \leftarrow A + gErr \\ D \leftarrow D + h(|Err| - D) \\ RTO = A + 4D这里$M$表示测量得到的RTT，$A$表示平滑得到的RTT，$D$表示平滑得到的均值偏差，$g$取值$\frac{1}{8}$，$h$取值0.25。 观察这个计算方式可以发现，当$Err$持续是一个很小的值，也就是RTT的测量值比较稳定时，$D$趋于0，RTO就趋于$A$， 也就是RTO就是RTT，而上面简单的计算公式中，RTO总是两倍的RTT。这其实很好理解，如果RTT的测量值比较稳定， 就证明RTT波动很小，一旦某个报文在一个RTT范围内没有ACK回来，那么它就是一个异常点，大概率是发生了丢包， 所以立刻开始重传，而不是等待两倍的RTT时间。 Karn算法： 在重传数据的确认最后到达之前，不能更新RTT估计器。 因为所收到的重传数据的ACK，无法确定是哪一次重传报文的ACK。 另外，在重传数据时，已经采用了指数退避的策略来计算RTO，每次重传都将RTO乘2。 往返时间RTT的测量： 大多数源于伯克利的TCP实现在任何时候对每个连接仅测量一次RTT值。 在发送一个报文段时，如果给定连接的定时器已经被使用，则该报文段不被计时。 这里的定时器使用滴答计数器来进行： 可以看到对于RTT的测量并不是精确的，而是通过它经历了几个滴答来计算它的时间， 就像图中的第一次测量，即使实际上是1.061秒，但是它经过了3个滴答，所以被记录为1.5秒。 RTO的初始化以及重传时的计算： 变量$A$和$D$分别被初始化为0和3秒。 也就是说初始的重传时间就是： RTO_{init}=A+2D=0 + 2 \times 3 = 6s 因子2D只在这个初始化计算中使用。正如前面提到的，以后使用4D和A相加来计算RTO。 那么第一次超时时间就是6s，当接着发生第二次超时的情况下，首先使用正确的公式计算$RTO_1$，再进行指数退避（乘2）： RTO_2 = 2 RTO_1 = 2 (A + 4D) = 2(0 + 4 \times 3) = 24s再往后接着进行指数退避： $RTO_3 = 2RTO_2 = 48s$ $RTO_4 = 2RTO_3 = 64s$ $RTO_5 = 64s$ $RTO_6 = 64s$ $RTO_7 = 64s$ … 注意到指数退避的最大值为64s。 拥塞避免算法 拥塞避免算法是一种处理丢失分组的方法。 该算法假定由于分组受到损坏引起的丢失是非常少的（远小于1%）， 因此分组丢失就意味着在源主机和目的主机之间的某处网络上发生了拥塞。 有两种分组丢失的指示：发生超时和接收到重复的确认。 注意到之前说了慢启动算法，它会一直增加cwnd窗口大小，那么这样增长下去，很可能到达一个上限，造成网络拥塞， 以至于产生丢包。那么此时仅仅重传丢失的报文是不够的，因为现在的cwnd窗口大小明显是过大了的， 所以还要将它缩小，这就是这里的拥塞避免算法。 拥塞避免算法和慢启动算法需要对每个连接维持两个变量：一个拥塞窗口cwnd和一个慢启动门限ssthresh。 这样得到的算法的工作过程如下： 对一个给定的连接，初始化cwnd为1个报文段，ssthresh为65535个字节。 TCP输出例程的输出不能超过cwnd和接收方通告窗口的大小。拥塞避免是发送方使用的流量控制， 而通告窗口则是接收方进行的流量控制。前者是发送方感受到的网络拥塞的估计， 而后者则与接收方在该连接上的可用缓存大小有关。 当拥塞发生时（超时或收到重复确认），ssthresh被设置为当前窗口大小的一半 （cwnd和接收方通告窗口大小的最小值，但最少为2个报文段）。 此外，如果是超时引起了拥塞，则cwnd被设置为1个报文段（这就是慢启动）。 当新的数据被对方确认时，就增加cwnd，但增加的方法依赖于我们是否正在进行慢启动或拥塞避免。 如果cwnd小于或等于ssthresh，则正在进行慢启动，否则正在进行拥塞避免。 慢启动一直持续到我们回到当拥塞发生时所处位置的一半时候才停止（因为我们记录了在步骤2中给我们制造麻烦的窗口大小的一半）， 然后转为执行拥塞避免。 ssthresh：标记上一次拥塞发生时，窗口大小的一半。 也就是说，首先慢启动（其实是指数增长），然后为了降低窗口增长速度， 所以需要在cwnd超过ssthresh时，降低窗口的增长速度，进入拥塞避免增长模式。 拥塞避免：求每次收到一个确认时将cwnd增加1/cwnd。也就是说一个往返时间内最多为cwnd增加1个报文段。 快速重传与快速恢复算法 在收到一个失序的报文段时，TCP立即需要产生一个ACK（一个重复的ACK）。 这个重复的ACK不应该被迟延。 该重复的ACK的目的在于让对方知道收到一个失序的报文段，并告诉对方自己希望收到的序号。 直接先来看算法过程： 当收到第3个重复的ACK时（不算第一个正常的ACK），将ssthresh设置为当前拥塞窗口cwnd的一半。重传丢失的报文段。 设置cwnd为ssthresh加上3倍的报文段大小。 每次收到另一个重复的ACK时，cwnd增加1个报文段大小并发送1个分组（如果新的cwnd允许发送）。 当下一个确认新数据的ACK到达时，设置cwnd为ssthresh（在第1步中设置的值）。 这个ACK应该是在进行重传后的一个往返时间内对步骤1中重传的确认。 另外，这个ACK也应该是对丢失的分组和收到的第1个重复的ACK之间的所有中间报文段的确认。 这一步采用的是拥塞避免，因为当分组丢失时我们将当前的速率减半。 这个算法由收到3个重复的ACK而启动，其中的道理在于： 首先TCP的发送顺序并不等于接收顺序，后发送的报文可能先被接收方所接收，这时也会产生重复ACK，所以这个值不能定得太小。 这大概率证明了这个ACK之后的那个报文丢失，所以无需等到超时重传计时器，而是立即重传。 丢失报文后面至少有三个报文得到了接收，并且ACK也成功的传送了回来，侧面证明了网络状况还好，所以没有必要直接进行慢启动。 每一个新的重复ACK，就代表一个报文被接收，更加证明了网络状况并不差。 看上面书中所举的例子，可以看到它的各个变量的变化情况就与上面的算法描述一致。 按每条路由进行度量 当一个TCP连接关闭时，如果已经发送了足够多的数据来获得有意义统计资料，且目的结点的路由表项不是一个默认的表项， 那么下列信息就保存在路由表项中以备下次使用：被平滑的RTT、被平滑的均值偏差以及慢启动门限。 所谓“足够多的数据”是指16个窗口的数据，这样就可得到16个RTT采样，从而使被平滑的RTT过滤器能够集中在正确结果的5%以内。 当建立一个新的连接时，不论是主动还是被动， 如果该连接将要使用的路由表项已经有这些度量的值，则用这些度量来对相应的变量进行初始化。 ICMP的差错 TCP能够遇到的最常见的ICMP差错就是源站抑制、主机不可达和网络不可达。 当前基于伯克利的实现对这些错误的处理是： 一个接收到的源站抑制引起拥塞窗口cwnd被置为1个报文段大小来发起慢启动，但是慢启动门限ssthresh没有变化， 所以窗口将打开直至它或者开放了所有的通路（受窗口大小和往返时间的限制）或者发生了拥塞。 一个接收到的主机不可达或网络不可达实际上都被忽略，因为这两个差错都被认为是短暂现象。 这有可能是由于中间路由器被关闭而导致选路协议要花费数分钟才能稳定到另一个替换路由。 在这个过程中就可能发生这两个ICMP差错中的一个，但是连接并不必被关闭。 相反，TCP试图发送引起该差错的数据，尽管最终有可能会超时。 当前基于伯克利的实现记录发生的ICMP差错，如果连接超时，ICMP差错被转换为一个更合适的的差错码而不是“连接超时”。 需要了解到对于ICMP的差错，TCP有针对的处理就行了。 重新分组 当TCP超时并重传时，它不一定要重传同样的报文段。相反，TCP允许进行重新分组而发送一个较大的报文段， 这将有助于提高性能（当然，这个较大的报文段不能够超过接收方声明的MSS）。 在协议中这是允许的，因为TCP是使用字节序号而不是报文段序号来进行识别它所要发送的数据和进行确认。 观察书中所举的例子： 可以看到13:27字节在重传过程中，又键入了6个字符，于是重传就变成了13:33字节，这就是TCP的重新分组。 参考带外数据：TCP紧急模式分析 TCP带外数据]]></content>
      <categories>
        <category>network</category>
      </categories>
      <tags>
        <tag>TCP/IP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[练习使用FFmpeg将视频转码为hls，并添加水印]]></title>
    <url>%2F2019%2F08%2F08%2F2019-08-08-ffmpeg-practice%2F</url>
    <content type="text"><![CDATA[前提准备 一个高清的视频（这样转分辨率后起码不会糊）。 一个高清的水印图片（理由同上）。 FFmpeg，直接官网下一个就可以，这里我使用的就是FFmpeg.exe。 下面将各个部分拆开练习，最后再合在一起。 1234ffmpeg -i 11.mp4 -vf &quot;movie=wm.png [logo];[logo][in]scale2ref=w=oh*mdar:h=ih/10[logo-rescale][video-out];[video-out][logo-rescale]overlay=x=main_w/10-w/2:y=main_h/10-h/2 [out]&quot;-codec:v libx264 -s 1920x1080 -codec:a mp3 -map 0 -f ssegment -segment_format mpegts -segment_list playlist.m3u8 -segment_time 10 1080P%03d.ts 一些基本命令 1.转格式（容器转换）1$ ffmpeg -i 11.mp4 out.avi 直接在输出文件的后缀指定需要转的格式（容器）就行。 这样转码是会使用默认编码器，需要设置转码为copy来达到视频质量无损失。参考设置视频编码。 2.转分辨率1$ ffmpeg -i 11.mp4 -s 1280x720 out.mp4 通过-s 1280x720指定分辨率为720P。 3.设置视频编码1$ ffmpeg -i 11.mp4 -s 1280x720 -codec:v libx264 -codec:a mp3 out.mp4 这里通过-codec:v libx264指定视频编码格式为x264，通过-codec:a mp3指定音频编码格式为mp3。 1$ ffmpeg -i 4K_BXJG.mp4 -codec:v copy -codec:a copy out.avi 这里的转码格式被指定为copy，实际上FFmpeg就会省去编解码过程，速度非常快。 Stream copy is a mode selected by supplying the copy parameter to the -codec option. It makes ffmpeg omit the decoding and encoding step for the specified stream, so it does only demuxing and muxing. It is useful for changing the container format or modifying container-level metadata. Since there is no decoding or encoding, it is very fast and there is no quality loss. 上面是官网中的描述，可以看到使用copy可以无损的转化容器格式。 4.转图片分辨率1$ ffmpeg -i wm.png -s 100x100 out.png 可以看到转图片其实和转视频是同理的。 5.流选择一个正常的视频至少有两个流，一个视频流，一个音频流。 通过输入而不输出可以查看媒体文件信息： 123456789101112131415161718$ ffmpeg -i 4K_BXJG.mp4Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '4K_BXJG.mp4': Metadata: major_brand : mp42 minor_version : 0 compatible_brands: mp42mp41 creation_time : 2017-02-04T16:09:19.000000Z Duration: 00:00:54.02, start: 0.000000, bitrate: 39655 kb/s Stream #0:0(eng): Video: h264 (High) (avc1 / 0x31637661), yuv420p(tv, bt709), 3840x2160 [SAR 1:1 DAR 16:9], 39366 kb/s, 23.98 fps, 23.98 tbr, 24k tbn, 47.95 tbc (default) Metadata: creation_time : 2017-02-04T16:09:19.000000Z handler_name : ?Mainconcept Video Media Handler encoder : AVC Coding Stream #0:1(eng): Audio: aac (mp4a / 0x6134706D), 48000 Hz, stereo, fltp, 317 kb/s (default) Metadata: creation_time : 2017-02-04T16:09:19.000000Z handler_name : #Mainconcept MP4 Sound Media Handler 可以看到这个mp4文件包含两个流，0号流为视频流，1号流为音频流。 同时输入两个媒体文件试试： 1234567891011121314151617181920212223242526272829303132$ ffmpeg -i 4K_Z4.mp4 -i 4K_BXJG.mp4Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '4K_Z4.mp4': Metadata: major_brand : isom minor_version : 512 compatible_brands: mp41mp42 creation_time : 2015-01-05T13:14:01.000000Z Duration: 00:02:22.56, start: 0.000000, bitrate: 44851 kb/s Stream #0:0(eng): Video: h264 (High) (avc1 / 0x31637661), yuv420p(tv, bt709), 3840x2160 [SAR 1:1 DAR 16:9], 44717 kb/s, 50 fps, 50 tbr, 30k tbn, 100 tbc (default) Metadata: creation_time : 2015-01-05T13:14:01.000000Z encoder : AVC Coding Stream #0:1(eng): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 128 kb/s (default) Metadata: creation_time : 2015-01-05T13:14:01.000000ZInput #1, mov,mp4,m4a,3gp,3g2,mj2, from '4K_BXJG.mp4': Metadata: major_brand : mp42 minor_version : 0 compatible_brands: mp42mp41 creation_time : 2017-02-04T16:09:19.000000Z Duration: 00:00:54.02, start: 0.000000, bitrate: 39655 kb/s Stream #1:0(eng): Video: h264 (High) (avc1 / 0x31637661), yuv420p(tv, bt709), 3840x2160 [SAR 1:1 DAR 16:9], 39366 kb/s, 23.98 fps, 23.98 tbr, 24k tbn, 47.95 tbc (default) Metadata: creation_time : 2017-02-04T16:09:19.000000Z handler_name : ?Mainconcept Video Media Handler encoder : AVC Coding Stream #1:1(eng): Audio: aac (mp4a / 0x6134706D), 48000 Hz, stereo, fltp, 317 kb/s (default) Metadata: creation_time : 2017-02-04T16:09:19.000000Z handler_name : #Mainconcept MP4 Sound Media Handler 可以发现流编号上面的规律，0:0、0:1表示第一个文件的两个流，1:0、1:1则表示第二个文件的两个流。 提取视频流： 1$ ffmpeg -i 1080_BXJG.mp4 -map 0:0 -codec copy out.mp4 上面的命令将原视频的视频流单独分离出来。 合并音视频流： 1$ ffmpeg -i 1080_BXJG.mp4 -i 4K_Z4.mp4 -map 0:0 -codec copy -map 1:1 -codec copy out.mp4 提取第一个视频的视频流，第二个视频的音频流，合并为out.mp4。 filter设置 1.过滤器图先看官方文档说的： Filtering in FFmpeg is enabled through the libavfilter library. In libavfilter, a filter can have multiple inputs and multiple outputs. To illustrate the sorts of things that are possible, we consider the following filtergraph. 12345 [main]input --&gt; split ---------------------&gt; overlay --&gt; output | ^ |[tmp] [flip]| +-----&gt; crop --&gt; vflip -------+ This filtergraph splits the input stream in two streams, then sends one stream through the crop filter and the vflip filter, before merging it back with the other stream by overlaying it on top. filter即常说的滤镜，Filtering过程可以是一个多输入多输出的过程，多个输入的视频经过处理合并成一个或多个输出。 上面的过程的命令为： 1$ ffmpeg -i INPUT -vf "split [main][tmp]; [tmp] crop=iw:ih/2:0:0, vflip [flip]; [main][flip] overlay=0:H/2" OUTPUT 它的效果如下： 它将原视频复制（split），裁剪一半（crop），反转（cflip），覆盖（overlay）到原视频之上。 可以看到主要的处理过程在-vf之后，它就代表filtergraph也就是过滤器图（相似的还有-filter_complex），相当于定义了一个视频处理的过程。 注意到：-vf不能有多个输入，而-filter_complex可以。 2.label（标签）标签用于对流进行标记，目前我只知道它一些简单的用法。 例如上面的命令中的过滤器图： &quot;split [main][tmp]; [tmp] crop=iw:ih/2:0:0, vflip [flip]; [main][flip] overlay=0:H/2&quot; 它将原视频进行split，将它复制为两个流，一个命名为[main]，一个命名为[tmp]。 它将[tmp]流进行crop操作和vfilp操作，处理之后的流命名为[flip]。 输入[main]和[flip]流，进行overlay（覆盖）操作，根据这里[main][flip]的先后关系，这里将[flip]覆盖到[main]之上。 例如下面的命令，将第二个输入视频分辨率降低一半，然后覆盖到第一个视频之上： 1$ ffmpeg -i 1080_BXJG.mp4 -i 1080_BXJG.mp4 -filter_complex "[1:0]scale=w=iw/2:h=ih/2[scaled]; [0][scaled] overlay" tmp1.mp4 其中&quot;[1:0]scale=w=iw/2:h=ih/2[scaled]; [0][scaled] overlay&quot;： [1:0]代表第二个视频的第一个流（视频流），将它的scale变为一半（iw/2和ih/2），然后标记为[scaled]。 [0]就代表第一个输入的媒体文件，将[scaled]置于它之上然后输出。 3.将视频分辨率降低一半可以利用scale来完成，其中iw、ih分别代表输入的宽和高。 1$ ffmpeg -i 11.mp4 -vf "[in]scale=w=iw/2:h=ih/2[out]" out.mp4 为了展示效果，这里将降低了分辨率的视频覆盖到原视频之上： 1$ ffmpeg -i 1080_BXJG.mp4 -filter_complex "[0:0]scale=w=iw/2:h=ih/2[scaled]; [0][scaled] overlay" tmp1.mp4 4.为视频添加水印为视频添加水印，也就是将水印覆盖到原视频之上，即overlay滤镜。 简单覆盖： 1$ ffmpeg -i 1080_BXJG.mp4 -i wm.png -filter_complex overlay out.mp4 简单覆盖时，没有对水印大小进行调整，会造成下面的结果： 大小调整-1： 1$ ffmpeg -i 1080_BXJG.mp4 -i logo.png -filter_complex '[1]scale=w=200:h=-1[scaled]; [0][scaled]overlay' out.mp4 使用scale滤镜来进行分辨率的调整，再将它覆盖到原视频之上： 可以看到这里成功的对水印的大小进行了调整，但是又有一点太小了。 那么问题来了，难道要不断的试来找到一个健康的分辨率吗？ 大小调整-2： 1$ ffmpeg -i 1080_BXJG.mp4 -i logo.png -filter_complex '[1][0]scale2ref=w=oh*mdar:h=ih/8[scaled][0-out]; [0-out][scaled]overlay' out.mp4 scale2ref滤镜有两个输入，它对第一个输入进行rescale， 但它与scale滤镜的区别在于，它使用第二个输入作为参考，来进行第一个输入的rescale。 这里的[1][0]scale2ref=w=oh*mdar:h=ih/8[scaled][0-out]中，ih指的就是参考输入[0]的宽度， 为了保证水印的横纵比，这里使用oh*mdar来定义水印的宽度。 水印位置调整-1： 可以看到上面水印都是贴着视频的左上角的，这里来对它的位置进行调整： 1$ ffmpeg -i 1080_BXJG.mp4 -i logo.png -filter_complex '[1][0]scale2ref=w=oh*mdar:h=ih/8[scaled][0-out]; [0-out][scaled]overlay=x=300:y=300' out.mp4 可以通过指定overlay的坐标来确定水印覆盖的位置（默认x=0，y=0，即左上角），这里将坐标指定在300，300，效果如下： 水印位置调整-2： 1$ ffmpeg -i 1080_BXJG.mp4 -i logo.png -filter_complex '[1][0]scale2ref=w=oh*mdar:h=ih/8[scaled][0-out]; [0-out][scaled]overlay=x=main_w/10-w/2:y=main_h/10-h/2' out.mp4 这里main_w和main_h指的就是视频输入（[0-out]）的宽和高，w和h则是水印输入（[scaled]）的宽和高， 这里将水印的中心点指定在视频输入的1/10宽和1/10高的位置。 转hlshls即包括一个m3u(8)的索引文件，TS媒体分片文件和key加密串文件。 在ffmpeg中可以使用hls或者segment来实现。 使用segment muxer1$ ffmpeg -i 1080_BXJG.mp4 -f segment -segment_format mpegts -segment_list playlist.m3u8 -segment_list_size 0 -segment_time 5 out%03d.ts 上面的命令将输入视频切割为5秒一片的ts文件。其中，-segment_list playlist.m3u8设置文件list输出到playlist.m3u8， -segment_list_size 0设置playlist.m3u8里面将包含所有切分出来的分片， -segment_time 5设置分片大小为5秒。 得到如下结果： 使用hls muxer1$ ffmpeg -i 1080_BXJG.mp4 -map 0 -f hls -hls_segment_type mpegts -hls_list_size 6 -hls_time 5 -hls_segment_filename 'out%03d.ts' playlist.m3u8 参数基本一样，就不多解释了，效果如下： 转码，水印，hls将上面的命令进行总结，可以融合得到下面这条命令： 12345$ ffmpeg -i 1080_BXJG.mp4 -i logo.png \ -filter_complex '[1][0]scale2ref=w=oh*mdar:h=ih/8[scaled][0-out]; \ [0-out][scaled]overlay=x=main_w/10-w/2:y=main_h/10-h/2[over];[over]scale=w=1920:h=-1[out]' \ -map [out] -c:v h264 -c:a mp3 \ -f hls -hls_segment_type mpegts -hls_list_size 0 -hls_time 5 -hls_segment_filename 'out%03d.ts' playlist.m3u8 这条命令将一个视频文件添加水印，并切割为5秒的ts小文件，同时将分辨率调整为1080P。 基于nginx的m3u8的点播、直播的实现首先看我这里nginx的配置： 1234567891011121314151617181920212223$ cat /etc/nginx/nginx.conf ... server &#123; listen 11111 default_server; listen [::]:11111 default_server; server_name long; location / &#123; root html; index index.html index.htm; &#125; location /hls &#123; types &#123; application/vnd.apple.mpegusr m3u8; video/mp2t ts; &#125; root /mycephfs; add_header Cache-Control no-cache; &#125; &#125; ... 我这里将nginx的服务起在11111端口，文件目录为/mycephfs/hls，浏览器打开192.168.90.233:11111： 点播： 将对应的hls文件放入nginx的文件目录下： 12345678910111213$ ll /mycephfs/hls/videos/BXJG-hls/-rw-rw-r-- 1 cluster cluster 3938600 Aug 13 12:47 out000.ts-rw-rw-r-- 1 cluster cluster 1366196 Aug 13 12:47 out001.ts-rw-rw-r-- 1 cluster cluster 926088 Aug 13 12:47 out002.ts-rw-rw-r-- 1 cluster cluster 3286992 Aug 13 12:47 out003.ts-rw-rw-r-- 1 cluster cluster 7581852 Aug 13 12:47 out004.ts-rw-rw-r-- 1 cluster cluster 1721516 Aug 13 12:47 out005.ts-rw-rw-r-- 1 cluster cluster 3373848 Aug 13 12:48 out006.ts-rw-rw-r-- 1 cluster cluster 3193556 Aug 13 12:48 out007.ts-rw-rw-r-- 1 cluster cluster 5041220 Aug 13 12:48 out008.ts-rw-rw-r-- 1 cluster cluster 3229088 Aug 13 12:48 out009.ts-rw-rw-r-- 1 cluster cluster 370 Aug 13 12:48 playlist.m3u8 直接使用potplayer打开链接http://192.168.90.233:11111/hls/videos/BXJG-hls/playlist.m3u8： 点播效果完成。 直播： 这里直播和点播的十分相似，它通过不断的修改.m3u8文件来达到直播的目的。 同样的，使用一个本地的视频文件当作输入流来进行直播，需要加入-re来模拟直播流的输入速度，-stream_loop来不断重复输入。 另外，这个过程会不断产生新的ts文件，需要将使用过的小文件删除，添加-hls_list_size 5将.m3u8文件中的数量控制为5个， 添加-hls_flags delete_segments开启自动删除过时的ts小文件。 1234$ ffmpeg -re -stream_loop -1 -i 1080_MS.mp4 -i logo.png \ -filter_complex '[1][0]scale2ref=w=oh*mdar:h=ih/8[scaled][0-out];[0-out][scaled]overlay=x=main_w/10-w/2:y=main_h/10-h/2[over]' \ -map [over] -c:v h264 -map 0 -c:a copy \ -f hls -hls_segment_type mpegts -hls_flags delete_segments -hls_list_size 5 -hls_time 10 -hls_segment_filename 'out%05d.ts' live.m3u8 产生ts小文件的效果如下： 1234567891011$ ls /mycephfs/hls/videos/MS/total 49306-rw-rw-r-- 1 cluster cluster 227 Aug 14 16:41 live.m3u8-rw-rw-r-- 1 cluster cluster 6638092 Aug 14 16:40 out00017.ts-rw-rw-r-- 1 cluster cluster 6406100 Aug 14 16:40 out00018.ts-rw-rw-r-- 1 cluster cluster 7560044 Aug 14 16:40 out00019.ts-rw-rw-r-- 1 cluster cluster 7581664 Aug 14 16:41 out00020.ts-rw-rw-r-- 1 cluster cluster 9148832 Aug 14 16:41 out00021.ts-rw-rw-r-- 1 cluster cluster 8171984 Aug 14 16:41 out00022.ts-rw-rw-r-- 1 cluster cluster 4980736 Aug 14 16:41 out00023.ts 可以看到ffmpeg同一时刻只会保留6个ts文件。 同样的，使用potplayer打开连接http://192.168.90.233:11111/hls/videos/MS/live.m3u8，就可以看到直播， 和点播十分相似，不同之处在于，这里会不断的播放下去： 参考FFmpeg Documentation 使用FFMPEG生成HLS]]></content>
      <categories>
        <category>media</category>
      </categories>
      <tags>
        <tag>ffmpeg</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph+iscsi的简单使用]]></title>
    <url>%2F2019%2F08%2F06%2F2019-08-06-ceph-iscsi%2F</url>
    <content type="text"><![CDATA[前提准备首先看到ceph官网给出的大体结构： 可以看到主要分为ceph集群、rbd、iscsi网关、initiator（也就是客户端）构成。 那么所需的准备就如下： 一个HEALTH_OK的ceph集群，还有剩余的存储空间（给创建的rbd使用）。 这里是我所搭建的集群： 1234567891011121314151617$ sudo ceph -s cluster: id: 51e9f534-b15a-4273-953c-9b56e9312510 health: HEALTH_OK services: mon: 3 daemons, quorum node1,node2,node3 mgr: node1(active), standbys: node2, node3 mds: cephfs-1/1/1 up &#123;0=node1=up:active&#125; osd: 6 osds: 6 up, 6 in data: pools: 2 pools, 64 pgs objects: 508 objects, 1.9 GiB usage: 26 GiB used, 6.0 TiB / 6.0 TiB avail pgs: 64 active+clean 两台linux主机，作为iscsi网关，可以是集群中的主机。 一台linux主机，作为linux系统下的客户端。 一台windows主机，作为windows系统下的客户端。 配置ceph-iscsi网关 修改osd配置安装官网所述，先修改osd的配置： 123[osd]osd heartbeat grace = 20osd heartbeat interval = 5 将上述配置添加到所有ceph节点的/etc/ceph/ceph.conf文件中， 当然可以使用ceph-deploy来推送。 1234567891011121314151617$ vim ceph.conf$ cat ceph.conf[global]fsid = 51e9f534-b15a-4273-953c-9b56e9312510mon_initial_members = node1, node2, node3mon_host = 192.168.90.233,192.168.90.234,192.168.90.235auth_cluster_required = cephxauth_service_required = cephxauth_client_required = cephxpublic_network = 192.168.0.0/16[osd]osd heartbeat grace = 20osd heartbeat interval = 5$ ceph-deploy --overwrite-conf config push node1 node2 node3 下载所需要的相关rpm包这里直接选择Using the Command Line Interface，感觉这个更靠谱一些。 按照官网所述，yum的repo中需要有以下rpm包： 直接yum install试试可以发现，只有targetcli和python-rtslib能装上，而且版本都比官网说的要低，好吧，麻烦来了。 经过一段几个小时*的搜索……从下面链接中找到了rpm包： Build new RPM for 3.0 Missing ceph-iscsi-cli package 新建repo文件： 1234567891011121314151617181920$ sudo vim /etc/yum.repo.d/iscsi.repo$ cat /etc/yum.repo.d/iscsi.repo[ceph-iscsi]name=Ceph-iscsibaseurl=https://4.chacra.ceph.com/r/ceph-iscsi/master/88f3f67981c7da15448f140f711a1a8413d450b0/centos/7/flavors/default/noarch/priority=1gpgcheck=0[tcmu-runner]name=tcmu-runnerbaseurl=https://3.chacra.ceph.com/r/tcmu-runner/master/eef511565078fb4e2ed52caaff16e6c7e75ed6c3/centos/7/flavors/default/x86_64/priority=1gpgcheck=0[python-rtslib]name=python-rtslibbaseurl=https://2.chacra.ceph.com/r/python-rtslib/master/67eb1605c697b6307d8083b2962f5170db13d306/centos/7/flavors/default/noarch/priority=1gpgcheck=0 这里我使用的是本地源，将上面的包下载到本地源： 12$ sudo yum install --downloadonly --downloaddir=yum/ceph-iscsi/ targetcli python-rtslib tcmu-runner ceph-iscsi$ createrepo -p -d -o yum/ yum/ 注意到这里没有包含targetcli的repo，因为没有找到，使用yum基础的Base源或者是Ceph源可以安装targetcli-2.1.fb46-7.el7.noarch.rpm， 虽然官网需要的是targetcli-2.1.fb47 or newer package，但在后续使用中发现没有影响，所以这里就不用管targetcli了。 那么这里下载下来的就是： targetcli-2.1.fb46-7.el7.noarch.rpm python-rtslib-2.1.fb68-1.noarch.rpm tcmu-runner-1.4.0-0.1.51.geef5115.el7.x86_64.rpm ceph-iscsi-3.2-8.g88f3f67.el7.noarch.rpm ceph-iscsi网关初始配置如果使用的不是集群内的节点作为ceph-iscsi网关，那就需要进行一些初始的配置。 安装ceph。 从集群中的一台机器上拷贝/etc/ceph/ceph.conf到本机的/etc/ceph/ceph.conf。 从集群中的一台机器上拷贝/etc/ceph/ceph.client.admin.keyring到本机的/etc/ceph/ceph.client.admin.keyring。 当然第2和3步可以直接在deploy节点使用ceph-deploy admin {node-gateway}，{node-gateway}就表示网关节点的名字。 可见这里就是为了将ceph-iscsi网关节点变成一个admin节点。 这时在网关节点上应该可以执行相关命令操作ceph集群，例如sudo ceph -s查询当前集群的状态。 安装配置iscsi这里官网建议先切换到root用户，方便一点： 1$ su root 在两个网关节点上都安装iscsi（注意到上面已经将相关包下载到了本地源，所以可以直接yum安装）： 1# yum install -y ceph-iscsi 服务启动： 先创建rbd pool，如果没有的话。 12345678# ceph osd lspools1 cephfs_data2 cephfs_metadata# ceph osd pool create rbd 128pool 'rbd' created 创建并修改/etc/ceph/iscsi-gateway.cfg文件： 12345678910111213141516171819202122232425262728293031# vim /etc/ceph/iscsi-gateway.cfg# cat /etc/ceph/iscsi-gateway.cfg[config]# Name of the Ceph storage cluster. A suitable Ceph configuration file allowing# access to the Ceph storage cluster from the gateway node is required, if not# colocated on an OSD node.cluster_name = ceph# Place a copy of the ceph cluster's admin keyring in the gateway's /etc/ceph# drectory and reference the filename heregateway_keyring = ceph.client.admin.keyring# API settings.# The API supports a number of options that allow you to tailor it to your# local environment. If you want to run the API under https, you will need to# create cert/key files that are compatible for each iSCSI gateway node, that is# not locked to a specific node. SSL cert and key files *must* be called# 'iscsi-gateway.crt' and 'iscsi-gateway.key' and placed in the '/etc/ceph/' directory# on *each* gateway node. With the SSL files in place, you can use 'api_secure = true'# to switch to https mode.# To support the API, the bear minimum settings are:api_secure = false# Additional API configuration options are as follows, defaults shown.# api_user = admin# api_password = admin# api_port = 5001trusted_ip_list = 192.168.90.234,192.168.90.235 上面trusted_ip_list填写的就是两台网关的ip（这里不讨论多网卡的情况）。 在另外一台网关上复制这个文件： 1$ sudo scp cluster@node2:/etc/ceph/iscsi-gateway.cfg /etc/ceph/iscsi-gateway.cfg 在两台网关上都开启rbd-target-api服务： 123# systemctl daemon-reload# systemctl enable rbd-target-api# systemctl start rbd-target-api 配置：（在其中一台网关配置就行） 12345678910111213141516171819202122232425262728293031# gwcli/&gt; cd /iscsi-targets/iscsi-targets&gt; create iqn.2003-01.com.redhat.iscsi-gw:iscsi-igwok/iscsi-targets&gt; cd iqn.2003-01.com.redhat.iscsi-gw:iscsi-igw/gateways/iscsi-target...-igw/gateways&gt; create node2 192.168.90.234Adding gateway, sync'ing 0 disk(s) and 0 client(s)ok/iscsi-target...-igw/gateways&gt; create node3 192.168.90.235Adding gateway, sync'ing 0 disk(s) and 0 client(s)ok/iscsi-target...-igw/gateways&gt; cd /disks/disks&gt; create pool=rbd image=disk_1 size=200Gok/disks&gt; cd /iscsi-targets/iqn.2003-01.com.redhat.iscsi-gw:iscsi-igw/hosts/iscsi-target...csi-igw/hosts&gt; create iqn.1994-05.com.redhat:rh7-clientok/iscsi-target...at:rh7-client&gt; auth username=myiscsiusername password=myiscsipasswordok/iscsi-target...at:rh7-client&gt; disk add rbd/disk_1ok 配置完成，可以看到我当前的目录结构： 1234567891011121314151617181920212223/&gt; lso- / ......................................................................... [...] o- cluster ......................................................... [Clusters: 1] | o- ceph .......................................................... [HEALTH_WARN] | o- pools .......................................................... [Pools: 3] | | o- cephfs_data ... [(x3), Commit: 0.00Y/2028052096K (0%), Used: 2029431878b] | | o- cephfs_metadata .... [(x3), Commit: 0.00Y/2028052096K (0%), Used: 77834b] | | o- rbd ................ [(x3), Commit: 200G/2028052096K (10%), Used: 15352b] | o- topology ................................................ [OSDs: 6,MONs: 3] o- disks ........................................................ [200G, Disks: 1] | o- rbd ............................................................ [rbd (200G)] | o- disk_1 ................................................ [rbd/disk_1 (200G)] o- iscsi-targets ............................... [DiscoveryAuth: None, Targets: 1] o- iqn.2003-01.com.redhat.iscsi-gw:iscsi-igw ..................... [Gateways: 2] o- disks .......................................................... [Disks: 1] | o- rbd/disk_1 ............................................... [Owner: node3] o- gateways ............................................ [Up: 2/2, Portals: 2] | o- node2 ............................................. [192.168.90.234 (UP)] | o- node3 ............................................. [192.168.90.235 (UP)] o- host-groups .................................................. [Groups : 0] o- hosts .............................................. [Hosts: 1: Auth: CHAP] o- iqn.1994-05.com.redhat:rh7-client .......... [Auth: CHAP, Disks: 1(200G)] o- lun 0 ................................ [rbd/disk_1(200G), Owner: node3] linux客户端配置在作为客户端的linux主机上。 安装相关组件： 12$ sudo yum install -y iscsi-initiator-utils$ sudo yum install -y device-mapper-multipath 开启multipathd服务并进行配置： 12345678910111213141516171819$ sudo mpathconf --enable --with_multipathd y$ sudo vim /etc/multipath.conf$ sudo cat /etc/multipath.confdevices &#123; device &#123; vendor "LIO-ORG" hardware_handler "1 alua" path_grouping_policy "failover" path_selector "queue-length 0" failback 60 path_checker tur prio alua prio_args exclusive_pref_bit fast_io_fail_tmo 25 no_path_retry queue &#125;&#125; 修改客户端名称： 1234$ sudo vim /etc/iscsi/initiatorname.iscsi$ sudo cat /etc/iscsi/initiatorname.iscsiInitiatorName=iqn.1994-05.com.redhat:rh7-client 修改chap认证配置文件： 1234567891011121314151617$ sudo vim /etc/iscsi/iscsid.conf$ sudo cat /etc/iscsi/iscsid.conf...# *************# CHAP Settings# *************# To enable CHAP authentication set node.session.auth.authmethod# to CHAP. The default is None.node.session.auth.authmethod = CHAP# To set a CHAP username and password for initiator# authentication by the target(s), uncomment the following lines:node.session.auth.username = myiscsiusernamenode.session.auth.password = myiscsipassword... 发现target： 1234$ sudo iscsiadm -m discovery -t st -p 192.168.90.234192.168.90.234:3260,1 iqn.2003-01.org.linux-iscsi.rheln1192.168.90.235:3260,2 iqn.2003-01.org.linux-iscsi.rheln1 登入target： 1$ sudo iscsiadm -m node -T iqn.2003-01.org.linux-iscsi.rheln1 -l 查看是否成功： 1234567$ sudo multipath -llmpatha (360014050fedd563975249adb2e84e978) dm-2 LIO-ORG ,TCMU device size=200G features='1 queue_if_no_path' hwhandler='1 alua' wp=rw|-+- policy='queue-length 0' prio=50 status=active| `- 3:0:0:0 sdc 8:32 active ready running`-+- policy='queue-length 0' prio=10 status=enabled `- 2:0:0:0 sdb 8:16 active ready running 在fdisk中就可以直接看到这个“硬盘”： 12345678$ sudo fdisk -l...Disk /dev/mapper/mpatha: 10.7 GB, 10737418240 bytes, 20971520 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 524288 bytes... windows客户端配置在控制面板-&gt;管理工具-&gt;iSCSI 发起程序： 修改发起程序名称： 添加发现目标门户： 可以看到出现目标： 连接到该目标： 修改高级设置： 可以看到已连接： 可以在磁盘管理中看到硬盘已经连接： 这里将它分为F盘，并且成功往里面放了一个文件： Health Warning注意到此时ceph集群可能会出现一个Health Warning： 12345678910111213141516171819202122$ sudo ceph -s cluster: id: 51e9f534-b15a-4273-953c-9b56e9312510 health: HEALTH_WARN application not enabled on 1 pool(s) services: mon: 3 daemons, quorum node1,node2,node3 mgr: node1(active), standbys: node2, node3 mds: cephfs-1/1/1 up &#123;0=node1=up:active&#125; osd: 6 osds: 6 up, 6 in tcmu-runner: 2 daemons active data: pools: 3 pools, 192 pgs objects: 559 objects, 2.1 GiB usage: 27 GiB used, 6.0 TiB / 6.0 TiB avail pgs: 192 active+clean io: client: 2.5 KiB/s rd, 2 op/s rd, 0 op/s wr 从官网中可以看到这个warning发生的原因New in Luminous: pool tags， 另外使用ceph health detail也可以看到解决方法： 123456$ sudo ceph health detailHEALTH_WARN application not enabled on 1 pool(s)POOL_APP_NOT_ENABLED application not enabled on 1 pool(s) application not enabled on pool 'rbd' use 'ceph osd pool application enable &lt;pool-name&gt; &lt;app-name&gt;', where &lt;app-name&gt; is 'cephfs', 'rbd', 'rgw', or freeform for custom applications. 这里就将所建立的rbd池标记为rbd： 12345678910111213141516171819202122232425$ sudo ceph osd pool application enable rbd rbdenabled application 'rbd' on pool 'rbd'$ sudo ceph -s cluster: id: 51e9f534-b15a-4273-953c-9b56e9312510 health: HEALTH_OK services: mon: 3 daemons, quorum node1,node2,node3 mgr: node1(active), standbys: node2, node3 mds: cephfs-1/1/1 up &#123;0=node1=up:active&#125; osd: 6 osds: 6 up, 6 in tcmu-runner: 2 daemons active data: pools: 3 pools, 192 pgs objects: 559 objects, 2.1 GiB usage: 27 GiB used, 6.0 TiB / 6.0 TiB avail pgs: 192 active+clean io: client: 2.0 KiB/s rd, 1 op/s rd, 0 op/s wr 问题解决。 参考CEPH ISCSI GATEWAY 看Ceph如何实现原生的ISCSI Build new RPM for 3.0 Missing ceph-iscsi-cli package New in Luminous: pool tags]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>centos</tag>
        <tag>ceph</tag>
        <tag>iscsi</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph+samba的简单使用]]></title>
    <url>%2F2019%2F08%2F06%2F2019-08-06-ceph-samba%2F</url>
    <content type="text"><![CDATA[前提准备有一个HEALTH_OK的ceph集群，还有剩余的存储空间， 并且创建了CephFS。 这里是我所搭建的集群： 1234567891011121314151617$ sudo ceph -s cluster: id: 51e9f534-b15a-4273-953c-9b56e9312510 health: HEALTH_OK services: mon: 3 daemons, quorum node1,node2,node3 mgr: node1(active), standbys: node2, node3 mds: cephfs-1/1/1 up &#123;0=node1=up:active&#125; osd: 6 osds: 6 up, 6 in data: pools: 2 pools, 64 pgs objects: 316 objects, 1.1 GiB usage: 23 GiB used, 6.0 TiB / 6.0 TiB avail pgs: 64 active+clean 一台linux主机（简称smb服务器），能够与Ceph集群通信，用于开启smb服务。（当然也可以是Ceph集群中的某一节点， 我这里使用的就是Ceph集群中的node1） 一台windows主机，能够直接ping通smb服务器（一定要直接ping通最好，也就是同一个局域网下， 因为windows使用smb服务的时候，端口是默认定死的，你端口映射是没用的）。 搭建smb服务器首先尝试搭建smb服务器，这里和ceph暂时没有关系。 第一步，在作为smb服务器的linux主机上安装samba： 1$ sudo yum install -y samba samba-client 修改smb.conf： 123456789101112131415161718192021222324252627282930313233343536373839$ sudo vim /etc/samba/smb.conf$ sudo cat /etc/samba/smb.conf[global]workgroup = WORKGROUP netbios name = longserver string = Linux Samba testsecurity = user [samba]path = /smb/sambawriteable = yesbrowseable = yesguest ok = yes// 检查配置文件正确性$ testparmLoad smb config files from /etc/samba/smb.confrlimit_max: increasing rlimit_max (1024) to minimum Windows limit (16384)Processing section "[samba]"Loaded services file OK.Server role: ROLE_STANDALONEPress enter to see a dump of your service definitions# Global parameters[global] netbios name = LONG security = USER server string = Linux Samba test idmap config * : backend = tdb[samba] guest ok = Yes path = /smb/samba read only = No 注意这里的workgroup和netbios name字段填写的是windows主机上所显示的工作组和用户名， 可以在windows主机上通过我的电脑-&gt;属性，进行查看。 path字段即共享文件夹位置，这里将它创建出来。 1$ sudo mkdir -p /smb/samba 将共享文件夹的用户和组改为nobody，权限改为777： 1234567// 确定nobody的组名用户名都叫nobody$ id nobodyuid=99(nobody) gid=99(nobody) groups=99(nobody)// 设置权限$ sudo chown -R nobody:nobody /smb$ sudo chmod -R 777 /smb 启动smb服务： 12$ sudo systemctl enable smb$ sudo systemctl start smb 本机测试服务是否成功启动，注意因为共享文件夹设置的guest ok，所以这里不用输入密码也可以连接： 12345678910111213141516171819202122232425262728293031$ sudo smbclient -L \\192.168.90.233Enter WORKGROUP\cluster's password: Anonymous login successful Sharename Type Comment --------- ---- ------- samba Disk IPC$ IPC IPC Service (Linux Samba test)Reconnecting with SMB1 for workgroup listing.Anonymous login successful Server Comment --------- ------- Workgroup Master --------- -------$ smbclient //192.168.90.233/sambaEnter WORKGROUP\cluster's password: Anonymous login successfulTry "help" to get a list of possible commands.smb: \&gt; ls . D 0 Mon Aug 5 17:57:17 2019 .. DR 0 Mon Aug 5 17:47:05 2019 test D 0 Mon Aug 5 17:50:02 2019 public D 0 Mon Aug 5 17:57:25 2019 2030415872 blocks of size 1024. 2029215744 blocks availablesmb: \&gt; ok，smb服务配置成功。 使用windows访问CephFS通过上面的步骤，显而易见，只要将CephFS挂载到smb服务所共享的目录下即可： 1$ sudo mount -t ceph 192.168.90.233:6789:/ /smb/samba -o name=admin,secret=AQCSvDZdqDJ+LRAAwl2YpecB2kvb7Rmp4nGJXQ== 挂载之后，就相当于将CephFS共享了出来。 这里可以创建一个用户来访问共享： 12$ sudo adduser smbuser$ sudo smbpasswd -a smbuser 在windows中打开文件资源管理器（其实就是任意文件夹）： 在路径栏输入smb服务器位置： 回车，输入刚创建的用户名和密码，即可： 创建文件： 修改文件内容并成功保存： 到这里，就完成了使用windows来对CephFS进行访问。 参考Centos7 配置samba服务]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>centos</tag>
        <tag>ceph</tag>
        <tag>cephfs</tag>
        <tag>nfs</tag>
        <tag>samba</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph-mimic-13.2.5中CephFS的初步使用]]></title>
    <url>%2F2019%2F08%2F05%2F2019-08-05-ceph-fs-use%2F</url>
    <content type="text"><![CDATA[前提准备有一个HEALTH_OK的ceph集群，并且还有剩余的存储空间。 这里是我所搭建的集群： 123456789101112131415161718$ sudo ceph -s cluster: id: a20b153c-c907-41bb-a5b2-753a40e2085c health: HEALTH_WARN clock skew detected on mon.node2 services: mon: 4 daemons, quorum node1,node2,node3,node4 mgr: node2(active), standbys: node3, node1, node4 osd: 4 osds: 4 up, 4 in rgw: 1 daemon active data: pools: 6 pools, 48 pgs objects: 492 objects, 1.1 GiB usage: 7.5 GiB used, 42 GiB / 50 GiB avail pgs: 48 active+clean 创建mds(METADATA SERVER)要使用CephFS，至少需要有一个mds，可以使用ceph-deploy工具进行创建： 1[deploy]$ ceph-deploy mds create node1 查看mds状态： 123[node1]$ sudo ceph mds stat, 1 up:standby 可以看到mds创建成功了。 创建CEPH FILESYSTEM根据官方文档所述A Ceph filesystem requires at least two RADOS pools, one for data and one for metadata.， 这里需要创建两个pool。 1234567[node1]$ ceph osd pool create cephfs_data 16pool 'cephfs_data' created[node1]$ ceph osd pool create cephfs_metadata 16pool 'cephfs_metadata' created 然后直接就可以进行CephFS的创建： 1234[node1]$ ceph fs new cephfs cephfs_metadata cephfs_data[node1]$ ceph fs lsname: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ] 此时sudo ceph mds stat命令和sudo ceph -s的输出就发生了变化： 12345678910111213141516171819202122[node1]$ sudo ceph mds statcephfs-1/1/1 up &#123;0=node1=up:active&#125;[node1]$ sudo ceph -s cluster: id: a20b153c-c907-41bb-a5b2-753a40e2085c health: HEALTH_OK services: mon: 4 daemons, quorum node1,node2,node3,node4 mgr: node2(active), standbys: node3, node1, node4 mds: cephfs-1/1/1 up &#123;0=node1=up:active&#125; // 增加了mds的状态 osd: 4 osds: 4 up, 4 in rgw: 1 daemon active data: pools: 8 pools, 112 pgs objects: 514 objects, 1.1 GiB usage: 7.5 GiB used, 42 GiB / 50 GiB avail pgs: 112 active+clean 注：集群中可以同时创建多个CephFS，这时候就会有默认FS、优先级等设置上的问题。 挂载CephFS挂载CephFS有几种不同的方式，直接挂载、fuse、fstab。 这里一一进行叙述，但是详细的内核态用户态什么的就先不分析了。 在哪台机器上执行都可以，无所谓。 直接挂载CephFS很简单，直接使用mount命令挂载就完事了，注意端口是6789，密钥去ceph.client.admin.keyring里看，不要在要挂载的目录下去执行挂载！： 12$ sudo mkdir /mnt/mycephfs$ sudo mount -t ceph 192.168.31.203:6789:/ /mnt/mycephfs -o name=admin,secret=AQBxGTVdnr4PAhAAd4KqF0802IwTk0wVzw0cZA== 复制一个文件进去试试： 1234567891011121314151617// 复制文件进去$ sudo mkdir /mnt/mycephfs/test$ sudo cp 11.mp4 /mnt/mycephfs/test/// 卸载看是不是都没了$ sudo umount /mnt/mycephfs$ ll /mnt/mycephfs/total 0// 重新挂载// 查看文件$ sudo mount -t ceph 192.168.31.203:6789:/ /mnt/mycephfs -o name=admin,secret=AQBxGTVdnr4PAhAAd4KqF0802IwTk0wVzw0cZA==$ ll /mnt/mycephfs/test/total 196753-rw-r--r-- 1 root root 201474919 Aug 5 10:03 11.mp4 可以看到文件成功的存入了CephFS中。 使用ceph-fuse进行挂载按照官方文档所说，把密钥和conf拷贝到要执行挂载的机器上： 12[client]$ sudo scp ming@192.168.31.203:/etc/ceph/ceph.client.admin.keyring /etc/ceph/ceph.client.admin.keyring[client]$ sudo scp ming@192.168.31.203:/etc/ceph/ceph.conf /etc/ceph/ceph.conf 要注意的是，现在需要使用的ceph-fuse命令是还没有按照的，需要手动按照一下: 1[client]$ sudo yum install -y ceph-fuse 进行挂载： 1234567891011[client]$ sudo mkdir /home/ming/cephfs[client]$ sudo ceph-fuse -m 192.168.31.203:6789 /home/ming/cephfsceph-fuse[1607]: starting ceph client2019-08-05 10:21:37.388 7f7f411bec00 -1 init, newargv = 0x55b21236d6e0 newargc=7ceph-fuse[1607]: starting fuse[client]$ ll cephfs/test/total 196753-rw-r--r-- 1 root root 201474919 Aug 5 10:03 11.mp4 可以看到成功的进行了挂载，并且也看到了已有的文件。 下面进行卸载，注意到卸载所使用的命令与挂载不同： 1$ sudo fusermount -u cephfs/ 使用fstab进行挂载也就是在/etc/fstab文件中进行配置，这样在开机时就会自动进行挂载。 首先，将密钥保存到一个文件里面： 1234$ sudo vim /etc/ceph/secret.key$ sudo cat /etc/ceph/secret.keyAQBxGTVdnr4PAhAAd4KqF0802IwTk0wVzw0cZA== 修改/etc/fstab： 12345678910$ sudo vim /etc/fstab$ sudo cat /etc/fstab.../dev/mapper/centos-root / xfs defaults 0 0UUID=37492c10-b6e6-4905-99ad-36207abe5b00 /boot xfs defaults 0 0/dev/mapper/centos-swap swap swap defaults 0 0# ceph fs192.168.31.203:6789:/ /mnt/mycephfs ceph name=admin,secretfile=/etc/ceph/secret.key,noatime,_netdev 0 2 重启虚拟机： 12345678$ sudo reboot$ ll /mnt/mycephfstotal 0drwxr-xr-x 1 root root 1 Aug 5 10:03 test$ ll /mnt/mycephfs/testtotal 196753-rw-r--r-- 1 root root 201474919 Aug 5 10:03 11.mp4 可以看到完成了开机自动挂载。 参考CEPH FILESYSTEM]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>centos</tag>
        <tag>ceph</tag>
        <tag>cephfs</tag>
        <tag>nfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph-mimic-13.2.5中s3的初步使用]]></title>
    <url>%2F2019%2F08%2F02%2F2019-08-02-ceph-s3-use%2F</url>
    <content type="text"><![CDATA[前提准备有一个HEALTH_OK的ceph集群，并且还有剩余的存储空间。 这里是我所搭建的集群： 12345678910111213141516$ sudo ceph -s cluster: id: a20b153c-c907-41bb-a5b2-753a40e2085c health: HEALTH_OK services: mon: 4 daemons, quorum node1,node2,node3,node4 mgr: node2(active), standbys: node3, node1, node4 osd: 4 osds: 4 up, 4 in rgw: 1 daemon active data: pools: 6 pools, 48 pgs objects: 198 objects, 3.2 KiB usage: 4.1 GiB used, 46 GiB / 50 GiB avail pgs: 48 active+clean 创建OBJECT GATEWAY注意到OBJECT GATEWAY不需要是mon节点或者osd节点。 首先，要在节点上安装相关的包，可以使用官网推荐的ceph-deploy install --rgw &lt;gateway-node1&gt; [&lt;gateway-node2&gt; ...]来进行安装， 但是这里由于指定版本、epel包下载容易失败等问题，还是推荐直接使用本地yum源来进行ceph和ceph-radosgw的安装。 1$ sudo yum install -y ceph-13.2.5 ceph-radosgw-13.2.5 第二步，将节点设为admin节点，我这里直接使用了集群中的node1节点： 1[deploy]$ ceph-deploy admin node1 第三步，创建GATEWAY INSTANCE： 1[deploy]$ ceph-deploy rgw create node1 服务的默认端口为7480，查看当前打开端口，可以看到服务成功建立起来： 1234567891011121314[node1]$ netstat -nlptProto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 192.168.31.203:6789 0.0.0.0:* LISTEN - tcp 0 0 192.168.31.203:6800 0.0.0.0:* LISTEN - tcp 0 0 192.168.31.203:6801 0.0.0.0:* LISTEN - tcp 0 0 192.168.31.203:6802 0.0.0.0:* LISTEN - tcp 0 0 192.168.31.203:6803 0.0.0.0:* LISTEN - tcp 0 0 192.168.31.203:6804 0.0.0.0:* LISTEN - tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN - tcp 0 0 0.0.0.0:7480 0.0.0.0:* LISTEN - tcp 0 0 127.0.0.1:25 0.0.0.0:* LISTEN - tcp6 0 0 :::22 :::* LISTEN - tcp6 0 0 ::1:25 :::* LISTEN - 查看当前pool，应该可以看到相关的pool被创建： 12345678$ sudo ceph osd pool lsdefault.rgw.meta.rgw.rootdefault.rgw.controldefault.rgw.logdefault.rgw.buckets.indexdefault.rgw.buckets.data 创建RADOSGW USERs3是一个Web服务接口，自然的，就需要相应的访问权限来与它交互，所以在使用它之前，需要创建用户。 12345678910111213141516171819202122232425262728293031[node1]$ sudo radosgw-admin user create --uid="testuser" --display-name="First User"&#123; "user_id": "testuser", "display_name": "First User", "email": "", "suspended": 0, "max_buckets": 1000, "subusers": [], "keys": [&#123; "user": "testuser", "access_key": "I0PJDPCIYZ665MW88W9R", "secret_key": "dxaXZ8U90SXydYzyS5ivamEP20hkLSUViiaR+ZDA" &#125;], "swift_keys": [], "caps": [], "op_mask": "read, write, delete", "default_placement": "", "placement_tags": [], "bucket_quota": &#123; "enabled": false, "max_size_kb": -1, "max_objects": -1 &#125;, "user_quota": &#123; "enabled": false, "max_size_kb": -1, "max_objects": -1 &#125;, "temp_url_keys": []&#125; 在这一长串的输出中，别的在现阶段不用管，记住access_key和secret_key就行。 想再查询access_key和secret_key，可以使用以下命令： 1234567891011121314151617// 查询有哪些用户[node1]$ sudo radosgw-admin user list[ "testuser"]// 查询用户信息[node1]$ sudo radosgw-admin user info --uid=testuser ... "keys": [&#123; "user": "testuser", "access_key": "I0PJDPCIYZ665MW88W9R", "secret_key": "dxaXZ8U90SXydYzyS5ivamEP20hkLSUViiaR+ZDA" &#125;], ... 使用S3 Browser（非必须）使用S3 Browser来浏览存储，以证明现在服务运行正常，连接正常。 下载S3 Browser 运行S3 Browser，它会让你输入你的S3账户： 然后将上面的信息都填上，如下： 连接成功之后，就可以看到当前的bucket情况，也可以使用它进行创建、删除、上传以及下载等： 到目前为止，算是完成了ceph集群方面的准备工作，下面就是写代码方面了。 创建工程我这里使用的是idea+java。 首先创建maven工程（处理依赖会简单很多）： （注：这里略过了idea中maven配置的相关内容。） 修改pom.xml文件，添加依赖: 1234567891011121314151617181920&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;test&lt;/groupId&gt; &lt;artifactId&gt;test&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;!-- https://mvnrepository.com/artifact/com.amazonaws/aws-java-sdk-s3 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.amazonaws&lt;/groupId&gt; &lt;artifactId&gt;aws-java-sdk-s3&lt;/artifactId&gt; &lt;version&gt;1.11.597&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; 接下来最重要的就是参考官方给的代码样例了。 最后我写的类如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131package s3;import com.amazonaws.auth.AWSStaticCredentialsProvider;import com.amazonaws.auth.BasicAWSCredentials;import com.amazonaws.client.builder.AwsClientBuilder;import com.amazonaws.services.s3.AmazonS3;import com.amazonaws.services.s3.AmazonS3ClientBuilder;import com.amazonaws.services.s3.model.*;import com.amazonaws.util.StringUtils;import java.io.*;import java.util.Iterator;import java.util.List;/** * S3操作类，完成一些基本的操作 * @author long * @version v0.1 */public class S3Template &#123; public static final String ORIGINAL_FOLDER = "video/original"; // 原始文件目录 public static final String DOWNLOAD_FOLDER = "video/download"; // 下载文件目录 public static final String TRANSCODING_FOLDER = "video/transcoding"; // 转码文件目录 private AmazonS3 s3Client = null; private MyLogger log = null; public S3Template(String endPoint, String accessKey, String secretKey) &#123; this.s3Client = AmazonS3ClientBuilder.standard() .withCredentials(new AWSStaticCredentialsProvider(new BasicAWSCredentials(accessKey, secretKey))) .withEndpointConfiguration(new AwsClientBuilder.EndpointConfiguration(endPoint, "")) .build(); log = new MyLogger(); &#125; public void listBuckets() &#123; List&lt;Bucket&gt; buckets = s3Client.listBuckets(); for (Bucket bucket : buckets) &#123; log.info(bucket.getName() + "\t" + StringUtils.fromDate(bucket.getCreationDate())); &#125; &#125; public void createBucket(String bucketName) &#123; if (!s3Client.doesBucketExistV2(bucketName)) &#123; s3Client.createBucket(new CreateBucketRequest(bucketName)); &#125; // Verify that the bucket was created by retrieving it and checking its location. String bucketLocation = s3Client.getBucketLocation(new GetBucketLocationRequest(bucketName)); log.info("Bucket location: " + bucketLocation); &#125; public void deleteBucket(String bucketName) &#123; if (!s3Client.doesBucketExistV2(bucketName)) &#123; log.info("Bucket " + bucketName + " does not exist."); return; &#125; ObjectListing objectListing = s3Client.listObjects(bucketName); while (true) &#123; Iterator&lt;S3ObjectSummary&gt; objIter = objectListing.getObjectSummaries().iterator(); while (objIter.hasNext()) &#123; s3Client.deleteObject(bucketName, objIter.next().getKey()); &#125; if (objectListing.isTruncated()) &#123; objectListing = s3Client.listNextBatchOfObjects(objectListing); &#125; else &#123; break; &#125; &#125; VersionListing versionList = s3Client.listVersions(new ListVersionsRequest().withBucketName(bucketName)); while (true) &#123; Iterator&lt;S3VersionSummary&gt; versionIter = versionList.getVersionSummaries().iterator(); while (versionIter.hasNext()) &#123; S3VersionSummary vs = versionIter.next(); s3Client.deleteVersion(bucketName, vs.getKey(), vs.getVersionId()); &#125; if (versionList.isTruncated()) &#123; versionList = s3Client.listNextBatchOfVersions(versionList); &#125; else &#123; break; &#125; &#125; s3Client.deleteBucket(bucketName); &#125; public void downFile(String bucketName, String key) throws IOException &#123; log.info("Downloading object " + key); S3Object fullObject = null; fullObject = s3Client.getObject(new GetObjectRequest(bucketName, key)); storeFile(fullObject.getObjectContent(), key); &#125; public void uploadFile(String bucketName, String fileObjKeyName, String fileName) &#123; if ( s3Client.doesObjectExist(bucketName, fileObjKeyName) ) &#123; log.info("The File " + fileName + " already exists."); return; &#125; // Upload a file as a new object with ContentType and title specified. log.info("Upload object " + fileName); PutObjectRequest request = new PutObjectRequest(bucketName, fileObjKeyName, new File(ORIGINAL_FOLDER + "/" + fileName)); ObjectMetadata metadata = new ObjectMetadata(); metadata.setContentType("plain/video"); metadata.addUserMetadata("x-amz-meta-title", "someTitle"); request.setMetadata(metadata); s3Client.putObject(request); &#125; public void storeFile(InputStream input, String fileName) throws IOException &#123; log.info("Store File " + fileName); // Read the text input stream one line at a time and display each line. BufferedInputStream reader = new BufferedInputStream(input); BufferedOutputStream writer = new BufferedOutputStream(new FileOutputStream(DOWNLOAD_FOLDER + "/" + fileName)); byte[] buff = new byte[1024]; int len = 0; while ((len = reader.read(buff)) != -1) &#123; writer.write(buff, 0, len); &#125; reader.close(); writer.close(); &#125; &#125; 成功上传文件到ceph并将其下载下来： 基本操作练习结束。 参考INSTALL CEPH OBJECT GATEWAY aws-doc-sdk-examples]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>centos</tag>
        <tag>ceph</tag>
        <tag>s3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7中lvm分区的创建与扩展]]></title>
    <url>%2F2019%2F07%2F23%2F2019-07-23-centos-lvm-create-extend%2F</url>
    <content type="text"><![CDATA[创建LVM分区首先，挂载虚拟硬盘到虚拟机上，这里挂了2个1T的硬盘。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768$ lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsda 8:0 0 100G 0 disk ├─sda1 8:1 0 1G 0 part /boot└─sda2 8:2 0 99G 0 part ├─centos-root 253:0 0 50G 0 lvm / ├─centos-swap 253:1 0 9.8G 0 lvm [SWAP] └─centos-home 253:2 0 39.2G 0 lvm /homesdb 8:16 0 1T 0 disk sdc 8:32 0 1T 0 disk sr0 11:0 1 1024M 0 rom // 创建物理卷$ sudo pvcreate /dev/sdb Physical volume "/dev/sdb" successfully created.// 创建卷组$ sudo vgcreate VG1 /dev/sdb Volume group "VG1" successfully created// 查看卷组剩多少PE$ sudo vgdisplay ... --- Volume group --- VG Name VG1 System ID Format lvm2 Metadata Areas 2 Metadata Sequence No 7 VG Access read/write VG Status resizable MAX LV 0 Cur LV 1 Open LV 0 Max PV 0 Cur PV 2 Act PV 2 VG Size &lt;2.00 TiB PE Size 4.00 MiB Total PE 524286 Alloc PE / Size 262143 / &lt;1024.00 GiB Free PE / Size 262143 / &lt;1024.00 GiB //剩余空间大小 VG UUID DYPyVN-9ssj-pLxN-bksi-9U0V-m2wQ-aQ3IPe ...// 将所有剩余空间创建为逻辑卷$ sudo lvcreate -l 262143 -n MyLvm0 VG1 Logical volume "MyLvm0" created.// 创建完成$ lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsda 8:0 0 100G 0 disk ├─sda1 8:1 0 1G 0 part /boot└─sda2 8:2 0 99G 0 part ├─centos-root 253:0 0 50G 0 lvm / ├─centos-swap 253:1 0 9.8G 0 lvm [SWAP] └─centos-home 253:2 0 39.2G 0 lvm /homesdb 8:16 0 1T 0 disk └─VG1-MyLvm0 253:3 0 1024G 0 lvm // 所创建的LVM逻辑卷sdc 8:32 0 1T 0 disk sr0 11:0 1 1024M 0 rom 扩展LVM分区上面使用了一块1T虚拟硬盘，下面将另外一块虚拟硬盘添加到刚才的逻辑卷中，实现扩展。 123456789101112131415161718192021222324252627282930313233343536373839404142434445$ lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsda 8:0 0 100G 0 disk ├─sda1 8:1 0 1G 0 part /boot└─sda2 8:2 0 99G 0 part ├─centos-root 253:0 0 50G 0 lvm / ├─centos-swap 253:1 0 9.8G 0 lvm [SWAP] └─centos-home 253:2 0 39.2G 0 lvm /homesdb 8:16 0 1T 0 disk └─VG1-MyLvm0 253:3 0 1024G 0 lvm // 需要扩展的LVM逻辑卷sdc 8:32 0 1T 0 disk // 额外的空间sr0 11:0 1 1024M 0 rom // 创建物理卷$ sudo pvcreate /dev/sdc Physical volume "/dev/sdc" successfully created.// 将它加入到卷组$ sudo vgextend VG1 /dev/sdc Volume group "VG1" successfully extended// 扩展逻辑卷MyLvm0$ sudo lvextend -l +262142 /dev/VG1/MyLvm0 Size of logical volume VG1/MyLvm0 changed from 1.00 TiB (262144 extents) to &lt;2.00 TiB (524286 extents). Logical volume VG1/MyLvm0 successfully resized.// 成功扩展$ lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsda 8:0 0 100G 0 disk ├─sda1 8:1 0 1G 0 part /boot└─sda2 8:2 0 99G 0 part ├─centos-root 253:0 0 50G 0 lvm / ├─centos-swap 253:1 0 9.8G 0 lvm [SWAP] └─centos-home 253:2 0 39.2G 0 lvm /homesdb 8:16 0 1T 0 disk └─VG1-MyLvm0 253:3 0 2T 0 lvm sdc 8:32 0 1T 0 disk └─VG1-MyLvm0 253:3 0 2T 0 lvm sr0 11:0 1 1024M 0 rom 参考Linux下添加磁盘创建lvm分区 centos7用lvm扩展xfs文件系统的根分区]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>centos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph-13.2.5添加mon，删除mon（ceph-deploy）]]></title>
    <url>%2F2019%2F07%2F22%2F2019-07-22-ceph-mon-add-destroy%2F</url>
    <content type="text"><![CDATA[前言在集群搭建完毕之后，一共有三个host，每个host都是mon节点，这时新创建一个虚拟机，想要让它加入到mon节点， 就需要学习如何添加mon节点。 添加mon节点这里添加的节点叫做node4。 由于这里使用的是ceph-deploy工具，所以首先要进行ssh免密登录的设置，然后再进行ntp设置。 当然，还需要对hosts文件进行修改。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859// 先添加集群用户[node4]$ useradd -d /home/&#123;username&#125; -m &#123;username&#125;[node4]$ echo "&#123;username&#125; ALL = (root) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/&#123;username&#125;[node4]$ sudo chmod 0440 /etc/sudoers.d/&#123;username&#125;// 修改hosts文件[deploy]$ sudo vim /etc/hosts[deploy]$ cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.31.203 node1192.168.31.204 node2192.168.31.205 node3192.168.31.207 node4// 其它所有节点拷贝hosts文件（不知道是不是必须的，反正我是做了这一步）[node1]$ sudo scp ming@192.168.31.202:/etc/hosts /etc/hosts[node2]$ sudo scp ming@192.168.31.202:/etc/hosts /etc/hosts[node3]$ sudo scp ming@192.168.31.202:/etc/hosts /etc/hosts[node4]$ sudo scp ming@192.168.31.202:/etc/hosts /etc/hosts// ssh免密登录设置[deploy]$ vim ~/.ssh/config[deploy]$ cat ~/.ssh/configHost node1 HostName 192.168.31.203 User mingHost node2 HostName 192.168.31.204 User mingHost node3 HostName 192.168.31.205 User mingHost node4 HostName 192.168.31.207 User ming[deploy]$ ssh-copy-id &#123;username&#125;@node4// ntp设置[node4]$ sudo yum install -y ntp[node4]$ sudo systemctl start ntpd[node4]$ sudo systemctl enable ntpd[node4]$ sudo vim /etc/ntp.conf[node4]$ cat /etc/ntp.conf...# Use public servers from the pool.ntp.org project.# Please consider joining the pool (http://www.pool.ntp.org/join.html).server 192.168.31.202 preferserver 0.centos.pool.ntp.org iburstserver 1.centos.pool.ntp.org iburstserver 2.centos.pool.ntp.org iburstserver 3.centos.pool.ntp.org iburst... 完成上面的准备工作之后，开始进行mon的添加， 注意：我这里使用的本地源来安装ceph，所以即使不装epel也无所谓，因为所需依赖包都在本地了， 具体可以参考之前的博客centos7上的ceph指定版本安装方法。 1234567891011121314151617181920212223242526272829303132// 安装ceph[node4]$ sudo yum install -y ceph// 修改ceph.conf配置。// 这里在mon_initial_members、mon_host上的修改可能并不是必须的（经本人测试），但感觉还是加上好一点。// 一定要增加public_network字段的配置，否则大概率会出现"admin_socket: exception getting command descriptions file ..."的错误。[deploy]$ vim ceph.conf[deploy]$ cat ceph.conf[global]fsid = a20b153c-c907-41bb-a5b2-753a40e2085cmon_initial_members = node1, node2, node3, node4mon_host = 192.168.31.203,192.168.31.204,192.168.31.205,192.168.31.207auth_cluster_required = cephxauth_service_required = cephxauth_client_required = cephxpublic_network = 192.168.31.0/24// 将这个配置发送到其它节点[deploy]$ ceph-deploy --overwrite-conf config push node1 node2 node3 node4// 添加mon节点[deploy]$ ceph-deploy mon add node4[deploy]$ ssh node1 sudo ceph -s ... services: mon: 4 daemons, quorum node1,node2,node3,node4 mgr: node1(active), standbys: node2, node3 osd: 0 osds: 0 up, 0 in ... 这样就完成了mon节点的添加，注意到其中的关键在于public_network字段的添加。 删除mon节点直接destroy就可以了： 1[deploy]$ ceph-deploy mon destroy node4 参考STORAGE CLUSTER QUICK START ceph在扩展mon节点时，要注意的问题]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>centos</tag>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph-13.2.5删除osd，新建osd]]></title>
    <url>%2F2019%2F07%2F19%2F2019-07-19-ceph-osd-create-remove%2F</url>
    <content type="text"><![CDATA[前言在搭建集群的时候，在node2上挂了一个lvm格式的osd（VG1/MyLvm），10G大小，想着既然是lvm，那就可以直接扩容， 所以又加了一个10G的虚拟硬盘，并扩容进了VG1/MyLvm，结果就是： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859$ lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsda 8:0 0 20G 0 disk ├─sda1 8:1 0 1G 0 part /boot└─sda2 8:2 0 19G 0 part ├─centos-root 253:0 0 17G 0 lvm / └─centos-swap 253:1 0 2G 0 lvm [SWAP]sdb 8:16 0 10G 0 disk └─sdb1 8:17 0 10G 0 part └─VG1-MyLvm 253:2 0 20G 0 lvm sdc 8:32 0 10G 0 disk └─sdc1 8:33 0 10G 0 part └─VG1-MyLvm 253:2 0 20G 0 lvm sr0 11:0 1 1024M 0 rom $ sudo vgdisplay --- Volume group --- VG Name centos System ID Format lvm2 Metadata Areas 1 Metadata Sequence No 3 VG Access read/write VG Status resizable MAX LV 0 Cur LV 2 Open LV 2 Max PV 0 Cur PV 1 Act PV 1 VG Size &lt;19.00 GiB PE Size 4.00 MiB Total PE 4863 Alloc PE / Size 4863 / &lt;19.00 GiB Free PE / Size 0 / 0 VG UUID fEJqFf-qaqN-ZeZe-3FG1-Jeya-SyAi-WCtB0u --- Volume group --- VG Name VG1 System ID Format lvm2 Metadata Areas 2 Metadata Sequence No 39 VG Access read/write VG Status resizable MAX LV 0 Cur LV 1 Open LV 1 Max PV 0 Cur PV 2 Act PV 2 VG Size 19.99 GiB PE Size 4.00 MiB Total PE 5118 Alloc PE / Size 5118 / 19.99 GiB Free PE / Size 0 / 0 VG UUID OMx43d-hsd1-80vh-DCvO-870t-zPSn-123yoU 首先可以看到逻辑卷MyLvm的确变成了20G大小。 1234567891011$ sudo ceph osd df treeID CLASS WEIGHT REWEIGHT SIZE USE AVAIL %USE VAR PGS TYPE NAME -1 0.02939 - 40 GiB 13 GiB 27 GiB 33.56 1.00 - root default -3 0.00980 - 10 GiB 1.1 GiB 8.9 GiB 11.42 0.34 - host node1 0 hdd 0.00980 1.00000 10 GiB 1.1 GiB 8.9 GiB 11.42 0.34 68 osd.0 -5 0.00980 - 20 GiB 11 GiB 8.9 GiB 55.71 1.66 - host node2 1 hdd 0.00980 1.00000 20 GiB 11 GiB 8.9 GiB 55.71 1.66 0 osd.1 -7 0.00980 - 10 GiB 1.1 GiB 8.9 GiB 11.42 0.34 - host node3 2 hdd 0.00980 1.00000 10 GiB 1.1 GiB 8.9 GiB 11.42 0.34 68 osd.2 TOTAL 40 GiB 13 GiB 27 GiB 33.56 可以看到osd.1的容量的确变成了20G，但是已经用了11G？？？ 刚挂上去的10G空间全部被标记为使用？？？ 不明白这是什么原理，可能这样去扩容是不对的，为了把这10G空间夺取回来， 决定先删除osd.1，清空lvm卷，再把它加回来。 删除osd这里直接参考官方教程ADDING/REMOVING OSDS 步骤即： 将osd设置为out。 关闭正在运行的osd进程。 从crush map中删除它。 删除OSD authentication key。 删除osd。 命令如下： 1234567891011121314151617$ sudo ceph osd out osd.1marked out osd.1.$ sudo systemctl stop ceph-osd@1$ sudo ceph osd crush remove osd.1removed item id 1 name 'osd.1' from crush map$ sudo ceph auth del osd.1updated$ sudo ceph osd rm 1removed osd.1 查看集群状态： 1234567891011121314151617181920212223242526272829303132$ sudo ceph -s cluster: id: 082d1625-1d68-4261-82c8-3fe9fe3ef489 health: HEALTH_WARN Degraded data redundancy: 256/768 objects degraded (33.333%), 32 pgs degraded, 68 pgs undersized services: mon: 3 daemons, quorum node1,node2,node3 mgr: node1(active), standbys: node3, node2 mds: fs_test-1/1/1 up &#123;0=node1=up:active&#125; osd: 2 osds: 2 up, 2 in rgw: 1 daemon active data: pools: 8 pools, 68 pgs objects: 256 objects, 136 MiB usage: 2.3 GiB used, 18 GiB / 20 GiB avail pgs: 256/768 objects degraded (33.333%) 36 active+undersized 32 active+undersized+degraded$ sudo ceph osd df treeID CLASS WEIGHT REWEIGHT SIZE USE AVAIL %USE VAR PGS TYPE NAME -1 0.01959 - 10 GiB 1.1 GiB 8.8 GiB 0 0 - root default -3 0.00980 - 10 GiB 1.1 GiB 8.8 GiB 11.48 1.00 - host node1 0 hdd 0.00980 1.00000 10 GiB 1.1 GiB 8.8 GiB 11.48 1.00 68 osd.0 -5 0 - 0 B 0 B 0 B 0 0 - host node2 -7 0.00980 - 10 GiB 1.1 GiB 8.8 GiB 11.48 1.00 - host node3 2 hdd 0.00980 1.00000 10 GiB 1.1 GiB 8.8 GiB 11.48 1.00 68 osd.2 TOTAL 20 GiB 2.3 GiB 18 GiB 11.48 可以看到osd.1已经删除，这里的WARN是因为只有两个osd节点造成的。 创建osd 方法一：使用ceph-deploy。 因为这里相当于是撤下来的盘，所以先将它里面的内容擦除，借助ceph-volume来进行： 12345678910111213141516171819202122232425262728293031$ sudo ceph-volume lvm zap VG1/MyLvm--&gt; Zapping: /dev/VG1/MyLvm--&gt; Unmounting /var/lib/ceph/osd/ceph-1Running command: /bin/umount -v /var/lib/ceph/osd/ceph-1 stderr: umount: /var/lib/ceph/osd/ceph-1 (tmpfs) unmountedRunning command: /usr/sbin/wipefs --all /dev/VG1/MyLvmRunning command: /bin/dd if=/dev/zero of=/dev/VG1/MyLvm bs=1M count=10Running command: /usr/sbin/lvchange --deltag ceph.type=block /dev/VG1/MyLvm stdout: Logical volume VG1/MyLvm changed.Running command: /usr/sbin/lvchange --deltag ceph.osd_id=1 /dev/VG1/MyLvm stdout: Logical volume VG1/MyLvm changed.Running command: /usr/sbin/lvchange --deltag ceph.cluster_fsid=082d1625-1d68-4261-82c8-3fe9fe3ef489 /dev/VG1/MyLvm stdout: Logical volume VG1/MyLvm changed.Running command: /usr/sbin/lvchange --deltag ceph.cluster_name=ceph /dev/VG1/MyLvm stdout: Logical volume VG1/MyLvm changed.Running command: /usr/sbin/lvchange --deltag ceph.osd_fsid=a59044b3-ff4c-4a99-978f-1336ff4504a0 /dev/VG1/MyLvm stdout: Logical volume VG1/MyLvm changed.Running command: /usr/sbin/lvchange --deltag ceph.encrypted=0 /dev/VG1/MyLvm stdout: Logical volume VG1/MyLvm changed.Running command: /usr/sbin/lvchange --deltag ceph.cephx_lockbox_secret= /dev/VG1/MyLvm stdout: Logical volume VG1/MyLvm changed.Running command: /usr/sbin/lvchange --deltag ceph.block_uuid=B553Ss-LYdv-3FEW-q5u9-XKph-qn2v-y3ecu0 /dev/VG1/MyLvm stdout: Logical volume VG1/MyLvm changed.Running command: /usr/sbin/lvchange --deltag ceph.block_device=/dev/VG1/MyLvm /dev/VG1/MyLvm stdout: Logical volume VG1/MyLvm changed.Running command: /usr/sbin/lvchange --deltag ceph.vdo=0 /dev/VG1/MyLvm stdout: Logical volume VG1/MyLvm changed.Running command: /usr/sbin/lvchange --deltag ceph.crush_device_class=None /dev/VG1/MyLvm stdout: Logical volume VG1/MyLvm changed.--&gt; Zapping successful for: &lt;LV: /dev/VG1/MyLvm&gt; 使用ceph-delpoy创建osd： 12345678910$ ceph-deploy osd create --data VG1/MyLvm node2............[node2][DEBUG ] --&gt; ceph-volume lvm activate successful for osd ID: 1[node2][DEBUG ] --&gt; ceph-volume lvm create successful for: VG1/MyLvm[node2][INFO ] checking OSD status...[node2][DEBUG ] find the location of an executable[node2][INFO ] Running command: sudo /bin/ceph --cluster=ceph osd stat --format=json[ceph_deploy.osd][DEBUG ] Host node2 is now ready for osd use. 查询集群状态： 1234567891011121314151617181920212223242526272829$ sudo ceph -s cluster: id: 082d1625-1d68-4261-82c8-3fe9fe3ef489 health: HEALTH_OK services: mon: 3 daemons, quorum node1,node2,node3 mgr: node1(active), standbys: node3, node2 mds: fs_test-1/1/1 up &#123;0=node1=up:active&#125; osd: 3 osds: 3 up, 3 in rgw: 1 daemon active data: pools: 8 pools, 68 pgs objects: 256 objects, 136 MiB usage: 3.5 GiB used, 37 GiB / 40 GiB avail pgs: 68 active+clean $ sudo ceph osd df treeID CLASS WEIGHT REWEIGHT SIZE USE AVAIL %USE VAR PGS TYPE NAME -1 0.03908 - 40 GiB 3.3 GiB 37 GiB 8.34 1.00 - root default -3 0.00980 - 10 GiB 1.1 GiB 8.8 GiB 11.49 1.38 - host node1 0 hdd 0.00980 1.00000 10 GiB 1.1 GiB 8.8 GiB 11.49 1.38 68 osd.0 -5 0.01949 - 20 GiB 1.0 GiB 19 GiB 5.20 0.62 - host node2 1 hdd 0.01949 1.00000 20 GiB 1.0 GiB 19 GiB 5.20 0.62 55 osd.1 -7 0.00980 - 10 GiB 1.1 GiB 8.8 GiB 11.49 1.38 - host node3 2 hdd 0.00980 1.00000 10 GiB 1.1 GiB 8.8 GiB 11.49 1.38 68 osd.2 TOTAL 40 GiB 3.3 GiB 37 GiB 8.34 MIN/MAX VAR: 0.62/1.38 STDDEV: 3.14 可以看到集群现在就健康了，并且osd.1就有19G可用了。 方法二：手动创建。 尝试官方教程失败，如下： 1234567891011121314151617$ sudo mount -o user_xattr /dev/VG1/MyLvm /var/lib/ceph/osd/ceph-1mount: wrong fs type, bad option, bad superblock on /dev/mapper/VG1-MyLvm, missing codepage or helper program, or other error In some cases useful info is found in syslog - try dmesg | tail or so.$ sudo mount /dev/VG1/MyLvm /var/lib/ceph/osd/ceph-1$ sudo ceph-osd -i 1 --mkfs --mkkey2019-07-19 16:54:52.691 7efc821c7d80 -1 auth: unable to find a keyring on /var/lib/ceph/osd/ceph-1/keyring: (2) No such file or directory2019-07-19 16:54:52.691 7efc821c7d80 -1 monclient: ERROR: missing keyring, cannot use cephx for authenticationfailed to fetch mon config (--no-mon-config to skip) 暂时放弃手动。 参考ADDING/REMOVING OSDS]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>centos</tag>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7上的ceph指定版本安装方法]]></title>
    <url>%2F2019%2F07%2F18%2F2019-07-18-ceph-install%2F</url>
    <content type="text"><![CDATA[目标在centos7.5中，安装ceph-13.2.5版本，能使用ceph -s命令查询到HEALTH_OK为止。 通过查询ceph官网以及各种博客可以知道，大概有以下几种ceph安装方法： 按照ceph官网教程，使用ceph-deploy工具进行安装。 按照ceph官网教程，下载源码并编译安装。 使用yum直接进行安装。 上面的三个方法我都尝试了，只有第三种方法是最靠谱的，也只有这种方法完成了集群搭建。 在大量的尝试安装之后，总结流程如下： 首先，按照官网教程完成前期准备工作。 其次，在用于搭建集群的每一台虚拟机上安装ceph-13.2.5。 最后，使用第一步搭好的ceph-deploy主机完成后续的集群搭建。 也就是99%参考官网教程，但是取代其中的ceph-deploy install XXX XXX XXX这一步。 下面依次叙述各个步骤。 前期准备难度：简单。 在ceph官网可以看到如何进行前期的准备（蓝色框部分），还是很详细的，INSTALLATION (CEPH-DEPLOY)。 这里同样参考官网教程，起了4台虚拟机，一台作为ceph-deploy，另外三台用于搭建cluster。 注意事项： 最好将每一台虚拟机上的/etc/hosts文件都进行修改，例如这里添加了下面内容： 123192.168.31.203 node1192.168.31.204 node2192.168.31.205 node3 这里的node1、node2、node3就是用来搭集群的三台虚拟机，不包含ceph-deploy虚拟机。 做这件事主要是因为后面在使用ceph-deploy mon create-initial这个命令时，它使用到这些修改后的hostname去创建文件， 在没有添加以上内容的机器上，就会使用localhost来代替本机，可能造成名称不同步而出现找不到文件的ERROR。 在用于搭建集群的每一台虚拟机上安装ceph-13.2.5难度：困难。 实际上在第一次成功装上之后，你就会发现这其实是一件十分简单的事情， 但是要到达第一次成功安装，有一个十分艰难的过程。 方法一：使用ceph-deploy工具进行安装。 在官网的教程中，就是使用ceph-deploy来进行ceph的安装的，但是在使用过程中会发现， 使用它安装只能指定大版本号（例如：mimic），而mimic现在的最新版本是13.2.6， 这里所需要的是13.2.5，使用它进行安装，就只能安装13.2.6。 当然在通过ceph-deploy install -h命令可以看到它可以指定镜像源--local-mirror，如果你有一个完整的本地ceph13.2.5的源， 也许用它来安装13.2.5也是可以的。 另外的问题是，就算你要装的就是13.2.6版本，在国内进行ceph-deploy install XXX时还是很容易出现error，因为一些依赖包可能会下载失败。 结论：方法一不合适。 方法二：下载源码并编译安装。 在使用ceph-deploy工具受挫之后，这里也尝试了使用源码进行编译安装。 如上图中的蓝色框部分，就是官网教你如何源码编译安装的过程。 当然，这一步我还是失败了，因为在执行make时，速度非常慢，而且它还会去下载各种包， 当它编译了4个小时之后，卡在了一个依赖包，又等了1个小时，它一点没动，只能ctrl+c， 再进行make就直接error了。 所有这里如果你想进行make，那先shutdown虚拟机，并多分几个cpu，在make时，使用make -j4开启多线程， 会快很多，当然，过程中还是有可能失败，但至少不用等这么久！ 结论：方法二不合适。 方法三：直接使用yum指定版本号安装。 难度：简单。 在前辈的指点之下，发现原来yum直接可以指定版本号进行安装？？？ 这里所需的命令就是: 1$ sudo yum install -y ceph-13.2.5 ceph-radosgw-13.2.5 所以其实最简单的方法就是： 配置ceph源（建议使用国内源，我这里使用了网易源）。 确定安装了epel-release。 sudo yum install -y ceph-13.2.5 ceph-radosgw-13.2.5一定要两个一起安装，并都带上版本号。 结论：方法三合适。 真正使用的方法：在方法三的基础上，建立局域网源。 使用局域网源的好处就是快，而且不会在安装时给你报个错，所以可以在方法三的基础上制作一个ceph局域网源。 关键命令就是： 1$ sudo yum install --downloadonly --downloaddir=/home/long/yum/ceph-13.2.5 ceph-13.2.5 ceph-radosgw-13.2.5 使用了--downloadonly指令之后，yum就不会真正去安装ceph，而是将ceph以及它所有的相关依赖（本机上没有的）都下载下来。 为了获得完成的依赖，可以安装一个minimum版本（不是minimum安装，而是iso镜像就是minimum）的centos7.5， 然后在它上面不装东西，而只用来获取所有的依赖。 所以这里的操作步骤就是： 安装一个minimum版本的centos7.5（非必须，但这样保证没有问题）。 sudo yum install --downloadonly --downloaddir=/home/long/yum/ceph-13.2.5 ceph-13.2.5 ceph-radosgw-13.2.5获取所有依赖。 参考之前的方法建立局域网源linux基础练习-2 在需要安装ceph的虚拟机上修改repo为局域网repo，然后进行ceph安装。 使用第一步搭好的ceph-deploy主机完成后续的集群搭建这里同样的，直接参考官方教程即可： 其它的步骤都一样，只是ceph-deploy install xxx xxx xxx这一步我们已经在上一步完成了。 可以看到我这里终于完成了集群的搭建： 12345678910111213141516$ ssh node1 sudo ceph -s cluster: id: 082d1625-1d68-4261-82c8-3fe9fe3ef489 health: HEALTH_OK services: mon: 3 daemons, quorum node1,node2,node3 mgr: node1(active), standbys: node2, node3 osd: 3 osds: 3 up, 3 in data: pools: 0 pools, 0 pgs objects: 0 objects, 0 B usage: 3.0 GiB used, 27 GiB / 30 GiB avail pgs: 这里每一个osd上挂载了一个10G的虚拟硬盘，第一个硬盘什么都没做直接挂在了osd上， 另外两个先创建成lvm分区（比较推荐）再进行的挂载。 参考INSTALLATION (CEPH-DEPLOY) centos7部署ceph]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>centos</tag>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux基础练习-2]]></title>
    <url>%2F2019%2F07%2F15%2F2019-07-15-linux-base-practice_2%2F</url>
    <content type="text"><![CDATA[练习目标使用iso镜像搭建一个本地源，安装httpd服务并局域网yum install成功 所需准备 两台centos虚拟机（一般是mini安装），一台作为yum源，一台测试yum源。 一个centos的iso镜像，这里选择的是CentOS-7-x86_64-Everything-1804.iso。 配置好网络，起码两台虚拟机能相互ping通。 配置本地yum源本地yum源，意思就是只有本机能够使用的yum源，这里需要先配置本地yum源， 然后再将其公布到局域网。 使用iso镜像搭建本地yum源，这里首先将iso镜像进行挂载： 完成后，iso镜像就已经挂到了/dev/cdrom目录上，新建一个目录将它挂载： 1234567891011121314151617181920$ mkdir /mnt/iso$ mount /dev/cdrom /mnt/isomount: /dev/sr0 is write-protected, mounting read-only$ ll /mnt/isototal 1640-rw-rw-r-- 1 root root 14 May 2 2018 CentOS_BuildTagdrwxr-xr-x 3 root root 2048 May 4 2018 EFI-rw-rw-r-- 1 root root 227 Aug 30 2017 EULA-rw-rw-r-- 1 root root 18009 Dec 10 2015 GPLdrwxr-xr-x 3 root root 2048 May 4 2018 imagesdrwxr-xr-x 2 root root 2048 May 4 2018 isolinuxdrwxr-xr-x 2 root root 2048 May 4 2018 LiveOSdrwxrwxr-x 2 root root 1640448 May 3 2018 Packagesdrwxrwxr-x 2 root root 4096 May 7 2018 repodata-rw-rw-r-- 1 root root 1690 Dec 10 2015 RPM-GPG-KEY-CentOS-7-rw-rw-r-- 1 root root 1690 Dec 10 2015 RPM-GPG-KEY-CentOS-Testing-7-r--r--r-- 1 root root 2883 May 7 2018 TRANS.TBL 然后将这些文件都复制出来（貌似也可以不复制出来，直接使用/mnt/iso目录作为本地源目录）： 12$ mkdir /home/long/yum$ cp -r /mnt/iso/* /home/long/yum 将现有的.repo文件备份，随便新建或者复制一个.repo文件，修改其中的内容： 123456789101112$ cd /etc/yum.repos.d/$ mkdir bak$ mv *.repo bak$ cp bak/CentOS-Base.repo .$ vim CentOS-Base.repo[base] # yun源的名字，唯一即可name=CentOS-$releasever - Base # 注释信息，随便baseurl=file:///home/long/yum # yum源路径，支持三种协议：http、ftp、fileenabled=1 # 1表示启用，0表示禁用gpgcheck=0 # 指纹校验，为0表示不校验gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7 #校验参考的文件 清除默认源，重新缓存本地源： 1234567891011121314151617181920$ yum clean allLoaded plugins: fastestmirrorCleaning repos: baseCleaning up everythingMaybe you want: rm -rf /var/cache/yum, to also free up space taken by orphaned data from disabled or removed reposCleaning up list of fastest mirrors$ yum makecacheLoaded plugins: fastestmirrorDetermining fastest mirrorsbase | 3.6 kB 00:00:00 (1/4): base/group_gz | 166 kB 00:00:00 (2/4): base/primary_db | 5.9 MB 00:00:00 (3/4): base/filelists_db | 6.9 MB 00:00:00 (4/4): base/other_db | 2.5 MB 00:00:00 Metadata Cache Created$ yum list 测试本地yum源是否配置成功： 12$ yum install httpd$ yum install wget 到这里本地yum源就搭建成功了 配置局域网yum源上面已经安装了httpd服务（yum install httpd），它默认的目录是/var/www/html，这里需要对它进行修改： 12$ rm -rf /var/www/html$ ln -s /home/long/yum /var/www/html 下面启动httpd服务，随便把它设置为开机自启： 12$ systemctl enable httpd$ systemctl start httpd 验证是否启动成功，在另一台虚拟机上进行测试： 1234567891011$ wget http://192.168.31.201/RPM-GPG-KEY-CentOS-7--2019-07-15 17:55:08-- http://192.168.31.201/RPM-GPG-KEY-CentOS-7Connecting to 192.168.31.201:80... connected.HTTP request sent, awaiting response... 200 OKLength: 1690 (1.7K)Saving to: ‘RPM-GPG-KEY-CentOS-7’100%[=================================================================================================================================&gt;] 1,690 --.-K/s in 0s 2019-07-15 17:55:08 (3.62 MB/s) - ‘RPM-GPG-KEY-CentOS-7’ saved [1690/1690] 这里就完成了局域网yum源的配置。 使用局域网yum源方法和使用本地yum源一样，唯一不一样的地方在于将file修改为http: 12345678$ vim CentOS-Base.repo[base]name=CentOS-$releasever - Basebaseurl=http://192.168.31.201:80/enabled=1gpgcheck=0gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7 尝试使用： 123456$ yum install vimLoaded plugins: fastestmirrorLoading mirror speeds from cached hostfilePackage matching 2:vim-enhanced-7.4.160-4.el7.x86_64 already installed. Checking for update.Nothing to do ok 过程中的问题403 Forbidden: 在执行wget http://192.168.31.201/RPM-GPG-KEY-CentOS-7命令时，出现403错误。 这里可能是由于一下的问题造成的： 防火墙问题。解决办法：关闭防火墙。 123$ systemctl status firewalld$ systemctl disable firewalld$ systemctl stop firewalld selinux问题。解决办法：关闭selinux。 12345678910111213$ getenforceEnforcing$ vim /etc/sysconfig/selinuxSELINUX=disabled$ reboot$ getenforceDisabled 权限问题。解决办法：修改yum源目录的权限。 在centos上，httpd的log文件记录在/var/log/httpd/目录下，有access_log和error_log， 在access_log中可以看到接入信息，error_log中可以看到错误信息，这些信息十分有用。 查看error_log: 12345$ sudo tail /var/log/httpd/error_log...[Mon Jul 15 09:50:49.577353 2019] [core:error] [pid 1303] [client 192.168.31.202:37660] AH00037: Symbolic link not allowed or link target not accessible: /var/www/html... 可以看到问题在于权限不够，这里需要将/home/long/yum从上到下这几个目录都添加执行权限： 1$ chmod -R +x /home 当然这样将所有的文件和文件夹都添加了执行权限，实际上应该只需要将文件夹添加可执行权限就好。 2019-07-18补充在上面的操作过程中，使用的是iso镜像，镜像是已经建立成repo仓库的，所以可以直接使用。 但是，如果现在有一堆rpm文件在一个文件夹下，直接像上面那样是不行的，因为它还不是一个repo仓库， 简单来说就是文件中不包含repodata文件夹。 所以这里引入createrepo命令来将一个文件夹做成一个repo仓库。 可以直接使用yum安装它： 1$ sudo yum install createrepo 比如将文件夹/home/tmp创建为repo仓库： 1$ createrepo -p -d -o /home/tmp /home/tmp 如果在文件夹中又加入了新的rpm文件，则需要更新仓库： 1$ createrepo --update /home/tmp 参考CentOS7 配置局域网内软件源 http模式 临时和永久关闭Selinux 解决Symbolic link not allowed or link target not accessible问题一例]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>centos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux基础练习-1]]></title>
    <url>%2F2019%2F07%2F11%2F2019-07-12-linux-base-practice_1%2F</url>
    <content type="text"><![CDATA[创建文件并挂载到swap通过free命令查看当前内存使用情况： 12345$ free -m total used free shared buff/cache availableMem: 3790 122 3467 8 200 3435Swap: 3071 0 3071 首先，需要使用dd命令创建一个固定大小的空文件: 12345$ dd if=/dev/zero of=test.file bs=1M count=10241024+0 records in1024+0 records out1073741824 bytes (1.1 GB) copied, 0.696093 s, 1.5 GB/s 以上命令就创建了一个大小为1G的文件test.file。 注意到其中if=/dev/zero就是指定输入文件为/dev/zero，类似的还有/dev/null、/dev/random、/dev/urandom。 /dev/null，“空”设备，也有人称它为黑洞。任何输入到这个“设备”的数据都将被直接丢弃。最常用的用法是把不需要的输出重定向到这个文件。 /dev/zero，“零”设备，可以无限的提供空字符（0x00，ASCII代码NUL）。常用来生成一个特定大小的文件。 /dev/random，随机数设备，提供不间断的随机字节流。产生随机数据依赖系统中断，当系统中断不足时，/dev/random设备会“挂起”，因而产生数据速度较慢，但随机性好。 /dev/urandom，随机数设备，提供不间断的随机字节流。不依赖系统中断，数据产生速度快，但随机性较低。 然后，将这个文件转换为swap格式，并挂载： 1234$ mkswap test.fileSetting up swapspace version 1, size = 1048572 KiBno label, UUID=ddb65a66-7f6b-4394-8014-8ad22a10a953 1234$ swapon test.fileswapon: /home/long/test.file: insecure permissions 0664, 0600 suggested.swapon: /home/long/test.file: insecure file owner 1000, 0 (root) suggested. 再次通过free命令查看当前内存使用情况： 12345$ free -m total used free shared buff/cache availableMem: 3790 122 2414 8 1252 3408Swap: 4095 0 4095 可以看到swap空间增加了1G。 也许你想要将这个swap文件开机自动挂起： 1234$ vim /etc/fstab在文件中添加一行：/home/long/test.file swap swap defaults 0 0 重启之后可以看到： 12345$ free -m total used free shared buff/cache availableMem: 3790 123 3462 8 205 3433Swap: 4095 0 4095 可以看到完成了开机自动挂载。 最后，可以相应的通过使用swapoff命令来进行卸载： 12345678$ swapoff test.file$ free -m total used free shared buff/cache availableMem: 3790 117 3486 8 186 3448Swap: 3071 0 3071 当然还要删除/etc/fstab中的开机自动挂载的命令。 配置静态ip，重启后也能生效，系统可以访问公网首先使用ip addr查询当前接口： 1234567891011121314$ ip addr1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: enp0s3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UNKNOWN group default qlen 1000 link/ether 08:00:27:26:e4:08 brd ff:ff:ff:ff:ff:ff inet 192.168.31.202/24 brd 192.168.31.255 scope global noprefixroute enp0s3 valid_lft forever preferred_lft forever inet6 fe80::ac:cc04:a87b:74ae/64 scope link noprefixroute valid_lft forever preferred_lft forever 可以看到网卡名为enp0s3，网卡的配置文件在/etc/sysconfig/network-script/目录下， 在其中可以看到当前的网卡配置： 12345678910111213141516171819$ cat /etc/sysconfig/network-scripts/ifcfg-enp0s3TYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=dhcpDEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=stable-privacyNAME=enp0s3UUID=0ee98256-fc86-4fbf-aba5-8c46e914fa89DEVICE=enp0s3ONBOOT=yesDNS1=8.8.8.8DNS2=9.9.9.9 将其中的BOOTPROTO修改为static，再加入静态ip的相关配置，如下： 1234BOOTPROTO=staticIPADDR=192.168.31.202NETMASK=255.255.255.0GATEWAY=192.168.31.254 重启网络服务： 12345678910111213141516$ systemctl restart network$ ip addr1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: enp0s3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UNKNOWN group default qlen 1000 link/ether 08:00:27:26:e4:08 brd ff:ff:ff:ff:ff:ff inet 192.168.31.202/24 brd 192.168.31.255 scope global noprefixroute enp0s3 valid_lft forever preferred_lft forever inet6 fe80::ac:cc04:a87b:74ae/64 scope link noprefixroute valid_lft forever preferred_lft forever 可以看到已经完成了静态ip的设置，测试连通性： 123456$ ping www.baidu.comPING www.a.shifen.com (14.215.177.38) 56(84) bytes of data.64 bytes from 14.215.177.38 (14.215.177.38): icmp_seq=1 ttl=53 time=7.23 ms64 bytes from 14.215.177.38 (14.215.177.38): icmp_seq=2 ttl=53 time=6.77 ms64 bytes from 14.215.177.38 (14.215.177.38): icmp_seq=3 ttl=53 time=7.00 ms 可以看到网络连接正常，也就完成了静态ip的设置。 问题： 在测试连通性的过程中，会发现主机（win 10）可以ping通虚拟机（centos7.5），但是虚拟机却无法ping通主机， 通过查询博客，发现问题主要是由于win10自带的防火墙造成的，可以通过以下方法来解决这个问题： 控制面板-系统和安全-Windows Defender防火墙-高级设置-入站规则-文件和打印机共享(回显请求 - ICMPv4-In) 将两条上面名字的入站规则启用，虚拟机就可以向外ping通主机了。 手动释放掉系统的全部buffer/cache，使得内存的free增加这里需要手动释放内存，主要涉及到的文件就是/proc/sys/vm/drop_caches，通过查询可以了解到， “/proc/sys是一个虚拟文件系统，可以通过对它的读写操作做为与kernel实体间进行通信的一种手段”， 可以对它进行不同的修改来让内核释放内存： 1234567891011121314151617Writing to this file causes the kernel to drop clean caches,dentries and inodes from memory, causing that memory to become free.To free pagecache, use echo 1 &gt; /proc/sys/vm/drop_caches;to free dentries and inodes, use echo 2 &gt; /proc/sys/vm/drop_caches;to free pagecache, dentries and inodes, use echo 3 &gt;/proc/sys/vm/drop_caches.Because this is a non-destructive operation and dirty objects are not freeable, the user should run sync first. 可以看到，这里建议在清除内存之间执行sync命令，它将”将有关文件系统的存储器常驻信息送入物理介质内”。 12345678910111213$ free -h total used free shared buff/cache availableMem: 3.7G 115M 3.4G 8.5M 161M 3.4GSwap: 3.0G 0B 3.0G$ sync$ echo 1 &gt; /proc/sys/vm/drop_caches$ free -h total used free shared buff/cache availableMem: 3.7G 116M 3.5G 8.5M 89M 3.4GSwap: 3.0G 0B 3.0G 可以看到buff/cache从161M变化到了89M。 使用systemd的管理方式定制一个开机启动服务原理： systemctl的每一个服务对应一个服务配置文件，以.service结尾，所有有效的服务配置文件默认存放在两个地方， /usr/lib/systemd/system/这个目录存放的是服务的真实配置文件，/etc/systemd/system/这个目录存放的是 开机自启动的服务，多数为软连接。 启动顺序： 当运行systemctl命令时，systemctl先去上面/etc/systemd/system目录中找目标service文件，找不到则 去/usr/lib/systemd/system/中找，还找不到就失败。 找到service后，会读取service文件并将@后面的参数传给服务。 开始监控这个服务，接管它的基本操作。 目标： 建立一个开机启动服务，每次开机时打印当前时间到/tmp/hello。 首先，写好shell脚本： 12345$ cat my_hello.sh#!/bin/bashdate +"%Y-%m-%d %H:%M:%S --- hello systemd" &gt;&gt; /tmp/hello 然后，编写服务脚本： 12345678910111213$ cat my_hello.service[Unit]Description=systemd test[Service]Type=simpleExecStart=/home/long/my_hello.sh[Install]WantedBy=multi-user.target 可以看到这里将Type指定为simple，将my_hello.sh作为启动脚本。 更多的Type可以参考CentOS 7之Systemd详解之服务单元设置system.service。 将服务脚本复制到/usr/lib/systemd/system/目录下： 1$ cp my_hello.service /usr/lib/systemd/system/ 启动服务看是否能够成功： 12345$ systemctl start my_hello$ cat /tmp/hello2019-07-12 15:23:33 --- hello systemd 可以看到服务启动成功，下面将它设置为开机自启： 123$ systemctl enable my_helloCreated symlink from /etc/systemd/system/multi-user.target.wants/my_hello.serviceto /usr/lib/systemd/system/my_hello.service. 可以看到它为服务在/etc/systemd/system目录下创建了软连接。 123456$ reboot$ cat /tmp/hello2019-07-12 15:23:33 --- hello systemd2019-07-12 15:35:44 --- hello systemd 可以看到服务自启动成功。 参考Linux中的虚拟设备/dev/null、/dev/zero、/dev/random和/dev/urandom VirtualBox虚拟机ping不通宿主机解决方案 echo N&gt;/proc/sys/vm/drop_caches清理缓存 Linux sync命令的作用 Linux sync命令的作用分析 CentOS 7之Systemd详解之服务单元设置system.service]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>centos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java数组拷贝Benchmark]]></title>
    <url>%2F2018%2F07%2F19%2F2018-07-19-java-array-copy-bench%2F</url>
    <content type="text"><![CDATA[在《疯狂Java》的7.3，关于Object类的内容中，作者表示使用Object的默认clone方法比使用静态copy方法快两倍。 这一点十分的有趣，毕竟平常在写算法题的时候，经常会需要对数组进行拷贝，而我最常用的还是System.arraycopy()方法，那么难道Object.clone()真的要更快吗？ 实践是检验真理的唯一标准，这里就对各种数组拷贝的方法进行比较，直接上代码比时间， 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import java.util.Arrays;public class ArrayCopyTest &#123; public static void main(String[] args) &#123; final int N = 10000; final int M = 5000000; long start, end; int[] a = new int[N]; for (int i = 0; i &lt; N; i++ ) &#123; a[i] = i; &#125; start = System.currentTimeMillis(); for ( int i = 0; i &lt; M; i++ ) &#123; int[] b = a.clone(); &#125; end = System.currentTimeMillis(); System.out.println("Object.clone()用时：" + (end - start) + "ms"); start = System.currentTimeMillis(); for ( int i = 0; i &lt; M; i++ ) &#123; int[] c = new int[N]; System.arraycopy(a, 0, c, 0, N); &#125; end = System.currentTimeMillis(); System.out.println("System.arraycopy()用时：" + (end - start) + "ms"); start = System.currentTimeMillis(); for ( int i = 0; i &lt; M; i++ ) &#123; int[] c = new int[N]; for (int j = 0; j &lt; N; j++ ) &#123; c[j] = a[j]; &#125; &#125; end = System.currentTimeMillis(); System.out.println("循环拷贝用时：" + (end - start) + "ms"); start = System.currentTimeMillis(); for ( int i = 0; i &lt; M; i++ ) &#123; int[] c = Arrays.copyOf(a, N); &#125; end = System.currentTimeMillis(); System.out.println("Arrays.copyOf()用时：" + (end - start) + "ms"); &#125;&#125; 这里一共使用了四种数组拷贝的方法，Object.clone()、System.arraycopy()、for循环、Arrays.copyOf()。 测试结果1： 123456$ java ArrayCopyTestObject.clone()用时：14491msSystem.arraycopy()用时：12859ms循环拷贝用时：12700msArrays.copyOf()用时：13938ms 测试结果2： 123456$ java ArrayCopyTestObject.clone()用时：44249msSystem.arraycopy()用时：38115ms循环拷贝用时：38092msArrays.copyOf()用时：44240ms …显然，这是完全没有想到的情况，上面是在两台不同的电脑上跑出来的结果，速度排序for循环&gt;System.arraycopy()&gt;Arrays.copyOf()&gt;Object.clone()。 不是说好的Object.clone()更快吗？这就算了。那么为了for循环竟然是最快的？？？ 好吧，只能简单看一下源码了。 System.arraycopy(): 123public static native void arraycopy(Object src, int srcPos, Object dest, int destPos, int length); Arrays.copyOf()，它实际调用的是copyOfRange(): 123456789101112public static &lt;T,U&gt; T[] copyOfRange(U[] original, int from, int to, Class&lt;? extends T[]&gt; newType) &#123; int newLength = to - from; if (newLength &lt; 0) throw new IllegalArgumentException(from + " &gt; " + to); @SuppressWarnings("unchecked") T[] copy = ((Object)newType == (Object)Object[].class) ? (T[]) new Object[newLength] : (T[]) Array.newInstance(newType.getComponentType(), newLength); System.arraycopy(original, from, copy, 0, Math.min(original.length - from, newLength)); return copy;&#125; Object.clone(): 1protected native Object clone() throws CloneNotSupportedException; 可以发现，System.arraycopy()和Object.clone()调用的都是native方法，Arrays.copyOf()调用的是System.arraycopy()。 所以Arrays.copyOf()肯定是比System.arraycopy()慢。但是为什么这些方法都没有最简单的for循环快？？？ 重新测试： 或许是刚才测试的顺序不对？ 修改代码重新测试。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485import java.util.Arrays;public class ArrayCopyTest &#123; private static final int N = 100000; private static final int M = 500000; private static int[] a; public static void main(String[] args) &#123; long a, b, c, d; a = b = c = d = 0; for (int i = 0; i &lt; 10; i++) &#123; a += test1(); b += test2(); c += test3(); d += test4(); &#125; System.out.println("\n************************************\n"); System.out.println("Object.clone()总用时：" + a + "ms"); System.out.println("System.arraycopy()总用时：" + b + "ms"); System.out.println("循环拷贝总用时：" + c + "ms"); System.out.println("Arrays.copyOf()总用时：" + d + "ms"); &#125; private static long test1() &#123; long start, end; start = System.currentTimeMillis(); for ( int i = 0; i &lt; M; i++ ) &#123; a = new int[N]; int[] b = a.clone(); &#125; end = System.currentTimeMillis(); System.out.println("Object.clone()用时：" + (end - start) + "ms"); return end - start; &#125; private static long test2() &#123; long start, end; start = System.currentTimeMillis(); for ( int i = 0; i &lt; M; i++ ) &#123; a = new int[N]; int[] c = new int[N]; System.arraycopy(a, 0, c, 0, N); &#125; end = System.currentTimeMillis(); System.out.println("System.arraycopy()用时：" + (end - start) + "ms"); return end - start; &#125; private static long test3() &#123; long start, end; start = System.currentTimeMillis(); for ( int i = 0; i &lt; M; i++ ) &#123; a = new int[N]; int[] c = new int[N]; for (int j = 0; j &lt; N; j++ ) &#123; c[j] = a[j]; &#125; &#125; end = System.currentTimeMillis(); System.out.println("循环拷贝用时：" + (end - start) + "ms"); return end - start; &#125; private static long test4() &#123; long start, end; start = System.currentTimeMillis(); for ( int i = 0; i &lt; M; i++ ) &#123; a = new int[N]; int[] c = Arrays.copyOf(a, N); &#125; end = System.currentTimeMillis(); System.out.println("Arrays.copyOf()用时：" + (end - start) + "ms"); return end - start; &#125;&#125; 输出： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849$ java ArrayCopyTestObject.clone()用时：24763msSystem.arraycopy()用时：23489ms循环拷贝用时：20988msArrays.copyOf()用时：21984msObject.clone()用时：23799msSystem.arraycopy()用时：23507ms循环拷贝用时：24672msArrays.copyOf()用时：22697msObject.clone()用时：23126msSystem.arraycopy()用时：21553ms循环拷贝用时：23705msArrays.copyOf()用时：21827msObject.clone()用时：21787msSystem.arraycopy()用时：21636ms循环拷贝用时：23640msArrays.copyOf()用时：21864msObject.clone()用时：22287msSystem.arraycopy()用时：23185ms循环拷贝用时：25596msArrays.copyOf()用时：22266msObject.clone()用时：22126msSystem.arraycopy()用时：22300ms循环拷贝用时：25772msArrays.copyOf()用时：22275msObject.clone()用时：22513msSystem.arraycopy()用时：22313ms循环拷贝用时：23993msArrays.copyOf()用时：22965msObject.clone()用时：22571msSystem.arraycopy()用时：22962ms循环拷贝用时：24482msArrays.copyOf()用时：22656msObject.clone()用时：22233msSystem.arraycopy()用时：23668ms循环拷贝用时：25993msArrays.copyOf()用时：23749msObject.clone()用时：22468msSystem.arraycopy()用时：22449ms循环拷贝用时：24113msArrays.copyOf()用时：22535ms************************************Object.clone()总用时：227673msSystem.arraycopy()总用时：227062ms循环拷贝总用时：242954msArrays.copyOf()总用时：224818ms 可以看到在第一轮的时候，循环拷贝最快，后面就变成最慢的了。 但是这里Arrays.copyOf()比System.arraycopy()快。 算了，随便了，这三个其实速度都差不多，具体速度每次测得的结果都不一样，喜欢用哪一个用哪一个吧。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018华为软件精英挑战赛总结]]></title>
    <url>%2F2018%2F05%2F10%2F2018-05-10-hwrj-2018%2F</url>
    <content type="text"><![CDATA[github地址 总结一下自己2018年参见华为软挑的心里历程吧。 自己连续两年参加华为软挑，都止步于区域复赛，去年第9，今年第10。 第一年水平不够，第二年时间不够（当然主要是自己水平不够），也是有一点遗憾吧。 当然，自己参赛的目标就是进入复赛，所以也还好了。 下面内容分为初赛和复赛，但是预测部分单独拿出来，因为它前后的思路没有太大变化。 初赛 比赛题目赛事页面 由于租户对ECS实例(虚拟机,VM)请求的行为具有一定规律,可以通过对历史ECS实例请求的分析,预测到未来一段时间的ECS实例请求,然后对预测的请求分配资源,这样可以 找到一个接近最优的分配策略,实现资源最大化利用,同时也能参考预测的结果制定云数据中心的建设计划。 赛题分为两个部分，一个是虚拟机数量预测，一个是虚拟机最优化部署。 具体来说就是： 一个数据中心的场景。 数据中心现在的业务是向客户出租虚拟机。 客户有多种规格的虚拟机可以选择。 出租虚拟机有一定的规律。 如果能够预测到未来一段时间需要出租的虚拟机，就可以提前针对进行部署。 上面就是虚拟机数量预测的部分。 部署部分： 有一堆虚拟机（多种不同规格）需要部署。 有不同规格的物理机提供挑选，使用数量没有限制。 主要考量两个资源维度，CPU以及内存。 假如考量的是CPU，那么，利用率为虚拟机总CPU / 使用的物理机总CPU。 这一部分就是最优化部署部分。 虚拟机规格：(题目中定死) 123456789101112131415flavor1 1 1024flavor2 1 2048flavor3 1 4096flavor4 2 2048flavor5 2 4096flavor6 2 8192flavor7 4 4096flavor8 4 8192flavor9 4 16384flavor10 8 8192flavor11 8 16384flavor12 8 32768flavor13 16 16384flavor14 16 32768flavor15 16 65536 输入样例，分为train.txt和input.txt: train.txt: 123456789101112565f3813-9c5f flavor6 2015-12-01 00:17:03565f3814-baad flavor1 2015-12-01 01:14:34565f3815-a015 flavor9 2015-12-01 01:18:12565f3816-894d flavor4 2015-12-01 08:05:41565f3817-b641 flavor2 2015-12-01 08:40:44565f3818-9c59 flavor2 2015-12-01 10:54:25565f3819-a5ba flavor2 2015-12-01 10:54:28565f381a-bc52 flavor2 2015-12-01 10:54:28565f381b-be68 flavor2 2015-12-01 10:54:31565f381c-8308 flavor2 2015-12-01 15:38:31565f381d-aeb8 flavor1 2015-12-01 16:22:30565f381e-bbd7 flavor8 2015-12-01 17:38:09 每一行就是在某个时间，有客户在某个时间租借了某个规格的虚拟机。 注意，可能规格会出现flavor20等情况，需自行处理。 input.txt: 123456789101156 128 12003flavor5 2 4096flavor10 8 8192flavor15 16 65536CPU2016-01-25 00:00:002016-02-01 00:00:00 第一行表示物理机的规格(CPU 内存 硬盘)，当然硬盘在本次比赛中就不需要考虑。 第二部分是需要预测的虚拟机，主要信息是哪几种规格的虚拟机需要预测，后面带的是虚拟机的cpu和内存（MB）， 当然，这个规格信息也是多余的，因为这都是题目固定的规格。 这里的CPU就代表部署时考量CPU。 最后两个日期是需要预测的时间段。（一定会挨着最后的历史数据日期，时间段小于两周） 输出样例，: 123456789106flavor5 3flavor10 2flavor15 141 flavor5 22 flavor5 1 flavor10 13 flavor15 14 flavor10 1 第一部分，一共6台虚拟机，需要预测的规格每种各是多少（为0也要写）。 第二部分，使用4台物理机，怎么放。 评分公式： 也就是预测部分要准确，部署部分最优化，才能达到比较高的分数。 部署部分思路先说部署部分，这一部分比较好说一点。 先看公式，利用率为虚拟机总CPU / 使用的物理机总CPU。 在预测完毕之后，实际上虚拟机的数量和规格都确定了，那么分子实际上就确定了。 分母的大小和使用物理机总数有关。 看上面两点可以发现，真正的目标其实是使用最少的物理机来进行部署，和分子大小无关， 所以究竟是考虑CPU还是内存，其实都是一样的优化目标，解出来的答案都是一样的。 那么，这个问题就变成了有一堆东西需要用背包装，使用背包个数越少越好，一想就是个背包问题， 当然，背包问题是问一个背包能装多少东西，这里是使用最少的背包来装东西，所以还是不太一样。 这里实际上应该是一个装箱问题，瞬间变成了一个NP难问题，只能使用近似解法来求解。 首先，第一种启发式解法就是，使用一个背包，尽量装满，然后换下一个背包，这里就把它叫做贪心背包。 然后参考:装箱问题近似算法概述 那么这里的解法就有： 贪心背包。 降序首次适应算法(FFD, First Fit Decreasing)。 降序最佳适应算法(BFD, Best Fit Decreasing)。 升序首次适应算法(FFI, First Fit Increasing)。 升序最佳适应算法(BFI, Best Fit Iecreasing)。 而且这里的value值可以使用CPU，也可以使用内存，所以相当于一共有10种不同解法。 那么如何选择呢？当然是不选择，这些算法的运行速度都非常快，直接将它们全部用一遍，使用最好的那一个。 trick: 如果最后一台物理机只装了一个很小虚拟机，但是这已经是算法解出来最好的方式了，但是它拉低了总的利用率，怎么办？那么这里直接将这个虚拟机删除了就得了， 也就是对预测进行微小的修改。由于预测本来就不是很准，小修改并不会带来什么影响，但是能提升利用率。 复赛 题目变化复赛题目相比较初赛，只有一些小的变化。 初赛物理机规格只有一种，这里变为三种。 部署同时考量CPU和内存。 预测时间段变为1~4星期，并且可能从训练数据结束后0~2星期内开始。 需要预测的虚拟机规格增加到18种。 部署上的变化这里由于多了两种规格的物理机，而且同时考量的CPU和内存，所以这里就需要有所变化。 这里的优化目标相当于最小化剩余资源的碎片大小，考虑贪心背包方法，这里背包的value值就需要重新定义， 为了达到最小化剩余资源的碎片大小的目标，试着把一种规格的虚拟机value定义为： 虚拟机CPU/物理机CPU + 虚拟机内存/物理机内存 那么这里有三种规格的物理机，怎么选呢？在使用一个新背包来装的时候，将三个背包都试一遍，选择装的最满的那一个。 这个最满的定义就是碎片率最小，物理机使用CPU/物理机CPU + 物理机使用内存/物理机内存。 结果这里的效果非常好，基本能够达到部署分数97%以上…启发式有时候就是这样。 PS: 程序里面在两台物理机一样满的时候，还使用了一种奇葩的比较方式，而且思路上是写错了才出现的，结果效果更好了… 由于代码写错了导致看不懂思路，这里没法阐述… 总之通过这样的方法，在部署问题上的提升空间不大了。 预测部分其实就像上面部署所描述的，虽然部署是个NP难问题，但是其实部署很多时候都能接近最优或者是达到最优， 所以最难的部分还是在预测上面。 分析数据比赛给了一部分线下训练数据，当然只是用来进行线下的测试，测试现在的预测模型的效果，而不是用来训练模型， 因为这次比赛的训练阶段是放在线上进行的，会输入训练数据。而且线下的这部分训练数据并不多。 而且数据很奇怪，分成两个部分，2015.01~2015.08和2015.12~2015.01，这就导致第二部分的数据时间段较短， 并且与第一部分的数据差距很大。 这里为了连续性，所以只分析了2015.01~2015.08这一块数据。 总体分布： 可以看到每一天的VM数量都有很大的变化，看不出有什么规律。 每周的VM数量： 这个可以明显的感觉到后面的VM数量比前面的要多。 VM数量累计和： 上面的图看着就比较平滑，有一点线性增长的感觉，当然图上略微比线性增长快一点。 下面分布画出几种flavor的数据分布情况。 flavor1: flavor2: flavor8: 上面没有把所有都画出来，但是实际上差不多，可以发现数量分布很不均匀，有的flavor数量多， 画图看起来就比较平滑，有的flavor数量少，看起来很难看。 这也导致了模型很难达到较高的精度，因为数据本来就带有了太多的随机性。 基本模型思路这里首先就会想到使用线性回归，当然，这里的线性回归可能有两种理解， 线性回归理解1： 样本有n个属性，通过n个属性乘上n个权重相加得到y。 w_1 x_1 + w_2 x_2 + ... + w_n x_n = y那么可以把前面n天的VM数量作为样本属性，后面接着一段时间的vm数量作为样本y，来进行模型训练。 线性回归理解2： 就是一条直线， y = kx + b那么也就是 x 就是某一天（或者某段时间）， y就是这一天（或这段时间）的VM数量。 这里为了简单，我把线性回归理解2叫做真线性回归。 这里当然还可以使用别的模型，例如， 二次函数回归： y = a x^2 + bx + c这个就类似真线性回归，只不过使用二次曲线来拟合。 LSTM： 这种时间序列的肯定会想到LSTM，但是这里数据量太少，直接PASS。 神经网络： 同样的这里数据量太少，直接PASS。 arima： 写起来可能比较复杂，而且效果可能也不会好，感觉这里数据没太多内在规律，数量量也不够。 究竟怎么样，这里没有尝试，不得而知。 直接平均法： 直接使用平均算出每一天或者每一周一种VM的数量，实际上效果还行。 加权平均法： 和平均的思想一样，但是越往后的天数权重越大，因为间隔时间越近的数据之间相关性肯定是越强。效果好于直接平均法。 直接(加权)平均乘系数法： 直接(加权)平均挺好的，但是通常结果偏小，因为VM增加越来越快，所以可以在直接平均的基础上再乘上一个大于1的系数。 事实上，大部分人使用的都是这个方法，系数完全靠猜。好吧，我们也是使用这样的方法进入复赛的。 基本采样思路采样应该依据模型来进行，不同模型对于采样的要求不一样，对比线性回归和真线性回归， 它两之间对于采样就有不一样的要求。 还需要注意的是，从上面图就可以看出，采样以天为基本单位，效果可能很差，因为变化太大了。 而且，星期六与星期天的VM数量较少。 所以采样可以用周作为单位来采样。 时间段采样 上图的采样方式以某几天作为X，后面几天作为Y，当然作为Y的这”几天”的长度一般设置为等于输入需要预测的时间段长度。 这里对每种VM的数量进行区分。 这样的采样方式得到的样本就比较适合线性回归、神经网络等模型。 图中X的某一列就是某一天VM的发生数量。这里为了平滑X的属性，可以将这一列的数值转化为以这一天为结束的前一周的VM总和。 按周采样 图中以每一周作为一个x坐标，这周的VM数量作为y。 这里对每种VM的数量进行区分。 这种采样方式就比较适合真线性回归，学习出来的直线就类似图中的红线。 VM总和采样 这里的采样以每一天为一个单位，但是这里的y表示从开始到这一天，flavor数量的累计。 这里对每种VM的数量进行区分。 这样采样的目的是为了使得样本更加平滑，适合真线性回归模型。 图上的红线就是真线性回归，蓝线就是二次回归。 不区分VM的种类上面的采样都是对VM的种类进行了区分的，这里还有一种思路，就是不对VM的种类进行区分， 采样直接针对所有VM的数量之和。 那么这样学习出来的模型只能预测出所有种类的VM总量，但是题目需要的是某一种VM的数量， 怎么办？ 这里可以通过历史数据，统计每一种VM所占总量的百分比，然后将预测出来的结果乘上相应比例，得到对于VM的数量。 这样不区分VM的种类的好处，就是能使得数量变化更加平滑，真线性回归能有更好的学习效果。 PS: 这里进行模型学习的时候，使用的都是随机梯度下降(SGD)，当然，对于真线性回归来说，可以直接使用直接求解得到最优解， 但是这里没有使用，因为试了一下效果反而变差了，不知道为什么。 最终提交模型初赛如上面所说，使用的是加权平均乘系数法。 在复赛中，尝试了几种模型，包括加权平均法乘系数，效果都不好。 最后一次提交的模型效果好一点，要不连第10名都没有，这个模型是： 不区分VM的总和采样 + 真线性回归 + 直接平均法 + 只使用最近两个月的数据 这里对于占总量的比例大于0.02的VM，使用真线性回归得到的预测结果，但是对于小于0.02的VM， 使用的是直接平均法。对于离得太远的数据，使用之后反而会使得效果变差，所以通常都不会把所有的训练数据都用上。 最后只得到了第10名，很遗憾没有进入复赛。 有空再看看别人的博客吧。]]></content>
      <categories>
        <category>competition</category>
      </categories>
      <tags>
        <tag>competition</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[匈牙利算法和KM算法]]></title>
    <url>%2F2018%2F05%2F05%2F2018-05-05-KM%2F</url>
    <content type="text"><![CDATA[本来是在写最小费用最大流的，然后找到Going Home这道题。这道题的确是可以使用费用流来解，但是复杂度有点儿高， 然后想了一下，其实可以用匹配来解决。那么既然都用匹配了，这里就将算法进行一下记录，毕竟匹配算法还是比较常用的。 在这里，主要还是针对偶图来说，下面一步一步记录。 偶图偶图也称为二部图，是指具有二分类（X，Y）的图，它的点集可以分解为两个（非空）子集X和Y，使得每条边的一个端点在X中， 另一个端点在Y中。 完全偶图就是X中每一个点与Y中每一个点都有连边。 下图就是一个偶图，但不是完全偶图： 匹配给定图G=(V,E)。设M是E的一个不包含环的子集，它的任意两条边在G中均不相邻，则称M为G的匹配。 M中一条边的两个端点称为在M下是配对的，同时这两个点也是饱和的。若在M下，G的每一个顶点都是饱和的， 则称M为G的完美匹配。若G没有另外的匹配M’，使得|M|&lt;|M’|，则称为G的最大匹配。完美匹配也是最大匹配， 反之不成立。 对于偶图来说，一条边连接了X中的一个点和Y中的一个点，通常在这里就是为了找到最大匹配或者是完美匹配， 使得X中（或Y中）的点能够都达到饱和。 偶图都有最大匹配，但是只有在X的点数等于Y的点数的情况下，才可能有完美匹配。X与Y点数相等的完全偶图显然一定有完美匹配。 对于一个X与Y点数相等的完全偶图，如果每一条边都有一个权重，那么在其中寻找一个具有最大权和的完美匹配，就称这个匹配为最优匹配。 上图是一个最大匹配，注意到它不是完美匹配（V8不饱和）。 匈牙利算法匈牙利算法可以用于在偶图中寻找最大匹配。 它的思路的确很简单，直接先举例： 要在上图中寻找最大匹配。 按照顺序来一个点一个点的进行匹配。 x1直接匹配到y1。 x2先查看y1，发现y1已经匹配了，所以又查看y2，它还没有匹配，所以x2匹配上y2。 x3先查看y2，发现y2已经匹配了，所以又查看y3，它还没有匹配，所以x3匹配上y3。 但是对于x4，很尴尬，它会发现y2，y3都已经匹配了。 这时就是体现匈牙利算法思想的时候了，那就是去寻求一条可扩路。 可扩路就是起点与终点均为非饱和点的交错路。交错路就是处于匹配中的边和未处于匹配中的边相互交错组成的路。 如上图所示，黄色边表示不在当前匹配中的边，红色边表示在匹配中的边，红黄交错，所以这里就是交错路。 对于上面一条路，它的起点和终点都是非饱和点，所有就是一条可扩路。 那么将黄色红色相互交换，就能得到下面那条路，相当于匹配边与未匹配边相互交换，这样这条路上所有的点就都饱和了，岂不美哉？ 那么回到x4，我们希望以x4这个未饱和点为起点，去找到一条可扩路，然后匹配边与非匹配边一交换，x4就能饱和了。 这里使用深度优先搜索的方式来找可扩路，先找到y2，然后y2找到x2，然后x2再找到y4。于是就找到了可扩路（x4，y2，x2，y4）。 PS：事实上在这里深度优先搜索时，要进过几次搜索失败才能到找到上面这条可扩路： x4，y2，x2，y1，x1，y2。（重复y2，搜索失败） x4，y2，x2，y1，x1，y3，x3，y2（重复y2，搜索失败） x4，y2，x2，y4。（搜索成功） 进行扩充，也就是将匹配（x2，y2）换成了（x2，y4）、（x4，y2）。 最后将（x5，y5）匹配上，这里就找到了最大匹配。 步骤总结： 初始化顶点编号，令n=1。 若n大于顶点数N，结束。否则步骤3。 深度优先搜索寻找顶点n的可扩路。如果找到，进行扩充，如果未找到，则这个顶点没法饱和。n = n + 1，返回步骤2. KM算法匈牙利算法用于在偶图中寻找最大匹配。 那么如果要在完全偶图中寻找最优匹配，就需要用到这里的KM算法。 KM算法的思想实际上并不复杂，举个简单的例如描述一下它的思想： 假如有M个工人去做M份工作，这些工作每个工人都会做，但是效率不一样，所以希望分配工作能够使得总效率最大。 先假设你不会KM算法，那么你就按照正常人的思路去想： 先让每个人都挑他们效率最高的那份工作，如果都不冲突，那显然分配已经最优了。否则有冲突，那么继续想办法。 对于冲突了的几个人，让他们中最叼的那个哥们换另一份工作试试，看看能不能解决冲突，这样损失的总效率最小。 比如A、B、C之间冲突了，假如一共5份工作(a,b,c,d,e)， 他们的效率分别是A(100,100,99,99,99)、B(8,7,4,3,2)、C(5,5,1,1,1)， 在第一轮里面，按照最大效率的选择，A想选ab中的一个，B想选a，C想选ab中的一个，显然他们三个冲突了。 假如这时d没人选，你如果拍脑袋想，“B你去做a，A你去做b，C你去做d，因为C你最垃圾，A做b的效率可是100”， 那么三人的总效率加起来就是8 + 100 + 1 = 109，但是很遗憾，不是最优。 这时你的分配应该是“B你去做a，C你去做b，A你去做d，因为C你就只能做这玩意，换一个别的你做不了，A换成d也差不多效率”， 这是三人的总效率加起来就是8 + 5 + 99 = 112，这时就是最好的分配方案。 当然这个例子比较简单，在复杂情况下冲突会发生多次。 那么按照完全偶图(X,Y)来说，思路就是： 控制目前图上能选边的数量，第一轮每个X都只有权重最大边能选。 使用匈牙利算法来分配，如果分配成功，这就是最优匹配，否则转步骤3。 对于冲突的部分，想办法添加边进来解决冲突，添加的边是冲突的几个点中权重损失最小的那一条，然后转步骤2。 上面的都是语言上的描述，很不准确，下面搬一下书上的数学语言描述。 可行顶点标号定义：若在顶点集$X \bigcup Y$上的实值函数L适合下述条件：对所有的$x \in X$及$y \in Y$均有 L(x) + L(y) \geqslant w(xy)则把这个函数称为该偶图的一个可行顶点标号。 可行顶点标号也就是每条边的权重都没有它的两端点标号之和大，不管偶图是什么样子，都存在一个可行顶点标号： \begin{cases} L(x) = \max_{y \in Y} w(xy), 若 x \in X \\ L(y) = 0, 若y \in Y \end{cases}其实这就是上面的“每个X都只有权重最大边能选”。 相等子图定义：若L是可行顶点标号，则$E_L$表示可行性顶点标号定义中，使得等式成立的那些边，即： E_L=\{xy|xy \in E, L(x) + L(y) = w(xy)\}则称具有边集$E_L$的G的生成子图为对应于可行顶点标号L的相等子图，记为$G_L$。 相关定理设L是G=(V,E)的可行顶点标号。若$G_L$包含完美匹配$M^*$，则$M^*$是G的最优匹配。 证明： 假设$G_L$包含完美匹配$M^*$。由于$G_L$是G的生成子图，所以$M^*$也就是G的完美匹配。于是 W(M^*) = \sum_{e \in M^*} w(e) = \sum_{v \in V} L(v)这是因为每个$e \in M^*$都属于这个相等子图，且$M^*$的边的端点覆盖V的每个顶点恰好一次。 另一方面，若M是G的任一完美匹配，则有 W(M) = \sum_{e \in M}w(e) \leqslant \sum_{v \in V} L(v)从上面两个式子可以看出，$W(M^*) \geqslant W(M)$。于是$M^*$是最优匹配。 Kuhn-MunkresKM算法由Kuhn(1955)和Munkers(1957)提出，所以取名叫KM。 它的思想就是： 给定一个可行顶点标号。 在相等子图上使用匈牙利算法，如果完美匹配，则也是最优匹配（上面已经证明）。否则进入下一步。 修改可行顶点标号，使得相等子图中冲突的几个点的边更多，再回到步骤2。 它的步骤如下（主要需要注意的就是它修改可行顶点标号的方式）： 从任一可行顶点标号L开始，然后决定$G_L$，并且在$G_L$中选取任一匹配M。 若X是M饱和的，则M是完美匹配，且由定理15知M是最优匹配，在这种情况下，算法终止。 否则，令u是一个M非饱和点，置$S=\{u\}$，$T= \varnothing$。 若$N_{G_L}(S) \supset T$，则转到步骤3。否则$N_{G_L}(S)=T$。计算 \alpha_L = \min_{x \in S \\ y \notin T} \{L(x) + L(y) - w(xy)\}且由 \hat{L} = \begin{cases} L(v) - \alpha_L , v \in S, \\ L(v) + \alpha_L , v \in T, \\ L(v), 其它 \end{cases}给出可行顶点标号$\hat{L}$（注意$\alpha_L &gt; 0$且$N_{G_{\hat{L}}} \supseteq T$）。 以$\hat{L}$代替L，以$G_{\hat{L}}$代替$G_L$。 在$N_{G_L}(S) \setminus T$中选择一个顶点y。和上节中树的生长程序一样，考察y是否M饱和。 若y是M饱和的，且$yz \in M$，则用$S \cup \{z\}$代替S，用$T \cup \{y\}$代替T，再转到步骤2. 否则，设P是$G_L$中的M可扩(u,y)路，用${M}’=M \triangle E(P)$代替M，并转到步骤1。 上面的$N_{G_L}(S)$表示在$G_L$这个图中，S这个点集的所有相邻的点。$N_{G_L}(S) \setminus T$表示$N_{G_L}(S)$这个集合减去T这个集合。$M \triangle E(P)$在这里表示$M - M \cap E(p) + (E(p) - M \cap E(p))$，其实也就是在可扩路上进行匹配扩展，边集交换。 例子实际操作只看上面的步骤其实还不太会实际操作，这里给一个实际的例子。 上图就是一个具体的例子，左边就是权重矩阵，行表示x点，列表示y点， x顶点的标号已经初始化（按照上面描述的可行顶点标号中的方法初始化）。 图的右边就是现在的相等子图（边的权重等于两端点的标号之和）。 那么下面的步骤就是在相等子图上使用匈牙利算法。 很简单的，前三个x点就能找到匹配。 但是到x4时，发现找不到匹配，即使经过搜索也找不到（肉眼就能看出来x1，x3，x4三个点，一起抢y2，y3两个点，显然不能完美匹配）。 那么修改顶点标号。 按照上面的KM算法步骤2中的公式$\alpha_L = \min_{x \in S \ y \notin T} \{L(x) + L(y) - w(xy)\}$， 这里的S就是{x1,x3,x4}这三个点， T就时{y2，y3}这两个点，可以算出$\alpha = 1$。 同样按照KM算法步骤2中的公式，用算出来的$\alpha = 1$，修改顶点标号，x1，x3，x4三个点标号减去$\alpha$，y2，y3两个点标号加上$\alpha$， 图中左边的顶点标号已经修改，右边相等子图发生了变化，蓝色边就是新加入的边。 另外需要注意的是(x2,y2)这条边不在相等子图里面了，这个细节表明了相等子图中的边可不是越来越多！增加边的同时也会有减少的边！ 所以上面算法说了，可以从任一可行标点开始！当然，通常写代码时从X标号值最大开始，思考起来会比较连贯。 然后很简单的就能得到完美匹配，也就是这里的最优匹配。 KM算法复杂度注意到每一次找可扩路失败会修改顶点标号，在修改顶点标号后，都会至少有一个新的y端点加入， 也就是对于一个未饱和点，最多修改$O(n)$次顶点标号就能找到可扩路。 KM算法需要找到可扩路的数量为$O(n)$。 每次尝试寻找可扩路的复杂度$O(n^2)$。 尝试找到可扩路，最多尝试$O(n)$次，也就是最多需要修改$O(n)$次顶点标号。 每一次修改顶点标号需要$O(n^2)$的复杂度。 从两个方面看， 可扩路的数量 * 尝试寻找可扩路次数 * 尝试寻找可扩路的复杂度为$O(n^4)$。 可扩路的数量 * 最多需要修改顶点标号的次数 * 每一次修改顶点标号的复杂度为$O(n^4)$。 所以KM算法的复杂度为上面两个部分相加，所有最后还是$O(n^4)$。 PS：网上很多博客都说可以把修改顶点标号的复杂度降到$O(n^3)$（的确可以），然后KM算法的复杂度就能降到$O(n^3)$??? 但是他们用的都是DFS，对此我表示深刻的怀疑，感觉应该是把复杂度降到$O(n^4) + O(n^3)$，但是实际上应该还是$O(n^4)$的复杂度， 不过降低了常数因子，应该是快了一倍，但是并不能有数量级上的变化。只有使用BFS的写法才能达到$O(n^3)$，后面进行解释。 KM算法个人理解可以从上面例子看出KM算法的精髓，那就是在相等子图找不到完美匹配时，修改顶点标号， 让冲突的那几个点在相等子图里面能够有更多的边，并至少多连接一个y顶点。 这里之所以$\alpha$取min，是为了顶点标号之和损失最小。因为： 最优匹配的值就等于所有点顶点标号之和。 在最优匹配里面每一个顶点出现一次仅一次，相等子图的每条边权重等于它两端点标号和。 这里对于X标号值从大到小的算法步骤而言，每一次发生冲突，都是因为几个x点它们共同可以选择的y点的数量比它们小（例如上面例子中三个x点竞争两个y点）， 也就是KM算法步骤2中的S集合的规模一定大于T集合，$|S|&gt;|T|$。所以在进行标号修改时，由于S中的点减去标号值，T中的点加上标号值， 所以所有点的标号之和就一定会比之前小。 如果发生冲突，S集合的规模一定大于T集合，$|S|&gt;|T|$。 标号修改量$\alpha$一定是个大于零的值。 在进行标号修改时，S中的每个点减去标号值，T中的每个点加上标号值。 所有点的标号之和一定减小。 上面这段是为了表达KM算法有一种这个意味：“最开始期望一个最大的最优匹配，如果不行再一点一点降低期望。” 代码是时候上代码了。 DFS版本DFS版本（复杂度$O(n^4)$）: 虽然我没有参考一个网络上的模板，但是写出来也大同小异… 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283public class KM_DFS &#123; private int[][] table = null; // 权重矩阵（方阵） private int[] xl = null; // X标号值 private int[] yl = null; // Y标号值 private int[] xMatch = null; // X点对应的匹配点 private int[] yMatch = null; // Y点对应的匹配点 private int n = 0; // 矩阵维度 private int a = 0; // 标号修改量 public int solve(int[][] table) &#123; // 入口，输入权重矩阵 this.table = table; init(); for ( int x = 0; x &lt; n; x++ ) &#123; // 为每一个x寻找匹配 boolean[] S = new boolean[n], T = new boolean[n]; // S集合，T集合 a = Integer.MAX_VALUE; while ( !dfs(S, T, x) ) &#123; // 找到可扩路结束，否则修改标号值 LModified(S, T); Arrays.fill(S, false); Arrays.fill(T, false); a = Integer.MAX_VALUE; &#125; &#125; int value = 0; for ( int x = 0; x &lt; n; x++ ) &#123; value += table[x][xMatch[x]]; &#125; return value; &#125; private boolean dfs(boolean[] S, boolean[] T, int x) &#123; // 深度优先搜索 S[x] = true; for ( int y = 0; y &lt; n; y++ ) &#123; if ( T[y] ) &#123; continue; &#125; int tmp = xl[x] + yl[y] - table[x][y]; if ( tmp == 0 ) &#123; // 在相等子树中 T[y] = true; if ( yMatch[y] == -1 || dfs(S, T, yMatch[y]) ) &#123; // 1. y顶点没有匹配，那么进行匹配 xMatch[x] = y; // 2. dfs寻找可扩路成功，那么这条x，y就会因为可扩路的扩展而交换到匹配中 yMatch[y] = x; return true; &#125; &#125; else &#123; // 不在相等子树中 a = Math.min(tmp, a); &#125; &#125; return false; &#125; private void init() &#123; this.n = table.length; this.xl = new int[n]; this.yl = new int[n]; Arrays.fill(xl, Integer.MIN_VALUE); for ( int x = 0; x &lt; n; x++ ) &#123; for ( int y = 0; y &lt; n; y++ ) &#123; if ( table[x][y] &gt; xl[x] ) &#123; xl[x] = table[x][y]; &#125; &#125; &#125; this.xMatch = new int[n]; this.yMatch = new int[n]; Arrays.fill(xMatch, -1); Arrays.fill(yMatch, -1); &#125; private void LModified(boolean[] S, boolean[] T) &#123; // 修改标号值 for ( int i = 0; i &lt; n; i++ ) &#123; if ( S[i] ) &#123; xl[i] -= a; &#125; if ( T[i] ) &#123; yl[i] += a; &#125; &#125; &#125; &#125; BFS版本版本1（复杂度$O(n^4)$）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100public class KM_BFS_1 &#123; private int[][] table = null; // 权重矩阵（方阵） private int[] xl = null; // X标号值 private int[] yl = null; // Y标号值 private int[] xMatch = null; // X点对应的匹配点 private int[] yMatch = null; // Y点对应的匹配点 private int n = 0; // 矩阵维度 public int solve(int[][] table) &#123; // 入口，输入权重矩阵 this.table = table; init(); for ( int x = 0; x &lt; n; x++ ) &#123; bfs(x); &#125; int value = 0; for ( int x = 0; x &lt; n; x++ ) &#123; value += table[x][xMatch[x]]; &#125; return value; &#125; private void bfs(int startX) &#123; // 为一个x点寻找匹配 boolean find = false; int endY = -1; int[] yPre = new int[n]; // 标识搜索路径上y点的前一个点 boolean[] S = new boolean[n], T = new boolean[n]; // S集合，T集合 Arrays.fill(yPre, -1); int a = Integer.MAX_VALUE; int[] queue = new int[n]; // 队列 int qs = 0, qe = 0; // 队列开始结束索引 queue[qe++] = startX; while (true) &#123; // 循环直到找到匹配 while (qs &lt; qe &amp;&amp; !find) &#123; // 队列不为空 int x = queue[qs++]; S[x] = true; for (int y = 0; y &lt; n; y++) &#123; int tmp = xl[x] + yl[y] - table[x][y]; if ( tmp == 0 ) &#123; // 相等子树中的边 if (T[y]) &#123; continue; &#125; T[y] = true; yPre[y] = x; if (yMatch[y] == -1) &#123; endY = y; find = true; break; &#125; else &#123; queue[qe++] = yMatch[y]; &#125; &#125; else &#123; // 不在相等子树中的边，记录一下最小差值 a = Math.min(a, tmp); &#125; &#125; &#125; if ( find ) &#123; break; &#125; qs = qe = 0; for ( int i = 0; i &lt; n; i++ ) &#123; // 根据a修改标号值 if ( S[i] ) &#123; xl[i] -= a; queue[qe++] = i; // 把所有在S中的点加回到队列中 &#125; if ( T[i] ) &#123; yl[i] += a; &#125; &#125; a = Integer.MAX_VALUE; &#125; while ( endY != -1 ) &#123; // 找到可扩路最后的y点后，回溯并扩充 int preX = yPre[endY], preY = xMatch[preX]; xMatch[preX] = endY; yMatch[endY] = preX; endY = preY; &#125; &#125; private void init() &#123; this.n = table.length; this.xl = new int[n]; this.yl = new int[n]; Arrays.fill(xl, Integer.MIN_VALUE); for ( int x = 0; x &lt; n; x++ ) &#123; for ( int y = 0; y &lt; n; y++ ) &#123; if ( table[x][y] &gt; xl[x] ) &#123; xl[x] = table[x][y]; &#125; &#125; &#125; this.xMatch = new int[n]; this.yMatch = new int[n]; Arrays.fill(xMatch, -1); Arrays.fill(yMatch, -1); &#125;&#125; 版本2（复杂度$O(n^3)$）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121public class KM_BFS_2 &#123; private int[][] table = null; // 权重矩阵（方阵） private int[] xl = null; // X标号值 private int[] yl = null; // Y标号值 private int[] xMatch = null; // X点对应的匹配点 private int[] yMatch = null; // Y点对应的匹配点 private int n = 0; // 矩阵维度 public int solve(int[][] table) &#123; // 入口，输入权重矩阵 this.table = table; init(); for ( int x = 0; x &lt; n; x++ ) &#123; bfs(x); &#125; int value = 0; for ( int x = 0; x &lt; n; x++ ) &#123; value += table[x][xMatch[x]]; &#125; return value; &#125; private void bfs(int startX) &#123; // 为一个x点寻找匹配 boolean find = false; int endY = -1; int[] yPre = new int[n]; // 标识搜索路径上y点的前一个点 boolean[] S = new boolean[n], T = new boolean[n]; // S集合，T集合 int[] slackY = new int[n]; // Y点的松弛变量 Arrays.fill(yPre, -1); Arrays.fill(slackY, Integer.MAX_VALUE); int[] queue = new int[n]; // 队列 int qs = 0, qe = 0; // 队列开始结束索引 queue[qe++] = startX; while (!find) &#123; // 循环直到找到匹配 while (qs &lt; qe &amp;&amp; !find) &#123; // 队列不为空 int x = queue[qs++]; S[x] = true; for ( int y = 0; y &lt; n; y++ ) &#123; if ( T[y] ) &#123; continue; &#125; int tmp = xl[x] + yl[y] - table[x][y]; if ( tmp == 0 ) &#123; // 相等子树中的边 T[y] = true; yPre[y] = x; if ( yMatch[y] == -1 ) &#123; endY = y; find = true; break; &#125; else &#123; queue[qe++] = yMatch[y]; &#125; &#125; else if ( slackY[y] &gt; tmp ) &#123; // 不在相等子树中的边，看是否能够更新松弛变量 slackY[y] = tmp; yPre[y] = x; &#125; &#125; &#125; if ( find ) &#123; break; &#125; int a = Integer.MAX_VALUE; for ( int y = 0; y &lt; n; y++ ) &#123; // 找到最小的松弛值 if ( !T[y] ) &#123; a = Math.min(a, slackY[y]); &#125; &#125; for ( int i = 0; i &lt; n; i++ ) &#123; // 根据a修改标号值 if ( S[i] ) &#123; xl[i] -= a; &#125; if ( T[i] ) &#123; yl[i] += a; &#125; &#125; qs = qe = 0; for ( int y = 0; y &lt; n; y++ ) &#123; // 重要！！！控制修改标号之后需要检查的x点 if ( !T[y] &amp;&amp; slackY[y] == a ) &#123; // 查看那些y点新加入到T集合，注意，这些y点的前向x点都记录在了yPre里面，所以这些x点不用再次入队 T[y] = true; if ( yMatch[y] == -1 ) &#123; // 新加入的y点没有匹配，那么就找到可扩路了 endY = y; find = true; break; &#125; else &#123; // 新加入的y点已经有匹配了，将它匹配的x加到队列 queue[qe++] = yMatch[y]; &#125; &#125; slackY[y] -= a; // 所有松弛值减去a。(对于T集合中的松弛值已经没用了，对于不在T集合里面的y点， &#125; // 它们的松弛值是通过S集合中的x点求出的，S集合中的x点的标号值在上面都减去了a，所以这里松弛值也要减去a) &#125; while ( endY != -1 ) &#123; // 找到可扩路最后的y点后，回溯并扩充 int preX = yPre[endY], preY = xMatch[preX]; xMatch[preX] = endY; yMatch[endY] = preX; endY = preY; &#125; &#125; private void init() &#123; this.n = table.length; this.xl = new int[n]; this.yl = new int[n]; Arrays.fill(xl, Integer.MIN_VALUE); for ( int x = 0; x &lt; n; x++ ) &#123; for ( int y = 0; y &lt; n; y++ ) &#123; if ( table[x][y] &gt; xl[x] ) &#123; xl[x] = table[x][y]; &#125; &#125; &#125; this.xMatch = new int[n]; this.yMatch = new int[n]; Arrays.fill(xMatch, -1); Arrays.fill(yMatch, -1); &#125;&#125; 代码解释不把一个算法写成代码，真的不能算看懂了这个算法。 写这几份代码花费了一天的时间，大部分时间都用在了BFS代码上，巧妙的地方太多了，先说一下BFS算法的思想： BFS算法思想：BFS_1代码的思想其实没什么特殊的，只是和DFS的遍历方式不一样，所有减小了一些边的重复遍历次数。 但是对于BFS_2这份代码来说，第一个就是松弛变量，它使得每个x节点不用重复入队，第二个是数组实现队列，省掉了很多的容器操作。 松弛变量（slack数组）这个东西，在DFS里面没有很大的作用，但是在BFS里面就变得很重要，它是BFS_2算法的核心。 松弛变量： 每一次修改标号值，都在相等子图里面会增加y点。 每一次增加的y点，就是松弛变量值最小的那些y点。 加入的y点在可扩路中的前向点是在计算最小松弛变量$L(x) + L(y) - w(xy)$中的那个x点。 修改标号值时使用的最小松弛变量值是从那些不在T集合中的y点的松弛变量值中选出来的。 所有y点的松弛变量值都是由在S集合中的那些x点算出来的。 未被加入T集合的y点，在修改标号值时，由于S集合的所有x点的标号值减去了$\alpha$，所以这些y点的松弛变量值也要减去$\alpha$。 BFS中的queue：本来应该是使用集合Queue来操作，但是那样增加了许多复杂度，实际上每个x最多入队一次，所以这里可以使用数组来进行操作。 BenchMark对上面三份代码进行测试，查看他们实际上的性能。 例子数 矩阵最大维度 权重范围 DFS用时（秒） BFS_1用时（秒） BFS_2用时（秒） 1000 300 10 2.013 0.34 0.389 1000 300 100 1.418 0.437 0.779 1000 300 1000 2.734 0.923 1.202 1000 300 10000 10.455 3.413 1.435 1000 300 100000 29.347 11.564 1.542 50 1000 10 3.651 0.231 0.263 50 1000 100 2.213 0.296 0.365 50 1000 1000 1.357 0.445 0.716 50 1000 10000 3.677 1.102 0.893 50 1000 100000 12.689 5.531 1.178 BFS全面优于DFS，BFS_2受到权重范围的影响较小。 上面的结果和之前的算法复杂度很接近，果然DFS是$O(n^4)$，BFS_1其实也是$O(n^4)$，BFS_2是真正的$O(n^3)$，当然在权重范围不大时，BFS_1是效果最好的。 例题下面使用一些例题来参考。 1. Going Home这道题来自杭州电子科技大学的OJ，题号1533，Going Home。 题目的意思就是，有几个人要回到房子里面去，每一个房子只能住一个人，现在人数和房子数一样，但是要怎么分配房子才能使的大家回房子的总路程最小。 我知道的解法有两种，第一种是将这看作图，然后使用最小费用最大流，这个方法可以AC，但是感觉太过于复杂了。 第二种方法就是把房子和人看成两个集合，相互连线，权重就是人到房子的距离，这样就转化为了一个寻找最优匹配的问题，不过这里是找路径总长最小， 那么为了沿用模板，把所有权重取一个负号就行。 代码就是用的上面的模板，这里就不贴了。 用时: 2. 奔小康赚大钱这道题来自HDU-2255。 这道题直接就是一个求最优匹配的问题。 这道题对java不友好！！！，我上面的KM算法都试了一下，java就是过不了，艹了。看了提交统计，就没有java版本通过。 改成c++之后就通过了？？？ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132#include&lt;iostream&gt;#include&lt;cstdio&gt;#include&lt;cstring&gt;using namespace std;const int N = 310;const int INF = 0x3f3f3f3f;int table[N][N]; // 权重矩阵（方阵）int xl[N], yl[N]; // 标号值int xMatch[N], yMatch[N]; // 点对应的匹配点bool S[N], T[N];int yPre[N], slackY[N];int queue[N]; // 队列int n; // 矩阵维度void bfs(int startX) &#123; // 为一个x点寻找匹配 memset(yPre, -1, sizeof(yPre)); memset(S, 0, sizeof(S)); memset(T, 0, sizeof(T)); memset(T, 0, sizeof(T)); memset(slackY, INF, sizeof(slackY)); bool find = false; int endY = -1; int qs = 0, qe = 0; // 队列开始结束索引 queue[qe++] = startX; while (!find) &#123; // 循环直到找到匹配 while (qs &lt; qe &amp;&amp; !find) &#123; // 队列不为空 int x = queue[qs++]; S[x] = true; for (int y = 0; y &lt; n; y++) &#123; if (T[y]) &#123; continue; &#125; int tmp = xl[x] + yl[y] - table[x][y]; if (tmp == 0) &#123; // 相等子树中的边 T[y] = true; yPre[y] = x; if (yMatch[y] == -1) &#123; endY = y; find = true; break; &#125; else &#123; queue[qe++] = yMatch[y]; &#125; &#125; else if (slackY[y] &gt; tmp) &#123; // 不在相等子树中的边，看是否能够更新松弛变量 slackY[y] = tmp; yPre[y] = x; &#125; &#125; &#125; if (find) &#123; break; &#125; int a = INF; for (int y = 0; y &lt; n; y++) &#123; // 找到最小的松弛值 if (!T[y] &amp;&amp; slackY[y] &lt; a) &#123; a = slackY[y]; &#125; &#125; for (int i = 0; i &lt; n; i++) &#123; // 根据a修改标号值 if (S[i]) &#123; xl[i] -= a; &#125; if (T[i]) &#123; yl[i] += a; &#125; &#125; qs = qe = 0; for (int y = 0; y &lt; n; y++) &#123; if (!T[y] &amp;&amp; slackY[y] == a) &#123; // 查看那些y点新加入到T集合，注意，这些y点的前向x点都记录在了yPre里面，所以这些x点不用再次入队 T[y] = true; if (yMatch[y] == -1) &#123; // 新加入的y点没有匹配，那么就找到可扩路了 endY = y; find = true; break; &#125; else &#123; // 新加入的y点已经有匹配了，将它匹配的x加到队列 queue[qe++] = yMatch[y]; &#125; &#125; slackY[y] -= a; // 所有松弛值减去a。(对于T集合中的松弛值已经没用了，对于不在T集合里面的y点， &#125; // 它们的松弛值是通过S集合中的x点求出的，S集合中的x点的标号值在上面都减去了a，所以这里松弛值也要减去a) &#125; while (endY != -1) &#123; // 找到可扩路最后的y点后，回溯并扩充 int preX = yPre[endY], preY = xMatch[preX]; xMatch[preX] = endY; yMatch[endY] = preX; endY = preY; &#125;&#125;int solve() &#123; memset(xMatch, -1, sizeof(xMatch)); memset(yMatch, -1, sizeof(yMatch)); memset(yl, 0, sizeof(yl)); for (int x = 0; x &lt; n; x++) &#123; for (int y = 0; y &lt; n; y++) &#123; if (table[x][y] &gt; xl[x]) &#123; xl[x] = table[x][y]; &#125; &#125; &#125; for (int x = 0; x &lt; n; x++) &#123; bfs(x); &#125; int value = 0; for (int x = 0; x &lt; n; x++) &#123; value += table[x][xMatch[x]]; &#125; return value;&#125;int main() &#123; while (~scanf("%d", &amp;n)) &#123; for (int i = 0; i &lt; n; i++) for (int j = 0; j &lt; n; j++) scanf("%d", &amp;table[i][j]); int ans = solve(); printf("%d\n", ans); &#125; return 0;&#125; 用时： 3. Cyclic Tour这道题来自HDU-1853 每一个点需要经过一次仅一次，所有的点都是单向边，所以： 每个点出入度都至少要大于1，否则不能形成环路。 每个点的出入度都大于1，那么一定有环路。 在最后的环游方案中，每个点被入度一次仅一次，出度一次仅一次。 那么把所有点既放到X集合里面，又放到Y集合里面，然后这个偶图的最优匹配就是这个题目的解。 代码从上面那份C++代码改一下就完成了。 总结 KM算法步骤比较简单，思想没那么简单，想写出一个好的代码更不简单。 HDU对java不友好。 参考资料： 《图论及其应用》-高等教育出版社-张先迪、李正良]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2017年华为软件精英挑战赛记录]]></title>
    <url>%2F2018%2F05%2F02%2F2018-05-02-2017_codecraft%2F</url>
    <content type="text"><![CDATA[初赛题目很好，网上找了一会也没有找到题目，我只想说，华为你把往年的题的链接给一下可好？ 凭借记忆，初赛的题目大概是，有一个网络，要在上面部署cdn服务器，怎么部署既能满足用户需求， 又能使得部署费用最少。 具体一点，相当于一个无向图，有一些点是小区，每个小区都有视频业务的需求，而且需求量不一样，每条边是一条链路， 链路有它的带宽限制。现在需要部署cdn服务器，让每一个小区的视频业务都能满足，同时部署cdn服务器的费用越低越好。 每一个点都可以部署cdn服务器（包括是小区的点），每一个小区有一个需求值，每一条边是一条链路，每条链路都是双向的， 每条链路有一个链路容量和一个单位费用（使用链路上一个单位的流量需要多少钱）。cdn服务器数量无限，cdn服务器输出能力无限， 同样cdn服务器也有一个单价。 限制：不能使用第三方库，程序运行时间不超过60s。 简单分析这个需要一步一步来分析： 该选择哪些点来部署服务器？ 在部署好了服务器的情况下，流量该如何流，才能满足所有用户，并且使得费用最小？ 这就是这个题目的两个核心问题。 该选择哪些点来部署服务器？这是这个题目最难的一部分，因为这个选择是一个NP难的问题，所以是没有现成的解法来教你如何求解这个问题的， 通常都是将它变成一个整数规划模型，然后使用求解器来求解。但是由于不能使用第三方库，所以代码全靠手写， 那么这个问题基本上只能使用启发式算法来解。 那么有哪些常用的启发式算法呢？遗传算法、模拟退火、蚁群算法、粒子群算法等。 很好，那么从哪个算法开始呢？显然都不用，第一步当然是写一个最简单的版本，先熟悉问题，在深入问题。 最简单的版本（原始想法）： 版本1：初始每一个小区都给一个cdn，然后再尝试一个个cdn删除，直到不能降低费用，或者不能满足用户需求为止。 版本2：初始没有cdn，然后尝试一个小区一个小区添加cdn，直到需求满足且不能降低费用为止。 初始版本的想法非常简单，但是很遗憾，这个想法在初赛的简单用例上能直接跑出最优解… (有时候问题就是这么简单，只不过很多人还没开始就觉得难，然后就没有然后了) 在部署好了服务器的情况下，流量该如何流，才能满足所有用户，并且使得费用最小？解决这个问题很简单，使用最小费用最大流就解决了。 使用最简单的增广路写法就好。 进阶分析同样的，两个步骤的想法。 该选择哪些点来部署服务器？由于最初始的版本效果很好，那么在这个基础上还能加强吗？ 当然能，很简单就能想到几个操作方法： cdn移动。现在cdn都是直接放在小区上面的，所以自然可以尝试移动cdn，只移动一步， 如果移动能降低费用就移动，否则维持原状。 cdn删除。这个删除不同于上面的原始想法中的删除，这个删除是根据cdn移动来的，如果有cdn移动成功了， 那么意味着图的结构变了，有的点就也许就可以删除了。 很遗憾，初赛使用的就是这个思路做的，最后大概成渝前3的水平… 在部署好了服务器的情况下，流量该如何流，才能满足所有用户，并且使得费用最小？上面选择了增广路写法。 但是，注意到时间限制60s，在这种分两步走的策略里面，需要跑很多次费用流， 这时候的问题就是如何解决好这个问题，也就是用时尽量少。 最小费用最大流： 这里我首先使用的是增广路解法，使用SPFA计算增广路的最短路径。 由于在一次求解最小费用最大流的过程中，需要反复的调用SPFA，那么在重复进行SPFA的过程中，不用每一次都重头开始， 也就是一部分最短路径的结果可以保留，这样在下一次计算时就可以少很多的计算量。 使用这样的方法，跑高级用例也没有问题，否则高级用例会有压力。 由于上面部署的思路太简单了，这个最小费用最大流的魔改是我觉得自己最有意思的地方了。 （具体用时忘记了…太过久远） 代码参见github2017-codecraft 在一年之后，看自己一年之前写的代码，真的写得稀烂…估计再过一年又会嫌弃现在。 复赛题目和上面的题目其实一样， // TODO]]></content>
      <categories>
        <category>competition</category>
      </categories>
      <tags>
        <tag>competition</tag>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jar包的运行以及jre精简]]></title>
    <url>%2F2018%2F03%2F19%2F2018-03-19-java_jar_jre_reduce%2F</url>
    <content type="text"><![CDATA[工程打包无论是什么IDE，打包都是一件十分简单的事。（这里不讨论自己手动打包的情形） jar包的运行如果环境变量中以及添加了jre的路径，那么java的执行如下： 1$ java -jar xxx.jar arg1 arg2 XXX 如果不想每次都使用命令行来输入的话，就可以把上面的命令变成一条bat脚本： 例如取名加start.bat，内容为： 12345@echo offstart .\jre\bin\java -jar xxx.jar %1 %2pause 其中@echo off意思为不打印输出，start意味着新建窗口执行后面的命令，pause表示暂停， 也就是可以在运行结束时出现请按任意键继续...，另外%1 %2表示两个输入参数。 注意：此时jar包的路径就是它的执行路径，有时候程序可能需要取这个路径来进行相对路径的写操作， java中可以使用下面语句来取到： 1System.getProperty("user.dir") jre不添加环境变量运行上面考虑的是环境变量中以及添加了jre的情况，有时候我们可能需要把程序放到别的电脑上来运行， 这时候如果别人的电脑上没有java环境，那就会执行不了，总不能让别人现装一个吧。 考虑到最大化的简便使用者，就需要把jre拿出来，和jar包绑在一起发送给别人。 那么如何使用jre来直接运行java呢？使用jre中bin下的java.exe或者javaw.exe即可： 123$ .\jre\bin\java -jar xxx.jar arg1 arg2 ...或者$ .\jre\bin\javaw -jar xxx.jar arg1 arg2 ... 它两之间没有什么不同，javaw会屏蔽打印信息，而java不会。 那么现在，就只需要将jre放到jar包同一个目录下，使用上面的命令就可以运行程序， 这样就能在没有java环境的电脑上运行，因为我们自带了java运行环境。 这里具体的目录结构当然不重要，看个人喜好了。 精简jre这里使用的是jre 1.8，从jdk-8u131下取出来的，大小为188M。 显然，相比于我的程序来说，这实在是太大了，如果不精简jre，整个程序包就得有近200M， 这有一点不能接受，所以还是需要精简jre。 这里我参考了博客整理JRE瘦身或精简JRE 那么有没有一些精简工具呢？ 搜索了半天，有一个greenvm，但是这程序好像已经非常久没有更新了， 这位作者是不是放弃它了，下载下来试了一下，好像有一点问题，总之我用的时候是报错了。 还有一个jrecreate，这个是oracle公司官方的，不过好像不是针对某个程序设计的？感觉就是提供了几个较小版本的jre？有待考证。 感觉大部分网友还是自己手动进行精简，那么如果要手动精简，首先整理几个点： jre文件下需要考虑的就是两个文件夹bin和lib。 bin文件夹下总的文件数不多，体积大概为80M，其中大部分东西都用不到。 lib文件夹下有很多文件，体积大概有100M，其中有少量的jar包会用到。 有了上面几个点，下面就是第一步精简的策略: 首先对于bin文件夹的精简策略： 手动删除几个文件，然后执行jar包，看是否报错，就这样精简到不能再精简。 想着手动挺烦的，但是其实上这里大部分文件都可以删除，而且文件也不多，几分钟就能清理完毕。 （我清除完就剩下8M，主要是jvm.dll比较大）。 第二，对于lib文件夹的精简策略： 同样的，手动删。 这里可能需要留的文件比较多，所以建议直接按文件大小，很多小文件（1KB，2KB）就不用删了，试一试删除一些大文件。 我这里最后剩下两个较大的文件charsets.jar和rt.jar，那么加起来还是有60M多，恩，还需要继续精简。 精简charsets.jar和rt.jarcharsets.jar和rt.jar中有很多程序运行时需要调用的class文件，所以我们删不掉它们，那么很明显， 只要将程序不用的class文件从中删去，就能减小大小。 要这样做，首先就得知道自己的程序在运行过程中使用了哪些类，可以使用下面命令来打印： 1$ java -jar -verbose:class XXX.jar 这样就能打印我们使用了哪些类，将这些信息重定向到文件中： 1$ java -jar -verbose:class XXX.jar &gt;&gt; class.txt 那么这里就得到了class.txt，其中以[Load ...]开头的就是在加载类。 进一步，我们要将charsets.jar和rt.jar类中的相关class提取出来： 将这两个文件给解压缩了，得到charsets，rt文件夹。 编写代码，取到class.txt中加载的类，对比的去charsets，rt文件夹中找，找到就复制到新文件夹。 这里的代码会附加到最后面，总之，这里得到两个新的charsets，rt文件夹，下面已经去除了不需要的class文件。 直接将这两个文件夹用zip打包（不要将charsets或者rt文件夹也给包了，这样就比原来的jar包多了一层目录了），改名为charsets.jar和rt.jar，放回jre所在的位置中替换既可。 这样下来lib文件夹可以缩小到10M以下。 讲道理应该是可以的，但是我这里报错了，可能是我个人的问题。 实际上的精简操作由于上面的方法最后在运行jar包时报错了，所以我手动添加了一些class回去。 事实上，对于charsets.jar，我没有改动内容，只是将它解压重新用zip压缩了一遍，这样的确可以缩小一点。 对于rt.jar，我将java和sun两个包下的类都保留了，因为不知道到底少了哪个class。 最终jre的大小精简到22M。 问题不知道在后面的使用中还会不会有别的问题。 2018-06-15：使用时问题，我程序中需要有输入文件，输入文件的格式可能是.xml、.xls、.xlsx、.csv， 但是在精简的时候我只使用了.csv文件测试，于是当输入文件变成.xls格式时，就出现了问腿。 所以，在不得已的时候还是不要尝试使用本方法精简jre，由于程序运行过程的变化，可能会出现问题。当然， 如果程序比较简单，没什么变化时，还是可以精简jre的。 代码参考了整理JRE瘦身或精简JRE，进行了少量改动。 jarInputPath是输入文件夹，这里下面就有两个文件夹rt、charsets。（解压了的） jarOutputPath是输出文件夹。 jarPackNames就是两个包名。 classListInputPath就是上面命令打印出来的信息。 classListOutputPath所有这里复制了的类的名字会保存到这个文件中。 另外:输入文件与输出文件的编码注意一下。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697import java.util.ArrayList;import java.io.*;public class ParseClass &#123; private static final String jarInputPath = "./input"; private static final String jarOutputPath = "./output"; private static final String[] jarPackNames = &#123;"rt", "charsets"&#125;; private static final String classListInputPath = "class.txt"; private static final String classListOutputPath = "classClean.txt"; private static ArrayList&lt;String&gt; classList = new ArrayList&lt;&gt;(); public static void main(String[] args) &#123; try &#123; int totalLine = 0; int totalLineOut = 0; BufferedReader br = new BufferedReader( new InputStreamReader(new FileInputStream(classListInputPath), "unicode")); BufferedWriter bw = new BufferedWriter( new OutputStreamWriter(new FileOutputStream(classListOutputPath), "UTF-8")); String line = null; while ((line = br.readLine()) != null) &#123; totalLine++; if ( !line.startsWith("[L") ) &#123; continue; &#125; String str = line.split(" ")[1]; classList.add(str); &#125; // copy class for ( String str : classList ) &#123; if ( copyClass(str) ) &#123; System.out.println( "copy : " + str); bw.write(str); bw.newLine(); totalLineOut++; &#125; &#125; System.out.println("The number of input line: " + totalLine); System.out.println("The number of out line: " + totalLineOut); br.close(); bw.close(); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; private static boolean copyClass(String className) throws IOException &#123; className = className.replaceAll("\\.", "/"); boolean flag = false; for ( String pack : jarPackNames ) &#123; String classPathIn = jarInputPath + "/" + pack + "/" + className + ".class"; String classPathOut = jarOutputPath + "/" + pack + "/" + className + ".class"; File fileIn = new File(classPathIn); File fileOut = new File(classPathOut); if ( !fileIn.exists() || fileOut.exists() ) &#123; continue; &#125; helpCopyFile(classPathIn, classPathOut); flag = true; break; &#125; return flag; &#125; private static void helpCopyFile(String input, String output) throws IOException &#123; File outDir = new File(output.substring(0, output.lastIndexOf("/"))); if ( !outDir.exists() ) &#123; outDir.mkdirs(); &#125; FileInputStream fis = new FileInputStream(input); FileOutputStream fos = new FileOutputStream(output); byte buf[] = new byte[256]; int len = 0; while ( (len = fis.read(buf)) != -1 ) &#123; fos.write(buf, 0, len); &#125; fos.flush(); fis.close(); fos.close(); &#125;&#125;]]></content>
      <categories>
        <category>guide</category>
        <category>java</category>
      </categories>
      <tags>
        <tag>guide</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实验室深度学习服务器选型]]></title>
    <url>%2F2018%2F03%2F08%2F2018-03-08-deep_learning_server_choosing%2F</url>
    <content type="text"><![CDATA[这一篇笔记主要是对实验室服务器选型的一些调研。 当然，其中的很多知识也许在自己以后的装机岁月中会用到。 （目前忽略AMD，毕竟没有接触过，也不是主流） 服务器目标服务器需要扮演的角色有： 作为云化环境下的低层资源。 提供GPU计算能力给深度学习或者其他科学计算。 硬件列表首先，要作为一台PC，它的组成部分至少有，机箱、主板、CPU、内存、硬盘、电源和CPU散热器。 其次，作为需要提供GPU计算能力的服务器，那么它就需要GPU硬件。 机箱 主板 CPU GPU 内存 硬盘 电源 CPU散热 下面，一个一个硬件的来记录调研的信息以及一些自己的理解。 首先可以参考一下A Full Hardware Guide to Deep Learning-Tim Dettmers， 其中比较详细的说了各个部分的问题。 PCIe首先，还是需要先了解一下PCIe通道，在主板、CPU、GPU的参数说明中通常会看到它， 在平常自己攒机的硬件选择时，通常都不用考虑它，因为你不会上多个GPU，但是一旦考虑到要装多个GPU的情景， 最好还是要了解一下。 PCI-Express (peripheral component interconnect express)，它是一种高速串行计算机扩展总线标准， PCIe属于高速串行点对点双通道高带宽传输，所连接的设备分配独享通道带宽，不共享总线带宽， 主要支持主动电源管理，错误报告，端对端的可靠性传输，热插拔以及服务质量(QOS)等功能。 目前一般都是PCIe 3.0，下面的表来自百度百科： PCI Express 版本行代码传输速率吞吐量×1×4×8×161.08b/10b2.5GT/s250MB/s1GB/s2GB/s4GB/s2.08b/10b5GT/s500MB/s2GB/s4GB/s8GB/s3.0128b/130b8GT/s984.6MB/s3.938GB/s7.877GB/s15.754GB/s4.0128b/130b16GT/s1.969GB/s7.877GB/s15.754GB/s31.508GB/s5.0128b/130b32 or 25GT/s3.9 or 3.08GB/s15.8 or 12.3GB/s31.5 or 24.6GB/s63.0 or 49.2GB/s 可以看到3.0单通道的传输速率为984.6MB/s。 简单说一下984.6MB/s是怎么和前面参数联系在一起的： 这里的行代码为128b/130b，意思就是传输的130bit中有128bit是实际传输的数据，其中的2bit可能是附带的信息位什么的， 传输速率为8GT/s，意思是传输能力为8Gbps（我是这样理解的），注意到Gbps表示1000兆比特，所以： 吞吐量 = \frac{8000 * 128}{130 * 8} = 984.6MB/s通常，主板参数中会显示它的PCIe通道数，这表示它所能最大支持的通道数，其实就是PCIe插槽插满的通道数， 要注意到，PCIe插槽的长度并不固定，如下图所示： CPU的PCIe通道数表示CPU所包含的通道数，也就是CPU能够提供的PCIe通道数量，即使主板上有再多的PCIe插槽， CPU的PCIe通道数不够也没用（这里不准确，可能主板支持PCIe扩展，但那好像也不能根本性解决问题）。 GPU的通道数表示GPU所需要使用的通道数，同时也表示插上它需要多大的PCIe插槽，通常都是x16的。 要注意到在PCIe3.0的时代，在深度学习方面，通常x16的通道GPU是用不完的，或许x8就已经足够了， 当然，如果PCIe通道能够完全满足是最好的。 PS：如果一颗CPU有40个PCIe通道，插入两块GPU是能够满足的，也就是16 + 16，如果插上3块GPU， 一般变成16 + 8 + 8，插上四块GPU时变成8 + 8 + 8 + 8。 另外，PCIe通道不只是GPU能用，例如高端的固态硬盘使用的就是PCIe通道。 总结一下，也就两个问题： 主板的PCIe插槽要够。 CPU能提供的PCIe通道要够。 主板主板的选择主要考虑的点： PCIe插槽数，能否插下四张GPU。 CPU槽数，是否要插两个CPU。 最大内存容量。这个看实际的需求来选择即可。 这里先说一说单CPU的主板选择： 大部分人的选择都是华硕X99-E WS，因为它支持7×PCI-E X16显卡插槽。 也就是它有7个插槽，所以它能够插下4张GPU（并不是有多少插槽就能插多少张GPU，事实上空间不足导致有的插槽不能再插）。 如果考虑可能会插到4张GPU的话，这张板子会是不错的选择。 但是，考虑到机器并不光是提供深度学习计算能力，还要有足够的CPU资源来进行虚拟化，所以可能需要选择到两路CPU。 一旦扯到两路CPU，感觉就像进入了一个另外的领域似的，因为支持两路CPU的都是服务器主板，资料也不多，非常复杂。 目前一般自己组机的话推荐使用 C612 芯片组的板子，支持两路CPU，PCIe插槽也多。 总之主板这方面的知识目前还是非常的欠缺… 对于华硕主板的详细参数可以去华硕官网查看，asus.com， 另外supermicro.org可以查到大部分主板的参数。 内存插槽数/通道数：考虑到内存的扩展性，自然需要考虑内存通道数量的问题。 内存通道这个概念很容易混淆，可以参考： 双通道内存有什么优点和缺点？-知乎 为什么只支持双通道内存的主板会配四个内存插槽？-知乎 简单来说，现在的主板大多支持双通道，比较好的主板（例如x99，服务器主板）会可能支持到四通道（也有支持三通道的主板，但是很少见）。 那么只支持双通道但是有四个内存插槽的板子什么意思？十分简单的来说，就是一个通道可以插两块内存， 当两块内存插在同一个通道上的时候，它只是容量叠加，当两个通道分别插上一块或两块内存的时候，才会开启双通道。 也就是两个通道，每个通道两个插槽，自然就是四个插槽。 CPUCPU的选择主要考虑两点： 核心数，核心数越多并行能力就越强。 PCIe通道数，目前单颗CPU最多的能够支持到40个通道。 最大支持内存。 对于单CPU的选择，就没什么要求，看它够不够40个通道，然后才是频率与核心数量， 如果不考虑并行，在运行深度学习模型的时候，其实只使用了CPU单核心，因为大部分计算工作都是GPU做了， CPU没有太大的任务，所以通常CPU核心数不关键，频率也不是很关键，不要太低就行了。 型号 核心数 PCIe通道数 最大内存支持 ECC内存支持 i7-6850k 6核12线程 PCIe通道数40 最大内存支持128G 不支持ECC内存 i7-7800x 6核12线程 PCIe通道数28 最大内存支持128G 不支持ECC内存 E5-2620 v3 6核12线程 PCIe通道数40 最大内存支持768G 支持ECC内存 对于双路CPU来说，那都是E3/E5，这个通常也不用怎么考虑，选两个通道数够的，频率别太低的就行。 Intel的CPU参数查询可以去官网：intel.com。 GPU对于选择GPU主要需要考虑的点： 价格。毕竟GPU可能是整台主机最贵的硬件，一般还是要掂量掂量。 深度学习的计算能力。时间也是金钱。 直接推荐阅读： Picking a GPU for Deep Learning-Slav Ivanov Which GPU(s) to Get for Deep Learning: My Experience and Advice for Using GPUs in Deep Learning-Tim Dettmers 这里面讲了很多关于深度学习GPU选择的问题。 AMD还是NVIDIA在深度学习这个方面，目前不用考虑AMD。 NVIDIA显卡的选择首先，所选择的型号需要支持CUDA，支持列表CUDA GPUs。 深度学习需要用到cuDNN库，那么自然需要显卡支持CUDA。 另外，由于NVIDIA在10系列的显卡上发了力，这一代的提升完全是突破式的，导致在深度学习的机器门槛低了很多， 1080、1080Ti、TITAN Xp这些桌面用户级显卡现在也完全可以胜任深度学习的计算， 比起老架构的计算卡甚至可以快出很多倍。 来看一看1080Ti与K80的比较： 从上图中，大概可以估计1080Ti比K80在深度学习模型的训练上，要快4倍以上。（那么K80之类的计算卡现在还有什么竞争力？这里需要注意到计算卡是双精度的，但是1080Ti之类的是单精度的，但是在很多需要计算的领域，包括深度学习，只需要单精度就够了。） 正是由于10系列显卡的发力，所以现在很多人以及实验室都会自己组装深度学习的工作站，因为这实在比直接买带计算卡的服务器便宜多了。 价格以2018年3月11日网上的价格为例： 型号 品牌 价格 网站 1080 公版 $989.99 亚马逊 1080Ti 公版 ￥10350.00 京东 TITAN Xp 公版 ￥11999.00 京东 TITAN V 公版 ￥26688.00 京东 K80 公版 $2298.00 亚马逊 P6000 公版 $4,669.00 亚马逊 ps：价格都是随意看的，可能不准确，同时亚马逊到国内还有关税没有考虑。 之前1080Ti性价比是最高的，因为那时公版1080Ti也就5000~6000人民币，现在涨了快一倍， 感觉TITAN Xp也可以考虑了。当然，怎么都比计算卡性价比高。 内存对于深度学习来说，内存其实并不重要，实际上训练数据都会读入到显存里面， 大部分装机建议内存至少要大于显存（显卡的内存总和），因为GPU显存大小并不大（以TITAN X为例，它的显存只有12G，也就是插4张也才48G显存，这时候64G的内存也是足够了）。 对于单CPU的主板来说，因为是大多是消费级，也基本不用至强CPU（不是所有主板都能插至强系列），所以基本不能使用ECC内存， 所以通常就直接考虑平常常见的内存就行，海盗船、金士顿、芝奇等的DDR4都可以。 对于双路CPU的主板来说，一般都要使用服务器内存，也就是带ECC功能的内存，同时为了良好的扩展性， 最好是RDIMMs的（LRDIMMs更好），相关可以参考： ECC内存-百度百科 LRDIMMs, RDIMMs, and Supermicro’s Latest Twin-AnandTech 另外简单说一下，对带有ECC的内存，还有一种是RECC内存（Reg ECC），它多比纯ECC内存要多带了一个Register， 它起到扩大内存容量的作用。 具体怎么选择，凭我的知识说不好。 ————分割线———— 下面首先十分推荐阅读内存系列一：快速读懂内存条标签-老狼， 写得很好。 内存条长短内存条有好几种长度，也就是不同的插槽类型，这个需要与主板相匹配。 内存频率：内存频率类似于CPU频率，频率越高也运行越快，但是在内存的频率选择时，需要按主板的内存频率支持来选。 内存容量：内存容量都是2的次方的大小，平常PC使用的一般是4G或者8G的大小，但也有单条16G、32G的大小。 硬盘首先这里不讨论固态盘，硬盘的选择需要考虑下列问题： 容量。 速度。 首先，需要注意到PC硬盘的定位是有好几种的，最简单的，看硬盘颜色， 分为黑盘、蓝盘、绿盘、红盘、紫盘等，关于它们之间的区别，看一个小科普就好： 你知道电脑硬盘有几种颜色吗？有哪些用途及如何选用你知道吗？-sohu 至于选哪种硬盘，个人使用蓝盘就够了，要好一点就用企业级的。 然后简单的知道硬盘还有两种接口就行，SAS和SATA。 电源 功率大小。 价格。 首先，对于功率大小，就是要保证电源能够提供给各个部件足够的功率，功率计算网站： OuterVision® Power Supply Calculator 电源功率一定不能选小了，否则就会很尴尬，而且还要考虑到多个显卡扩展的情况， 如果考虑到4显卡扩展，可能至少需要一个1600w的电源。 至于价格，一般情况下当然是电源越好，价格越高，考虑到可能不间断运行，在电源上就一定要舍得， 并且电源价格实际上也不算太高。 CPU散热 水冷：冷却效果好，安静，好看，但是可能漏液，导致BOOM。 风冷：冷却效果比不上水冷，声音稍大，安全性较好。 这个就看个人喜好了，如果是实验室，当然还是风冷稳一点，品牌一般推荐猫头鹰。 机箱机箱的选择可能会简单一点，也就是要一个足够大的机箱，能够放下主板以及其它的硬件就好了。 如果是机房统一管理放置的情况下，一般都是机架式或刀片服务器，可以插在机架上，占用空间少，放便管理。 但是这类服务器机箱一般比较小，放不下普通消费级显卡。所以在这种情况下一般考虑塔式机箱， 也就类似平常PC机的主机。 对于个人组装单CPU型的，一般推荐海盗船AIR540。对于双CPU型的，不知道能否装下主板， 这可以在购买时咨询店家。 双CPU所需的机箱不太清楚，可以在购买时询问一下店家。 最终建议如果是单CPU的深度学习工作站，那完全可以自己买配件组装： 组装过程十分简单。即使没有装过，看看教程也能学会。 过程很有意思。 对于每天接触电脑的人来说，这些知识也算生活技能了。 但是如果是双CPU，服务器主板，不太建议自己组装（可以买别的商家组好的），主要是： 各个配件不熟。 配件不好买。 商家组好的那种也不算太坑，并且可以询问商家定制化配置，出了问题也好处理。 所有参考资料如何配置一台适用于深度学习的工作站？-知乎 A Full Hardware Guide to Deep Learning-Tim Dettmers NVIDIA® DIGITS™ DevBox-NVIDIA What is a PCI-Express Lane?-superuser pcie-百度百科 PCIe传输速率和可用带宽（吞吐量）计算-CSDN ECC内存-百度百科 LRDIMMs, RDIMMs, and Supermicro’s Latest Twin-AnandTech 双通道内存有什么优点和缺点？-知乎 为什么只支持双通道内存的主板会配四个内存插槽？-知乎 内存系列一：快速读懂内存条标签-老狼 Picking a GPU for Deep Learning-Slav Ivanov Which GPU(s) to Get for Deep Learning: My Experience and Advice for Using GPUs in Deep Learning-Tim Dettmers 你知道电脑硬盘有几种颜色吗？有哪些用途及如何选用你知道吗？-sohu]]></content>
      <categories>
        <category>guide</category>
        <category>hardware</category>
      </categories>
      <tags>
        <tag>guide</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode 218 The Skyline Problem]]></title>
    <url>%2F2018%2F03%2F05%2F2018-03-05-leetcode_218%2F</url>
    <content type="text"><![CDATA[题目内容第一眼看到，“额，城市天际线（steam上一款游戏）？”，读了题目之后，明白它和游戏还真是有一点关系，那就是它真是城市的天际线。 所以这里的天际线的意思，其实是你在水平方向平视一个城市，它所展现出的轮廓，有点类似于工程制图里面的正视图。 那么建筑物在这道题中都以长方形来表示，与地面保持垂直，那么实际上，只需要给出房顶的坐标就可以确定一栋建筑。 题目要求就是求出这一条天际线，通过坐标点的形式来表示。 自己的解决思路本着能自己做就先不看别人的思路的原则，至少浪费了五个小时以上的时间，我一直都没有编写出来，今天把之前的代码删了，重新整理了一遍思路， 还是写了出来。 自己的思路就很直白，维护当前的天际线，把剩下的建筑一栋一栋加进来就好，那么就涉及到很多情况的判断，这种思路最容易出错的地方就在： 一条线一定要以左端为准，这样在出现两栋刚好贴在一起的建筑的情况下，才不会出错。 将两条线的各种重叠情况判断清楚，分情况一一处理。这里情况有好几种，很容易遗漏或者绕晕。 初始情况的考虑，以及最后一条线的处理。 由于这样的想法很好想，但是写起来十分痛苦，写完之后调了好几遍才过，搞不好就写乱了，然后就调不出来。 代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677class Solution &#123; private int begin = 0; public List&lt;int[]&gt; getSkyline(int[][] buildings) &#123; List&lt;int[]&gt; roofs = new ArrayList&lt;&gt;(); int[] floor = &#123;0, 0&#125;; roofs.add(floor); for ( int[] building : buildings ) &#123; helpAdd(roofs, building); &#125; for ( int i = 0; i &lt; roofs.size() - 1; i++ ) &#123; int[] roof = roofs.get(i); int[] next = roofs.get(i+1); if ( roof[1] == next[1] ) &#123; roofs.remove(i+1); i--; &#125; &#125; int[] first = roofs.get(0); if ( first[1] == 0 ) &#123; roofs.remove(0); &#125; return roofs; &#125; private void helpAdd(List&lt;int[]&gt; roofs, int[] bu) &#123; for ( int i = begin; i &lt; roofs.size() - 1; i++ ) &#123; int[] roof = roofs.get(i); int[] next = roofs.get(i+1); if ( roof[0] &gt;= bu[1] ) &#123; break; &#125; if ( roof[1] &gt;= bu[2] || next[0] &lt;= bu[0] ) &#123; continue; &#125; if ( roof[0] &lt; bu[0] ) &#123; if ( next[0] &lt;= bu[1] ) &#123; int[] nroof = &#123;bu[0], bu[2]&#125;; roofs.add(++i, nroof); begin = i; &#125; else &#123; int[] nroof = &#123;bu[0], bu[2]&#125;; int[] nroof2 = &#123;bu[1], roof[1]&#125;; roofs.add(i+1, nroof2); roofs.add(i+1, nroof); begin = i + 1; break; &#125; &#125; else &#123; if ( next[0] &lt;= bu[1] ) &#123; roof[1] = bu[2]; &#125; else &#123; int[] nroof = &#123;bu[1], roof[1]&#125;; roof[1] = bu[2]; roofs.add(i+1, nroof); break; &#125; &#125; &#125; int[] last = roofs.get(roofs.size()-1); if ( last[1] &lt; bu[2] &amp;&amp; last[0] &lt; bu[1] ) &#123; if ( last[0] &lt; bu[0] ) &#123; int[] nroof = &#123;bu[0], bu[2]&#125;; int[] nroof2 = &#123;bu[1], last[1]&#125;; roofs.add(nroof); roofs.add(nroof2); begin = roofs.size() - 2; &#125; else &#123; int[] nroof = &#123;bu[1], last[1]&#125;; last[1] = bu[2]; roofs.add(nroof); &#125; &#125; &#125;&#125; 算法复杂度：最坏情况$O(n^2)$，通常情况应该接近$O(n)$，主要是因为原本的建筑的左端起点已经排好序了，大量情况下只需要比较最后的几条线。 运行时间：4ms 击败：99.57% 另外，看了3ms的答案，别人的思路应该和我差不多，但是别人数据结构用了链表，这就是别人的优势了，也不知道为什么自己在写的时候没有想到。 广泛的解决思路看了一下比较广泛的解决思路，主要使用了优先队列，其实就是一个对高度的排序，然后将一个一个点确定下来。不得不说这个思路很巧妙， 想要一下子想到这个思路，并不容易。但是一旦理清了这个思路，写起来又会相对简单一点。 参考资料： leetCode上一哥们分享的解释，是真的强 YouTube上一个视频讲解，口音有点…]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>study</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法笔记]]></title>
    <url>%2F2018%2F02%2F03%2F2018-02-03-sorting-algorithm%2F</url>
    <content type="text"><![CDATA[关于这篇笔记最近做了几道比较难的leetCode题，其中一道题使用了基数排序（LeetCode-164题，另一种思路有桶排序的思想）， 突然发现自己好多东西都忘记了，或者说其实从来就没有真正的学会。所以这里决定重新回到算法导论的前面再看一看排序算法。 这篇笔记中包含算法冒泡排序、插入排序、希尔排序、归并排序、堆排序、快速排序、计数排序、基数排序、桶排序。 代码:github 代码的实现中均没有考虑负数，只考虑了非负数。 参考资料： 算法导论。 算法导论———ShellSort希尔排序 排序之希尔排序(shell sort) Data Structure Visualizations 冒泡排序冒泡排序是一种最为简单的排序算法，它的想法就是“比较所有相邻的数的大小，将小的放前面，大的放后面，重复这个过程”。 在算法执行过程中，可以看成，第一轮把最小的元素放到第一个位置，第二轮把第二小的元素放到第二个位置…重复N次。 在实现的时候每次可以检查是否发生交换，如何没有发生交换，就可以提前停止算法。 时间复杂度：$O(n^2)$ 空间复杂度：原址（也就是$O(1)$） 是否稳定：是 是否能处理负数：是 代码： 123456789101112131415161718192021222324252627public class BubbleSort &#123; public void sort(int[] nums, int begin, int end) &#123; if ( nums == null ) &#123; return; &#125; if ( nums.length &lt; 2 || begin &gt;= (end - 1)) &#123; return; &#125; begin = begin &lt; 0 ? 0 : begin; end = end &gt; nums.length ? nums.length : end; boolean change; for ( int i = begin; i &lt; end; i++ ) &#123; change = false; for ( int j = end - 1; j &gt; i; j-- ) &#123; if ( nums[j] &lt; nums[j-1] ) &#123; int tmp = nums[j-1]; nums[j-1] = nums[j]; nums[j] = tmp; change = true; &#125; &#125; if ( !change ) &#123; break; &#125; &#125; &#125;&#125; 插入排序插入排序也是一种基本的排序算法，它就类似于抓牌的过程，“当抓到一张新牌时，我们会把牌按大小顺序插入到它应该在的位置上”。 在感觉上插入排序和冒泡排序有一些类似，它们的复杂度也是一样，但是实际上插入排序要比冒泡排序快一点。 时间复杂度：$O(n^2)$ 空间复杂度：原址 是否稳定：是 是否能处理负数：是 代码： 123456789101112131415161718192021222324public class InsertionSort&#123; public void sort(int[] nums, int begin, int end) &#123; if ( nums == null ) &#123; return; &#125; if ( nums.length &lt; 2 || begin &gt;= (end - 1)) &#123; return; &#125; begin = begin &lt; 0 ? 0 : begin; end = end &gt; nums.length ? nums.length : end; for ( int i = begin + 1; i &lt; end; i++ ) &#123; for ( int j = i - 1; j &gt;= begin; j-- ) &#123; if ( nums[j] &lt;= nums[j+1] ) &#123; break; &#125; int tmp = nums[j]; nums[j] = nums[j+1]; nums[j+1] = tmp; &#125; &#125; &#125;&#125; 希尔排序希尔排序英文为Shell Sort，这里的‘Shell’实际上是一个人名。 希尔排序是对插入排序的一种改进，它的改进想法来自于“序列越基本有序，则插入排序效率越高”： 设想序列的最后一个数是最小的数，那么当对它进行插入时，将需要从最后的位置，一个一个向前挪到第一个， 所以这种情况如果越少发生，那么插入排序的效率也就越高。 这里“序列的基本有序”定义就是所有的数当前位置离排序后的位置越近，就越有序。也就是大的数就在序列的后面，小的数就在序列的前面。 有了上面的观察，那么就要想办法先将序列变得“有序”，然后再进行插入排序。 具体流程可以参考算法导论———ShellSort希尔排序，比较直观。 代码中gap选择A102549， 至于gap，可以参考wikiGap_sequences 时间复杂度：小于$O(n^{\frac{4}{3}})$ 空间复杂度：原址 是否稳定：否 是否能处理负数：是 代码： 12345678910111213141516171819202122232425262728293031public class ShellSort &#123; public void sort(int[] nums, int begin, int end) &#123; if ( nums == null ) &#123; return; &#125; if ( nums.length &lt; 2 || begin &gt;= (end - 1)) &#123; return; &#125; begin = begin &lt; 0 ? 0 : begin; end = end &gt; nums.length ? nums.length : end; int[] gaps = &#123;1, 4, 10, 23, 57, 132, 301, 701, 1750&#125;; int idx = gaps.length - 1; while ( idx &gt;= 0 ) &#123; int start = gaps[idx] + begin; for ( int i = start; i &lt; end; i++ ) &#123; for ( int j = i; j &gt;= start; j -= gaps[idx] ) &#123; int front = j - gaps[idx]; if ( nums[j] &gt;= nums[front] ) &#123; break; &#125; int tmp = nums[j]; nums[j] = nums[front]; nums[front] = tmp; &#125; &#125; idx--; &#125; &#125;&#125; 归并排序归并是一种递归排序的方式，它每一次递归将数据均分成左右两个部分，然后两个部分分别排序，这里分别排序也就是再次调用归并排序来进行。 当递归到元素只剩下一个的时候，那么它当然是有序的，于是就开始返回，从下往上进行合并。 两个已经排好序的序列合并很简单，新建一个数组，长度等于两个序列的总长， 对比两个序列的第一个元素，每次选最小的那一个放到新数组里就行了。 对于复杂度为$O(n^2)$的算法，相当于序列的每一个元素都要和序列的其它所有元素比一遍，也就是每个元素要比n-1次，一共n个元素， 所以复杂度就为$O(n^2)$。 对于归并排序来说，它每次进行二分，所以总的深度就为$log_2(n)$，在向上合并的过程中，比较一次就可以放下一个元素， 所以每一层最多需要比较n次，所以算法的复杂度就降到了$nlog(n)$。 时间复杂度：$nlog(n)$ 空间复杂度：$O(n)$ 是否稳定：是 是否能处理负数：是 代码： 123456789101112131415161718192021222324252627282930313233343536public class MergeSort &#123; public void sort(int[] nums, int begin, int end) &#123; if ( nums == null ) &#123; return; &#125; if ( nums.length &lt; 2 || begin &gt;= (end - 1)) &#123; return; &#125; begin = begin &lt; 0 ? 0 : begin; end = end &gt; nums.length ? nums.length : end; mergeSort(nums, begin, end); &#125; private void mergeSort(int[] nums, int begin, int end) &#123; if ( begin &gt;= (end - 1) ) &#123; return; &#125; int mid = (end + begin) / 2; mergeSort(nums, begin, mid); mergeSort(nums, mid, end); int[] a = new int[mid-begin], b = new int[end-mid]; System.arraycopy(nums, begin, a, 0, a.length); System.arraycopy(nums, mid, b, 0, b.length); int idx1 = 0, idx2 = 0; for ( int i = begin; i &lt; end; i++ ) &#123; if ( idx1 &gt;= a.length ) &#123; nums[i] = b[idx2++]; &#125; else if ( idx2 &gt;= b.length || a[idx1] &lt;= b[idx2] ) &#123; nums[i] = a[idx1++]; &#125; else &#123; nums[i] = b[idx2++]; &#125; &#125; &#125;&#125; 堆排序堆排序则是利用了最大堆的性质，建立一个最大堆所需的时间复杂度为$nlog(n)$，将最大堆的最大元素取出， 填上另一个数，再维护最大堆的性质，将这个数下沉到它应该的位置，复杂度为$log(n)$。 所以每次将最大堆的最大元素与堆尾（数组实现最大堆）的元素进行交换，堆大小减一，维护最大堆性质，反复即可。 时间复杂度：$nlog(n)$ 空间复杂度：原址 是否稳定：否 是否能处理负数：是 代码，实现中为了简化数组实现最大堆的过程，额外申请了空间保证从数组索引0开始： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public class HeapSort &#123; public void sort(int[] nums, int begin, int end) &#123; if ( nums == null ) &#123; return; &#125; if ( nums.length &lt; 2 || begin &gt;= (end - 1)) &#123; return; &#125; begin = begin &lt; 0 ? 0 : begin; end = end &gt; nums.length ? nums.length : end; int[] array = new int[end-begin]; System.arraycopy(nums, begin, array, 0, array.length); heapSort(array); System.arraycopy(array, 0, nums, begin, array.length); &#125; private void heapSort(int[] nums) &#123; buildMaxHeap(nums); for ( int i = nums.length-1; i &gt; 0; i-- ) &#123; int tmp = nums[0]; nums[0] = nums[i]; nums[i] = tmp; maxHeapify(nums, i, 0); &#125; &#125; private void buildMaxHeap(int[] heap) &#123; for ( int i = heap.length / 2 - 1; i &gt;= 0; i-- ) &#123; maxHeapify(heap, heap.length, i); &#125; &#125; private void maxHeapify(int[] heap, int len, int i) &#123; int l = left(i), r = right(i); int largest = i; if ( l &lt; len &amp;&amp; heap[l] &gt; heap[largest] ) &#123; largest = l; &#125; if ( r &lt; len &amp;&amp; heap[r] &gt; heap[largest] ) &#123; largest = r; &#125; if ( largest != i ) &#123; int tmp = heap[i]; heap[i] = heap[largest]; heap[largest] = tmp; maxHeapify(heap, len, largest); &#125; &#125; private int parent(int i) &#123; return (i + 1) / 2 - 1; &#125; private int left(int i) &#123; return (i + 1) * 2 - 1; &#125; private int right(int i) &#123; return (i + 1) * 2; &#125;&#125; 快速排序快速排序是程序员最常使用的排序算法了，它每次选择一个主元，将比主元小的元素放到左边，大的放到右边， 在左右划分中重复这个过程，直到划分中只剩下一个元素，也就完成了排序，它通常也是递归实现。 快速排序的最坏复杂度为$O(n^2)$，也就是每次都倒霉的选择了最大或者最小的那个元素，使得左右的划分十分的“不均匀”， 但是在期望情况下，它的复杂度为$nlog(n)$。具体的证明有一些繁琐，需要参考算法导论。 随机化版本：如果每次主元的选择都是固定位置的，那么很容易就能造出一个使复杂度变成$O(n^2)$的序列， 这一点可能会被不怀好意的人给利用，所以通常主元的选择会引入随机化，快速排序的复杂度能不被输入序列给影响。 时间复杂度：$nlog(n)$ 空间复杂度：原址 是否稳定：否 是否能处理负数：是 代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public class QuickSort &#123; private Random rand = new Random(); public void sort(int[] nums, int begin, int end) &#123; if ( nums == null ) &#123; return; &#125; if ( nums.length &lt; 2 || begin &gt;= (end - 1)) &#123; return; &#125; begin = begin &lt; 0 ? 0 : begin; end = end &gt; nums.length ? nums.length : end; quickSort(nums, begin, end-1); &#125; private void quickSort(int[] nums, int p, int q) &#123; if ( p &gt;= q ) &#123; return; &#125; int m = partition(nums, p, q); if ( m == -1 ) &#123; return; &#125; quickSort(nums, p, m - 1); quickSort(nums, m+1, q); &#125; private int partition(int[] nums, int p, int q) &#123; randomExchange(nums, p, q); int axle = nums[q]; int j = p - 1; int equals_count = 1; for (int i = p; i &lt; q; i++) &#123; if (nums[i] &lt; axle) &#123; j++; int tmp = nums[j]; nums[j] = nums[i]; nums[i] = tmp; &#125; else if ( nums[i] == axle ) &#123; j++; int tmp = nums[j]; nums[j] = nums[i]; nums[i] = tmp; equals_count++; &#125; &#125; if ( equals_count == nums.length ) &#123; // 防止所有元素都相等时还进行递归 return -1; &#125; j++; int tmp = nums[j]; nums[j] = nums[q]; nums[q] = tmp; return j; &#125; private void randomExchange(int[] nums, int p, int q) &#123; int exchange = rand.nextInt(q-p+1) + p; int tmp = nums[exchange]; nums[exchange] = nums[q]; nums[q] = tmp; &#125;&#125; 计数排序假设输入的数都是非负数，它们都小于某一个数N，那么我们就可以额外申请一个长度为N的数组对输入序列进行统计， 这个数组中第i个位置就表示序列中大小为i的数的个数。于是我们就可以使用这个计数的信息， 将输入序列中数放到它应该在的位置上。 计数排序有一个很大的假设，就是需要提前知道数的范围，在数的范围已知并且范围不大的时候，计数排序的时间效率为$O(n)$。 但是通常情况下这两个假设都难以满足，所以计数排序用得很少。 时间复杂度：$O(n)$ 空间复杂度：$O(n) + O(N)$ 是否稳定：是 是否能处理负数：否 代码： 123456789101112131415161718192021public class CountingSort &#123; public void sort(int[] nums, int k) &#123; if (nums == null || nums.length &lt; 2 ) &#123; return; &#125; int[] count = new int[k]; for ( int i = 0; i &lt; nums.length; i++ ) &#123; count[nums[i]]++; &#125; for ( int j = 1; j &lt; k; j++ ) &#123; count[j] += count[j-1]; &#125; int[] copy = new int[nums.length]; System.arraycopy(nums, 0, copy, 0, copy.length); for ( int i = copy.length-1; i &gt;= 0; i-- ) &#123; nums[--count[copy[i]]] = copy[i]; &#125; &#125;&#125; 基数排序基数排序的基础是计数排序，由于输入是一堆十进制数，那么它的每一位就是一个十进制数，也就是范围在0~9， 所以就可能按照计数排序的思想，对输入序列的某一位数进行排序。神奇的是，这样从个位往高位排一遍， 序列就完成了排序。 基数排序同样也只能处理非负数，但是它的复杂度只为$O(n)$，虽然其中的隐藏因子有一点大，当序列很长时， 基数排序会有很大的优势。 时间复杂度：$O(n)$ 空间复杂度：$O(n)$ 是否稳定：是 是否能处理负数：否 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class RadixSort &#123; public void sort(int[] nums, int begin, int end) &#123; if ( nums == null ) &#123; return; &#125; if ( nums.length &lt; 2 || begin &gt;= (end - 1)) &#123; return; &#125; begin = begin &lt; 0 ? 0 : begin; end = end &gt; nums.length ? nums.length : end; radixSort(nums, begin, end); &#125; private void radixSort(int[] nums, int begin, int end) &#123; int max = -1; for ( int i = begin; i &lt; end; i++ ) &#123; if ( nums[i] &gt; max ) &#123; max = nums[i]; &#125; &#125; int max_exp = 1; while ( (max / max_exp) &gt;= 10 ) &#123; max_exp *= 10; &#125; int[] count = new int[10]; int[] a = new int[end-begin], b = new int[a.length], radixs = new int[a.length], exchange; System.arraycopy(nums, begin, a, 0, a.length); int exp = 1; while ( exp &lt;= max_exp ) &#123; Arrays.fill(count,0); for ( int i = 0; i &lt; a.length; i++) &#123; radixs[i] = (a[i] / exp) % 10; count[radixs[i]]++; &#125; for ( int i = 1; i &lt; 10; i++ ) &#123; count[i] += count[i-1]; &#125; for ( int i = a.length-1; i &gt;= 0; i-- ) &#123; b[--count[radixs[i]]] = a[i]; &#125; exchange = b; b = a; a = exchange; exp *= 10; &#125; System.arraycopy(a, 0, nums, begin, a.length); &#125; &#125; 桶排序假设输入数据的范围是N，桶排序将N划分为m个范围，也就是m个桶，将输入序列的数一个一个丢到桶里， 然后进行桶内排序，再把所有的数据串起来，就完成了排序。 桶排序需要知道输入数据的范围，另外一个十分重要的假设是输入序列均匀分布，这样才能保证每个桶里面的元素不会太多。 个人认为计数排序就是一个特殊的桶排序，也就是桶的大小为1的时候的桶排序。 桶排序中的每一个桶其实是一个链表，它的空间消耗应该是上面所有算法中最多的。 时间复杂度：$O(n)$ 空间复杂度：$O(n)$ 是否稳定：是 是否能处理负数：否 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class BucketSort &#123; private class Bucket &#123; int num = -1; Bucket front, next; &#125; public void sort(int[] nums, int bound) &#123; if (nums == null || nums.length &lt; 2 ) &#123; return; &#125; int bucket_num = Math.min(nums.length, bound); Bucket[] buckets = new Bucket[bucket_num]; for ( int i = 0; i &lt; bucket_num; i++ ) &#123; buckets[i] = new Bucket(); &#125; for ( int num : nums ) &#123; insertHelper(buckets[(num*bucket_num) / bound], num); &#125; int idx = 0; for ( Bucket bucket : buckets ) &#123; while ( bucket.next != null ) &#123; bucket = bucket.next; nums[idx++] = bucket.num; &#125; &#125; &#125; private void insertHelper(Bucket bucket, int num) &#123; Bucket in = new Bucket(); in.num = num; while ( bucket.next != null &amp;&amp; bucket.next.num &lt; num ) &#123; bucket = bucket.next; &#125; if ( bucket.next == null ) &#123; bucket.next = in; in.front = bucket; &#125; else &#123; in.next = bucket.next; in.front = bucket; bucket.next.front = in; bucket.next = in; &#125; &#125;&#125; 代码时间效率对比其中基于比较的排序算法：冒泡排序、插入排序、希尔排序、归并排序、堆排序、快速排序。 其它排序算法：计数排序、基数排序、桶排序。 由算法导论可知，基于比较的排序算法的最坏时间复杂度一定是$nlog(n)$，而另外三个算法则没有这个限制，它们的复杂度都在$O(n)$。 时间程序效率对比，可能会受到个人代码编写的影响带来一些偏差，但是大概没有问题： 其中T表示序列个数，N表示序列的最大长度（在N一下进行随机），K表示取值上界（下界默认为大于等于0）。 算法\取值 T=100000, N=20, K=100 T=100000, N=50, K=100 T=100000, N=50, K=200000 T=100000, N=100, K=200000 T=10000, N=1000, K=200000 T=1000, N=20000, K=20000 T=5000, N=10000, K=200000 冒泡排序 26ms 128ms 133ms 457ms 2590ms 119063ms 138309ms 插入排序 22ms 51ms 48ms 124ms 801ms 29212ms 38779ms 希尔排序 22ms 69ms 67ms 134ms 239ms 709ms 1694ms 归并排序 64ms 167ms 161ms 305ms 375ms 892ms 2180ms 堆排序 25ms 90ms 84ms 155ms 242ms 666ms 1605ms 快速排序 38ms 93ms 87ms 145ms 193ms 519ms 1201ms 计数排序 32ms 30ms 10828ms 10633ms 1301ms 79ms 760ms 基数排序 35ms 46ms 99ms 172ms 146ms 237ms 689ms 桶排序 44ms 68ms 66ms 129ms 125ms 280ms 688ms 值得关注的点： 计数排序再取值范围与序列长度差距不大的情况下速度爆炸快。 基数排序平均来看是最快的，当然不能有非负数。 快速排序的稳定性是最好的。 插入排序在序列较短的情况下效率非常高。 快速排序作为最火的排序算法不是没有道理的，它有几点优势：速度快且稳定，$O(1)$的额外空间，能够处理负数。 特殊情况下，计数排序会十分有用。]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>study</tag>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Keras 自定义 generator]]></title>
    <url>%2F2017%2F12%2F29%2F2017-12-29-keras-generator%2F</url>
    <content type="text"><![CDATA[问题引入在做kaggle上的Facial Keypoints Detection时候，它输入是一些人脸的图片， 标签是类似眼角、鼻尖、嘴角等在图片上的坐标。 整个数据集的图片数量并不多，7000+，而且中间数据集其实可以分为两个部分， 一部分是质量较好的数据集，图片数2000+，剩下的就是另外一部分质量较差的。 实际上在训练的时候，只用了质量较好的这一部分，那么也就2000+的图片， 对于图片类的任务来说，样本太少了。 那么这里显然使用Data Augmentation是比较明智的选择，也就是将图片进行一下平移、镜像、对比度变化、旋转等操作，反正这些操作并不会影响到图片本身的性质，但是在神经网络看来又不会是重复样本。 所以，这里就需要使用generator，也就是流式的数据生成，因为不可能先把图片进行变换，保存下来再进行训练，因为一张图片可以变换成为无数张图片，所以需要一边使用原始图片来生成新图片，一边进行训练。 在Keras中，model.fit_generator()就是用来进行这种类型的训练的，它需要传入一个生成器，也就是python中的生成器。 注意到，在Keras中，提供了ImageDataGenerator这么一个类可以来进行图片的变换，其中有很多的功能。 但是，但是，但是，这里的任务不是分类，是回归，其中标签是眼角、鼻尖、嘴角等在图片上的坐标， 那么在图片进行变换的时候，当然这里坐标也需要跟着变，那么这里就得自己来写generator。 generator是什么？我现在只有简单的理解，它就是一个python的生成器，每一次返回一个batch的样本以及标签。 1234567891011def my_generator(X, Y, batch_size=32, gray_change_range=30, cut_out_size=88): indexs = list(range(X.shape[0])) while True: np.random.shuffle(indexs) for i in range(0, len(indexs), batch_size): ge_batch_x = np.empty((batch_size, cut_out_size, cut_out_size)) ge_batch_y = np.empty((batch_size, Y.shape[1])) ... yield ge_batch_x, ge_batch_y 生成器方法就类似于上面的代码，其中yield是关键的地方，程序每次运行到它的时候， 就会将它后面的数据返回，下一次调用又接着向后运行。 另外，这里写成了一个死循环while True，因为model.fit_generator()在使用在个函数的时候， 并不会在每一个epoch之后重新调用，那么如果这时候generator自己结束了就会有问题。 Keras使用generator进行训练这里使用model.fit_generator()来进行训练即可： 123fit_generator(self, generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0) 函数声明中有很多可以选择的参数，具体的说明可以去查看官方的文档，里面介绍了每一个参数的说明， 通常我们只需要定义几个参数就够了。 1model.fit_generator(my_generator(train_x, train_y), steps_per_epoch=train_x.shape[0] / 32, epochs=10) 上面的调用，参数中传入了生成器，多少个batch为一个epoch，训练多少个epoch。 关于参数设置多个workers的问题关于workers的文档说明： workers: Integer. Maximum number of processes to spin up when using process based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. 我的理解，它是与数据生成有关的，应该是使用多个线程来生成数据。 由于有时候生成数据的过程可能较慢，它可能会拖慢整个训练的速度，所以需要使用多个线程来同时生成， 使得生成数据不会是训练时间的瓶颈。 但是，当我设置workers=2时，程序报错，提示generator already executing， google之后，大概确定了这个是没有线程安全而导致的问题。 我没有实际去解决，因为其实一个线程已经够用了。如果下次需要开启多个线程，解决方法可以参考，Proper way of making a data generator which can handle multiple workers #1638]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>study</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Keras在多GPU时指定单GPU执行以及显存控制，以及任务挂起的命令。]]></title>
    <url>%2F2017%2F12%2F29%2F2017-12-29-keras-gpu-control%2F</url>
    <content type="text"><![CDATA[问题描述实验室有一台服务器，其中有两块GPU，现在有几个人可能会去同时使用它，如果不进行GPU选择的话， 就一次只能执行一个任务，下一个任务去执行的时候就会提示错误，也就是资源不足。 所以这里需要指定任务使用哪一个GPU。 另外，如果不进行显存控制，只要任务跑起来，不管有多少个GPU，不管现在任务实际需要多少的显存， 不管现在任务实际是在哪一块GPU上面跑，所有的GPU的显存都会被占满，所以需要进行一下控制， 以便多人同时一起使用。 注意：这里针对的Backend是TensorFlow。 解决方法在代码的头部加入一下代码即可： 123456import tensorflow as tffrom keras.backend.tensorflow_backend import set_sessionconfig = tf.ConfigProto()config.gpu_options.per_process_gpu_memory_fraction = 0.5config.gpu_options.visible_device_list = "1"set_session(tf.Session(config=config)) 这里的visible_device_list就指定了可见的GPU，也就是所使用的GPU。这里指定了1号GPU， 前面还有一张0号GPU。 这里的per_process_gpu_memory_fraction就指定了使用的GPU内存比例，这里设置了0.5。 运行任务时效果使用nvidia-smi命令查看： 12345678910111213141516171819+-----------------------------------------------------------------------------+| NVIDIA-SMI 375.26 Driver Version: 375.26 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. ||===============================+======================+======================|| 0 Tesla K40m On | 0000:04:00.0 Off | 0 || N/A 28C P8 20W / 235W | 2MiB / 11439MiB | 0% Default |+-------------------------------+----------------------+----------------------+| 1 Tesla K40m On | 0000:82:00.0 Off | 0 || N/A 50C P0 129W / 235W | 5860MiB / 11439MiB | 88% Default |+-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+| Processes: GPU Memory || GPU PID Type Process name Usage ||=============================================================================|| 1 19055 C python 5858MiB |+-----------------------------------------------------------------------------+ 可以看到只使用了1号GPU，同时只用了它的50%的显存。 一些小坑1. 关于Keras版本2.0.9对于这个版本，上面的代码并起不到效果，这是这个版本的BUG，在后续版本中就没有问题了。 我这直接升级了Keras到版本2.1.2，上面的代码就起了效果。 任务挂起因为一般都是在linux环境下面跑，ssh建立起一个连接之后，在这个连接下面启动任务。 但是有时任务一下子执行不完，所以当然想把任务挂起来，即使这个ssh连接关闭，也不影响任务的执行。 很简单，使用nohup命令即可，它的使用方法为： 123$ nohup 命令 &gt; logs.file 2&gt;&amp;1 &amp;[1] 18864 这里的命令就是要挂起执行的命令，在跑python脚本的时候一般就是python xxx.py。 另外其中的logs.file就是中间打印文件进行重定向的文件，使用它来存下中间的打印结果。 执行命名完毕之后，就会有一个[1] 18864，表示这个进程的进程id。]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>study</tag>
        <tag>deep learning</tag>
        <tag>Keras</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTML颜色表]]></title>
    <url>%2F2017%2F12%2F08%2F2017-12-08-html-color%2F</url>
    <content type="text"><![CDATA[来源常用HTML颜色表-HTML颜色代码色彩表 &nbsp; #000000 &nbsp; #2F0000 &nbsp; #600030 &nbsp; #460046 &nbsp; #28004D &nbsp; #272727 &nbsp; #4D0000 &nbsp; #820041 &nbsp; #5E005E &nbsp; #3A006F &nbsp; #3C3C3C &nbsp; #600000 &nbsp; #9F0050 &nbsp; #750075 &nbsp; #4B0091 &nbsp; #4F4F4F &nbsp; #750000 &nbsp; #BF0060 &nbsp; #930093 &nbsp; #5B00AE &nbsp; #5B5B5B &nbsp; #930000 &nbsp; #D9006C &nbsp; #AE00AE &nbsp; #6F00D2 &nbsp; #6C6C6C &nbsp; #AE0000 &nbsp; #F00078 &nbsp; #D200D2 &nbsp; #8600FF &nbsp; #7B7B7B &nbsp; #CE0000 &nbsp; #FF0080 &nbsp; #E800E8 &nbsp; #921AFF &nbsp; #8E8E8E &nbsp; #EA0000 &nbsp; #FF359A &nbsp; #FF00FF &nbsp; #9F35FF &nbsp; #9D9D9D &nbsp; #FF0000 &nbsp; #FF60AF &nbsp; #FF44FF &nbsp; #B15BFF &nbsp; #ADADAD &nbsp; #FF2D2D &nbsp; #FF79BC &nbsp; #FF77FF &nbsp; #BE77FF &nbsp; #BEBEBE &nbsp; #FF5151 &nbsp; #FF95CA &nbsp; #FF8EFF &nbsp; #CA8EFF &nbsp; #d0d0d0 &nbsp; #ff7575 &nbsp; #ffaad5 &nbsp; #ffa6ff &nbsp; #d3a4ff &nbsp; #E0E0E0 &nbsp; #FF9797 &nbsp; #FFC1E0 &nbsp; #FFBFFF &nbsp; #DCB5FF &nbsp; #F0F0F0 &nbsp; #FFB5B5 &nbsp; #FFD9EC &nbsp; #FFD0FF &nbsp; #E6CAFF &nbsp; #FCFCFC &nbsp; #FFD2D2 &nbsp; #FFECF5 &nbsp; #FFE6FF &nbsp; #F1E1FF &nbsp; #FFFFFF &nbsp; #FFECEC &nbsp; #FFF7FB &nbsp; #FFF7FF &nbsp; #FAF4FF &nbsp; #000079 &nbsp; #000079 &nbsp; #003E3E &nbsp; #006030 &nbsp; #006000 &nbsp; #000093 &nbsp; #003D79 &nbsp; #005757 &nbsp; #01814A &nbsp; #007500 &nbsp; #0000C6 &nbsp; #004B97 &nbsp; #007979 &nbsp; #019858 &nbsp; #009100 &nbsp; #0000C6 &nbsp; #005AB5 &nbsp; #009393 &nbsp; #01B468 &nbsp; #00A600 &nbsp; #0000E3 &nbsp; #0066CC &nbsp; #00AEAE &nbsp; #02C874 &nbsp; #00BB00 &nbsp; #2828FF &nbsp; #0072E3 &nbsp; #00CACA &nbsp; #02DF82 &nbsp; #00DB00 &nbsp; #4A4AFF &nbsp; #0080FF &nbsp; #00E3E3 &nbsp; #02F78E &nbsp; #00EC00 &nbsp; #6A6AFF &nbsp; #2894FF &nbsp; #00FFFF &nbsp; #1AFD9C &nbsp; #28FF28 &nbsp; #7D7DFF &nbsp; #46A3FF &nbsp; #4DFFFF &nbsp; #4EFEB3 &nbsp; #53FF53 &nbsp; #9393FF &nbsp; #66B3FF &nbsp; #80FFFF &nbsp; #7AFEC6 &nbsp; #79FF79 &nbsp; #AAAAFF &nbsp; #84C1FF &nbsp; #A6FFFF &nbsp; #96FED1 &nbsp; #93FF93 &nbsp; #B9B9FF &nbsp; #97CBFF &nbsp; #BBFFFF &nbsp; #ADFEDC &nbsp; #A6FFA6 &nbsp; #CECEFF &nbsp; #ACD6FF &nbsp; #CAFFFF &nbsp; #C1FFE4 &nbsp; #BBFFBB &nbsp; #DDDDFF &nbsp; #C4E1FF &nbsp; #D9FFFF &nbsp; #D7FFEE &nbsp; #CEFFCE &nbsp; #ECECFF &nbsp; #D2E9FF &nbsp; #ECFFFF &nbsp; #E8FFF5 &nbsp; #DFFFDF &nbsp; #FBFBFF &nbsp; #ECF5FF &nbsp; #FDFFFF &nbsp; #FBFFFD &nbsp; #F0FFF0 &nbsp; #467500 &nbsp; #424200 &nbsp; #5B4B00 &nbsp; #844200 &nbsp; #642100 &nbsp; #548C00 &nbsp; #5B5B00 &nbsp; #796400 &nbsp; #9F5000 &nbsp; #842B00 &nbsp; #64A600 &nbsp; #737300 &nbsp; #977C00 &nbsp; #BB5E00 &nbsp; #A23400 &nbsp; #73BF00 &nbsp; #8C8C00 &nbsp; #AE8F00 &nbsp; #D26900 &nbsp; #BB3D00 &nbsp; #82D900 &nbsp; #A6A600 &nbsp; #C6A300 &nbsp; #EA7500 &nbsp; #D94600 &nbsp; #8CEA00 &nbsp; #C4C400 &nbsp; #D9B300 &nbsp; #FF8000 &nbsp; #F75000 &nbsp; #9AFF02 &nbsp; #E1E100 &nbsp; #EAC100 &nbsp; #FF9224 &nbsp; #FF5809 &nbsp; #A8FF24 &nbsp; #F9F900 &nbsp; #FFD306 &nbsp; #FFA042 &nbsp; #FF8040 &nbsp; #B7FF4A &nbsp; #FFFF37 &nbsp; #FFDC35 &nbsp; #FFAF60 &nbsp; #FF8F59 &nbsp; #C2FF68 &nbsp; #FFFF6F &nbsp; #FFE153 &nbsp; #FFBB77 &nbsp; #FF9D6F &nbsp; #CCFF80 &nbsp; #FFFF93 &nbsp; #FFE66F &nbsp; #FFC78E &nbsp; #FFAD86 &nbsp; #D3FF93 &nbsp; #FFFFAA &nbsp; #FFED97 &nbsp; #FFD1A4 &nbsp; #FFBD9D &nbsp; #DEFFAC &nbsp; #FFFFB9 &nbsp; #FFF0AC &nbsp; #FFDCB9 &nbsp; #FFCBB3 &nbsp; #E8FFC4 &nbsp; #FFFFCE &nbsp; #FFF4C1 &nbsp; #FFE4CA &nbsp; #FFDAC8 &nbsp; #EFFFD7 &nbsp; #FFFFDF &nbsp; #FFF8D7 &nbsp; #FFEEDD &nbsp; #FFE6D9 &nbsp; #F5FFE8 &nbsp; #FFFFF4 &nbsp; #FFFCEC &nbsp; #FFFAF4 &nbsp; #FFF3EE &nbsp; #613030 &nbsp; #616130 &nbsp; #336666 &nbsp; #484891 &nbsp; #6C3365 &nbsp; #743A3A &nbsp; #707038 &nbsp; #3D7878 &nbsp; #5151A2 &nbsp; #7E3D76 &nbsp; #804040 &nbsp; #808040 &nbsp; #408080 &nbsp; #5A5AAD &nbsp; #8F4586 &nbsp; #984B4B &nbsp; #949449 &nbsp; #4F9D9D &nbsp; #7373B9 &nbsp; #9F4D95 &nbsp; #AD5A5A &nbsp; #A5A552 &nbsp; #5CADAD &nbsp; #8080C0 &nbsp; #AE57A4 &nbsp; #B87070 &nbsp; #AFAF61 &nbsp; #6FB7B7 &nbsp; #9999CC &nbsp; #B766AD &nbsp; #C48888 &nbsp; #B9B973 &nbsp; #81C0C0 &nbsp; #A6A6D2 &nbsp; #C07AB8 &nbsp; #CF9E9E &nbsp; #C2C287 &nbsp; #95CACA &nbsp; #B8B8DC &nbsp; #CA8EC2 &nbsp; #D9B3B3 &nbsp; #CDCD9A &nbsp; #A3D1D1 &nbsp; #C7C7E2 &nbsp; #D2A2CC &nbsp; #E1C4C4 &nbsp; #D6D6AD &nbsp; #B3D9D9 &nbsp; #D8D8EB &nbsp; #DAB1D5 &nbsp; #EBD6D6 &nbsp; #DEDEBE &nbsp; #C4E1E1 &nbsp; #E6E6F2 &nbsp; #E2C2DE &nbsp; #F2E6E6 &nbsp; #E8E8D0 &nbsp; #D1E9E9 &nbsp; #F3F3FA &nbsp; #EBD3E8]]></content>
      <categories>
        <category>html</category>
      </categories>
      <tags>
        <tag>html</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Residual Net and Inception Net]]></title>
    <url>%2F2017%2F11%2F29%2F2017-11-29-net-structures%2F</url>
    <content type="text"><![CDATA[Residual Net参考： coursera-convolutional-neural-networks-resnets 论文: He, Kaiming, et al. “Deep residual learning for image recognition.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2016. 网络结构 VGG-19:2014年ImageNet亚军，top-5错误率7.3%，19层神经网络。（那一年的冠军是InceptionNet，top-5错误率6.7%，22层神经网络） plain:为了与ResNet区分，给中间的网络取名为“平坦”。 residual:残差网络。为2015年ImageNet冠军，top-5错误率3.57%，152层神经网络。 为什么使用残差网络？原因，神经网络现在越来越深，但是简单的堆砌，使得网络深度增加，并不一定能使网络的精度增加。 图上56层的神经网络的精度反而比20层的神经网络精度低。 造成这个问题的原因有： 梯度消失、梯度爆炸。虽然这个问题很大程度上被normalization解决了，但是可能还是会带来一些影响。 随着网络深度的增加，网络精度达到饱和，这时再增加网络层数，会使得网络精度迅速下降。这不是由于过拟合而造成的，可能是由于更深的网络更加难训练造成的。 为此，引入残差网络的结构： 将前几层的输出加到后几层的输出之上，上面的X是经过激活函数之后的输出，F(X)还未经过激活函数， F(X)加上X之后再共同的经过激活函数。 这个结构的直接好处就是，如果网络精度已经饱和，中间新加入的层没有价值，残差网络将十分容易的将新加入层置为0（也就是F(X)=0，F(X)+X=X），这就不会影响到网络的精度。 上面左图中34层的plain网络反而比18层的网络训练精度要低，而对于右图中的ResNet不会存在这种情况， 对残差网络的另一种理解文献：Veit, Andreas, Michael Wilber, and Serge Belongie. “Residual networks are exponential ensembles of relatively shallow networks.” arXiv preprint arXiv:1605.06431 1 (2016). 通过下图可以表现出残差网络的另一种形式： 把左图变形一下就变成右图，那么这里注意到残差网络其实就相当于是多个网络的叠加，结果就相当于是这些网络一起投票投出来的，有一种Ensembling的意思在里面。 实现上的一些技巧在实际的残差网络实现中，有一些技巧： 限制，进入block的输入与出去block的输出维度要一致，包括长、宽以及深度。那么图中的卷积使用的都是same，也就是保持长宽不变的卷积。 技巧，假设这里输入长宽是64 x 64，那么对于右图来说，如果没有前后的1 x 1 x 64、1 x 1 x 256的两个卷积，输入直接进行3 x 3 x 256的卷积: Calculation = 64 \times 64 \times 3 \times 3 \times 256 \times 256 = 24'1591'9104加上1 x 1 x 64、1 x 1 x 256两个卷积层： \begin{equation}\begin{split} Calculation &= 64 \times 64 \times 1 \times 1 \times 256 \times 64 + 64 \times 64 \times 3 \times 3 \times 64 \times 64 + 64 \times 64 \times 1 \times 1 \times 64 \times 256 \\ &= 67108864 + 150994944 + 67108864 \\ &= 2'8521'2672 \end{split}\end{equation}通过这样的方法，计算量可以减少一个数量级，其中1 x 1 x 64的卷积把它称为bottleneck，它在这就类似于一个瓶颈。在实际中表明，这样的方法并不会对神经网络的精度造成损害，所以通常使用这样的方式来构建block，降低计算量。 Incepution Net2014年ImageNet冠军，top-5错误率6.7%，22层神经网络。那么实际上Incepution Net比residual net要早。 论文：Going Deeper with Convolutions,2014 Rethinking the Inception Architecture for Computer Vision,2015 网络结构在进行一次卷积操作时，不知道该选择1 x 1还是3 x 3还是5 x 5，怎么办？答：那么就在一层里面把它们全部都用上。 在同一层里面使用1 x 1、3 x 3、5 x 5以及max pooling，它们都使用same的卷积方式， 最后将它们的结果叠在一起就可以了。 同样的，这里也可以使用降维的思想（它的思想在ResNet之前）： 中间加入了几个1 x 1卷积，用于降低总的计算量。 比赛中使用的网络叫做GoogLeNet，整个网络的结构如下： 注意到中间有两个额外的输出，这是为了避免梯度消失，网络额外增加了2个辅助的softmax用于向前传导梯度。（博客上面看的，具体文章中还没看） 升级版本在Rethinking the Inception Architecture for Computer Vision,2015这篇文章中有新的思路，目前还没有看。 其中大概多了下面的操作：]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>study</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sibyl 论文笔记]]></title>
    <url>%2F2017%2F11%2F28%2F2017-11-28-Sibyl%2F</url>
    <content type="text"><![CDATA[Sibyl: A Practical Internet Route Oracle论文Calder, Matt, Yi-Ching Chiu, and Brandon Schlinker. “Sibyl: A Practical Internet Route Oracle.” 网络路由（Internet Route），描述两点之间的路由路径，经常用于解决网络问题或者是用于描述网络。 目前的形式现在获取网络路由的还是一些老的工具，例如路由跟踪（traceroute）、BGP route collectors以及Looking Glasses等。 有很多公开的测量平台可以使用，它们提供遍布整个世界的vantage points来发起traceroute， 例如RIPE Atlas、PlanetLab、Traceroute servers、DIMES、Dasu。 可以通过这些平台来发起traceroute，但是这样的查询方式存在很多的限制： 没有将历史的查询与实时的查询相结合。 平台与平台之间是独立的。 只能接受一个简单的question：“What is the path from here to there?” 因为是公开的平台，它们有资源限制。 这导致目前的路由跟踪所能获得的信息十分有限，通常只能回答一些简单的问题。 Sibyl论文提出一个网络路由查询系统，Sibyl[‘sɪbɪl]。 Sibyl提供正则表达式形式的查询，查询形式十分丰富，例如“在亚特兰大地区，哪些路由穿过Level3但是又不经过AT&amp;T？”。 为了满足查询，它的后端将从多种vantage points中发起traceroute，也就是使用多个测量平台来加快收敛。 问题：测量平台存在资源限制。由于一些查询需要发起很多的traceroute，它们都可能可以满足查询， 但是由于资源限制，不可能做到。 为了解决这个问题，需要下面三个步骤： 根据查询的结构缩小需要发出的traceroute，使得问题专注到一小部分traceroute。（正则表达式的结构） 结合历史数据来预测哪些未发出的traceroute更可能匹配到查询。（iPlane、RuleFit） 根据预测来最优化测量的资源分配。（贪心算法） Sibyl 结构 UI界面。 一批Query。 预测与Query相匹配的路径。 最优化资源的利用。 结合多个平台执行测量。 测量结果与Query比较，返回满足的测量结果。 查看当前Model的数据是不是过时，如果过时，就用本次的测量结果去更新它。 Query与正则表达Query的两种类型，一种是查询一条路径，例如查询从特定的源到目的的路径； 另一种是查询一个路径的集合，例如查询经过某个特定AS link的所有路径，为了知道哪些源目对使用了这条路径。 它们均使用正则表达式来表达，下面是文中给出的一些例子： Reverse traceroute：查询从r到s的路径： r-.*-s$Detecting prefix hijacks with iSpy：查询所有能够到达p的AS： \wedge \{.*\}-p$ \ \ by \ 其中的$\wedge$表示反方向也需要查询。 预测部分首先使用iPlane来预测一条未经测量的路径是否有可能匹配查询。 然后RuleFit来对预测出来的路径分配置信度。 iPlaneiPlane使用路径拼接的方法去预测一条没有测量过的路径。 例如预测从s到d的路径，iPlane将从s出发到某个目的的路径， 与从某个源出发目的为d的两条路径进行拼接，这两条路径需要存在相交的点。 原始的iPlane存在的问题： 对一个点对s到d之间会选出一条最好的预测路径，但这条路径可能是错的。 对于预测出的路径没有置信度，这不利于测量资源的分配。 解决思路： 考虑s到d之间所有预测出的路径。 给予这些路径赋予置信度。 RuleFitRuleFit是一种监督学习的方法，这里使用它来给拼接的路径分配权重。 训练大概流程：先取出路径的很多属性，使用RuleFit可以计算出各个属性的重要程度(内部使用decision trees或者lasso constraints)， The score for a spliced path is the sum of rule values for rules that match the spliced path’s features if the spliced path’s AS-path length is among the shortest, then increase the confidence (score) that it is very similar to &gt; the actual path 使用拼接路径的一些属性，RuleFit可以给出与真实路径之间相似程度分数。 最优化资源的利用 直接使用贪心的方法去近似的解决这个问题，每次选择使得期望utility最大的traceroute。 结合多个平台结合到多个平台可以提高系统的覆盖范围，每个平台都提供了很多vantage points，它们之间存在不重合的部分， 将它们结合到一起可以得到Path diversity的提升。 论文附录例子假如有三条已存在的traceroute: 为了简单这里IP的前八位就代表它是哪个AS。 查询问题，进过AS2和AS9的所有路径： \wedge .*AS2.*AS9.*$.首先将问题转化为FSA（论文没有找到FSA的具体说明）： 然后建立起前向和后向的两张表： 对左边的前向表进行一下解释（右表同理）： 表的第一行$S_1 \xrightarrow[]{.*} S_1$，表示路径的起点，所以这里所有的AS都可以作为起点。 表的第二行$S_1 \xrightarrow[]{AS2} S_2$，表示从起点开始，经过AS2，所以只有第一行中的Trace 1的AS2满足。 表的第三行$S_2 \xrightarrow[]{.*} S_2$，表示S2可以向后扩展（也就是上一行中的AS2），这里就可以扩展到3,4,5。 第四行向后没有满足的路径。 将前向后向路径拼接到一起，这里就可以从S2的位置拼接，前向路径1-&gt;2-&gt;3，后向路径13-&gt;9-&gt;3， 于是就得到路径1-&gt;2-&gt;3-&gt;9-&gt;13。 Likelihood estimation： 假如拼接得到以下的路径： 对于1.0.0.1到13.0.0.1拼接得到两条路径，其中路径A满足查询，路径B不满足，进行归一化0.41 = 0.7x0.7/(0.7 + 0.5)， 0.29 = 0.7x0.5/(0.7 + 0.5)，由于路径B不满足查询，所以最后1.0.0.1到13.0.0.1的likelihood为0.41。 对于15.0.0.1到16.0.0.1拼接得到两条路径，其中路径C、D都满足查询，进行归一化0.3 = 0.6x0.6/(0.6 + 0.6)， 0.3 = 0.6x0.6/(0.6 + 0.6)，所以最后15.0.0.1到16.0.0.1的likelihood为0.3 + 0.3 = 0.6。 所以这里路径15.0.0.1到16.0.0.1满足查询的概率大于路径1.0.0.1到13.0.0.1。]]></content>
      <categories>
        <category>paper</category>
        <category>network</category>
      </categories>
      <tags>
        <tag>study</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习第二章《线性代数》]]></title>
    <url>%2F2017%2F11%2F23%2F2017-11-23-linear-algebra%2F</url>
    <content type="text"><![CDATA[标量、向量、矩阵和张量]]></content>
      <categories>
        <category>deep learning</category>
        <category>math</category>
      </categories>
      <tags>
        <tag>study</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode 149 Max Points on a Line]]></title>
    <url>%2F2017%2F11%2F20%2F2017-11-20-leetcode_149%2F</url>
    <content type="text"><![CDATA[BeginDescription: Given n points on a 2D plane, find the maximum number of points that lie on the same straight line. 题目理解： 首先这道题的题干很短，基本第一遍都会理解错，也不清楚输入是个啥，所以这道题被踩了300多次，赞了100次不到… 经过几次错误的理解之后，明白了题目的意思：给一系列点，每个点有一个（x，y）的坐标取值，找出处于同一条直线上的最多点数。 这里需要注意： 可以有取值重复的坐标点。 直线可以是任意的斜率（以x,y的直角坐标系来看）。 思路思路一： 计算出可能的所有直线，统计每一条直线上的点的个数。直线以它的斜率加截距来表示： y = kx + b给予两个点的坐标，$(x_1,y_1)$、$(x_2,y_2)$，计算$k$与$b$的计算公式为： k = \frac{y_1 - y_2}{x_1 - x_2} b = y_1 - k x_1遇到的问题： 除法会有精度损失，可能造成不一样的直线算出来的斜率一样，而且可能会溢出，因为有加减运算。 不好统计直线上的点数，例如A、B、C三个点在一条线上，那么就有(A,B)(B,C)(A,C)三个组合， 那么这条直线上的点数该如何统计？ 思路二： 以每一个点为起点，计算它与其它的点组成的直线，统计直线上的点的次数。 这里的直线斜率直接使用分数表示，不需要截距，因为起点为同一个点。 这里的分数表示需要计算分子与分母的最大公约数，在后面进行介绍，感觉这个才是这道题的重点。 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475/** * Definition for a point. * class Point &#123; * int x; * int y; * Point() &#123; x = 0; y = 0; &#125; * Point(int a, int b) &#123; x = a; y = b; &#125; * &#125; */class Solution &#123; private class PCount &#123; Point point; int count = 1; public PCount(Point point) &#123; this.point = point; &#125; &#125; private int GCD(int a, int b) &#123; int c; while ( b != 0 ) &#123; c = a % b; a = b; b = c; &#125; return a; &#125; public int maxPoints(Point[] points) &#123; if ( points == null ) return 0; int max = 0; PCount[] pcount = new PCount[points.length]; int pidx = 0; Map&lt;String, Integer&gt; ptable = new HashMap&lt;&gt;(); for ( Point point : points ) &#123; String coord = point.x + "*" + point.y; Integer idx = ptable.get(coord); if ( idx == null ) &#123; ptable.put(coord, pidx); pcount[pidx++] = new PCount(point); &#125; else pcount[idx].count++; &#125; if ( pidx == 1 ) return points.length; for ( int i = 0; i &lt; pidx; i++ ) &#123; Point p1 = pcount[i].point; Map&lt;String, Integer&gt; lines = new HashMap&lt;&gt;(); for ( int j = i + 1; j &lt; pidx; j++ ) &#123; Point p2 = pcount[j].point; String line = null; if ( p1.x == p2.x ) &#123; line = "1/0"; &#125; else if ( p1.y == p2.y ) &#123; line = "0/1"; &#125; else &#123; int denominator = p1.y - p2.y; int numerator = p1.x - p2.x; int gcd = GCD(denominator, numerator); if ( gcd != 0 ) &#123; denominator /= gcd; numerator /= gcd; &#125; line = denominator + "/" + numerator; &#125; lines.put(line, lines.getOrDefault(line, pcount[i].count) + pcount[j].count); &#125; for ( int count : lines.values() ) max = max &lt; count ? count : max; &#125; return max; &#125;&#125; 最大公约数对于两个数A、B，求解它们的最大公约数。 方法一： 选出A、B中最小的数，向下一个一个进行取余，直到两个数的余数同时为0： 1234567public int GCD(int a, int b) &#123; int c = a &gt; b ? b : a; while ( a % c != 0 || b % c != 0 ) &#123; c--; &#125; return c;&#125; 方法二： 显然方法一的效率很低，那么肯定有别的快速的方法。 对A、B，假设它们的最大公约数为C，那么： A = k_1 C B = k_2 C注意，其中$k_1$与$k_2$一定是互质的，那么对A与B取余： D = A \% B = (k_1 \% k_2)C那么注意到D的取值情况： D的值为0，那么B就是最大公约数，其中$k_2=1$。 D的值不为0，D与B之间的最大公约数还是C。 由于D的绝对值会比A要小，并且B、D之间的最大公约数还是C，所以可以计算B、D之间的最大公约数来代替计算A、B之间的。 123456789public int GCD(int a, int b) &#123; int c; while ( b != 0 ) &#123; c = a % b; a = b; b = c; &#125; return a;&#125; 这个方法会比方法一快很多。]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>study</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SFC分配简单实验]]></title>
    <url>%2F2017%2F11%2F17%2F2017-11-17-SFC%2F</url>
    <content type="text"><![CDATA[github链接 2017年11月17日生成数据集这里先固定好整个SFC的结构，也就是一共几类VNF，以及每一类SFC的实例个数，以及QOS属性的数量和前向属性的个数K。 可以变动的参数有： 每一个VNF当前的QOS取值 每一个属性的权重 那么生成的样本的输入属性就是:所有VNF当前的QOS，以及每一个属性的权重组成的一个向量 样本的标签为每一类VNF的选择。 模拟环境示意图 确定到网络的结构1__init__(self, num_i = [2, 3], scope_L = [[1, 1000],[1, 100]], K = 1) num_i为每一类VNF的数量 scope_L 为每一个QOS属性的取值范围 K 为前向属性的数量 随机QOS请求首先确定出QOS请求的范围 注意到应该有QOS不能被SFC所满足 那么这里将它的范围上限定义为属性的上限乘上VNF的种类数，下限就为属性的下限。？ 生成样本写入文件生成一百万条样本 写成CSV文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import numpy as npimport sfc_model as sfcnum_i = [3,4,5]scope_L = [[1, 1000],[1, 100],[1,100],[1,1000]]K = 2my_model = sfc.sfc_model(num_i = num_i, scope_L = scope_L, K = K)def random_qos( N = 3, scope_L = [[1, 1000],[1, 100],[1,100],[1,1000]], K = 2 ): L = len(scope_L) qos = np.zeros(L) for i in range(L): qos[i] = np.random.rand(1)[0] * (scope_L[i][1] * N - scope_L[i][0]) + scope_L[i][0]# if i &lt; K:# qos[i] = np.random.rand(1)[0] * (scope_L[i][1] * N - scope_L[i][0]) + scope_L[i][0]# else:# qos[i] = np.random.rand(1)[0] * (scope_L[i][1] * N - scope_L[i][0]) + scope_L[i][0] return qos## 把 path 变成一个分类问题N = 3back_muti = np.zeros(N)category = 1for i in range(N): back_muti[i] = 1 category = category * num_i[i] for j in range(i+1, N): back_muti[i] = back_muti[i] * num_i[j]def get_category(path): if path is None: return category cate = 0 for i in range(N): cate = cate + path[i] * back_muti[i] return int(cate)T = 100 N = 3num_i = [3,4,5]samples = np.zeros((T, len(scope_L)*np.sum(num_i) + len(scope_L)*2))# labels = np.zeros((T, len(num_i) + np.sum(num_i)))labels = np.zeros((T, category+1))for t in range(T): qos = random_qos() my_model.construct() path, _ = my_model.serveQOS(random_qos()) sample = qos sample = np.hstack((sample, my_model.W)) sample = np.hstack((sample, my_model.qoses.reshape(-1))) label = np.zeros(category+1) idx = get_category(path) label[idx] = 1 samples[t] = sample.reshape(1,-1) labels[t] = label.reshape(1,-1) if t % 1000 == 0: print(t)np.savetxt('data/X_train.csv', samples, delimiter = ',')np.savetxt('data/Y_train.csv', labels, delimiter = ',') 神经网络直接使用keras，使用一个三隐藏层的神经网络进行尝试。 每一层1000个神经元。 12345678910111213141516171819def my_net(input_len = (56,), output_len = 61): X_input = Input(input_len) X = Dense(1000, activation='relu', name='fc1', kernel_initializer = glorot_uniform())(X_input) X = BatchNormalization(name = 'bn1')(X) X = Dense(1000, activation='relu', name='fc2', kernel_initializer = glorot_uniform())(X) X = BatchNormalization(name = 'bn2')(X) X = Dense(1000, activation='relu', name='fc3', kernel_initializer = glorot_uniform())(X) X = BatchNormalization(name = 'bn3')(X) X = Dropout(0.7)(X) X = Dense(output_len, activation='softmax', name='output', kernel_initializer = glorot_uniform())(X) model = Model(inputs = X_input, outputs = X, name='DemoNet') return model 目前的效果使用100万条样本，其中70万条作为训练集，30万作为测试集。 1234Epoch 50/50 - 124s - loss: 1.0770 - acc: 0.6653Test Loss = 1.12501473908Test Accuracy = 0.668746666667 2017年11月24日sfc网络数据生成前提： 网络结构最大5*5,否则网络过大会造成数据量爆炸。 Qos属性数量最大为4。理由同上。 想象中，请求从前向后依次经过每一类VNF，因为这里是加性的权重，经过顺序其实没有关系。 Qos属性的取值范围确定。反正进行归一化之后，范围没有任何影响。 可变参数： 网络中存在的VNF。在5*5的网络结构矩阵中，存在的VNF取值为1，否则为0。 VNF的Qos取值。由于是4维属性，那么就是5*5*4的矩阵。 前后向属性的个数。前向属性，Qos取值乘上-1。 每一个属性的权重。4维属性，那么将每一个属性的权重扩张为5*5的矩阵，组成5*5*4的矩阵。 Qos请求。4维属性，与上面相同，扩张为5*5*4的矩阵。 注意：这里的参数之所以设计为矩阵，是为了利用到卷积神经网络。 样本属性： 4层Qos属性取值，4层Qos属性权重，4层Qos请求，总共12层，5*5*12？ 样本标签： 5*6的矩阵，第一行有6列，每一列代表一个分类，那么就是选1、2、3、4、5或者不选（也就是不满足）。 图形表示： 神经网络训练五个同样结构的神经网络，每一个指示其中一类VNF的选取。 1234567891011121314151617181920212223242526272829def my_net(input_len = (5,5,12), classes = 6 ): X_input = Input(input_len) # padding to 6 * 6 X = ZeroPadding2D((1, 1))(X_input) # first conv X = Conv2D(filters = 64, kernel_size = (3, 3), strides = (1,1), padding = &apos;same&apos;, name = &apos;conv1&apos;, kernel_initializer = glorot_uniform())(X) X = BatchNormalization(axis = 3, name = &apos;bn1&apos;)(X) X = Activation(&apos;relu&apos;)(X) # second conv X = Conv2D(filters = 128, kernel_size = (3, 3), strides = (1,1), padding = &apos;same&apos;, name = &apos;conv2&apos;, kernel_initializer = glorot_uniform())(X) X = BatchNormalization(axis = 3, name = &apos;bn2&apos;)(X) X = Activation(&apos;relu&apos;)(X) # Pooling X = MaxPooling2D((2, 2), strides=(2, 2))(X) # output layer X = Flatten()(X) X = Dense(512, activation=&apos;relu&apos;, name=&apos;fc1&apos;, kernel_initializer = glorot_uniform())(X) X = Dropout(0.5)(X) X = Dense(classes, activation=&apos;softmax&apos;, name=&apos;fc2&apos;, kernel_initializer = glorot_uniform())(X) model = Model(inputs = X_input, outputs = X, name=&apos;SFC_Net&apos;) return model 训练集样本： 123$ print(X_train.shape, Y_train.shape)(100000, 5, 5, 12) (100000, 5, 6) 测试集样本： 123$ print(X_test.shape, Y_test.shape)(10000, 5, 5, 12) (10000, 5, 6) 样本中大约10%的样本为当前不能服务。 训练次数： epoch = 100。 测试集上精度： 单神经网络精度（单类VNF选择）：88.58 % 总精度：58.12%]]></content>
      <categories>
        <category>deep learning</category>
        <category>network</category>
      </categories>
      <tags>
        <tag>study</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卷积神经网络（CNN）笔记]]></title>
    <url>%2F2017%2F11%2F10%2F2017-11-10-CNN%2F</url>
    <content type="text"><![CDATA[简单描述Convolutional Neural Networks(CNN)，卷积神经网络主要是为了去解决计算机视觉上的问题，实际上，就是因为 CNN在计算机视觉上的成功应用，才让它变得这么火。 学习CNN，不光是为了在计算机视觉上的处理，更为重要的是去理解神经网络中蕴含的道理，事实上， 会觉得神经网络越来越像是一个搭积木的过程，有着各式各样的积木，你以随意的顺序或结构去搭建它们， 然后就得到了各式各样的神经网络类型。 CNN的结构CNN其实与全连接神经网络并没有什么太大的区别，它只是在全连接的神经网络中加入了一种新的连接层，卷积层。 如上图，网络先进过一层卷积层（CONV），然后通过一个RELU激活函数，再通过一个POOL层，最后输入到后面的两层全连接层。 通常CNN的结构就与上图一样，网络首先是一系列的卷积层（包括激活函数与POOL），在网络的最后跟着连接几层的全连接层。 卷积层这里引用到吴恩达课程作业中的一段演示视频，这一段视频十分的形象，这里引用到它： 如上面的视频所示，可以把输入层看作一个矩阵，它的维度为n_H_prev * n_W_prev * n_C_prev（这里先考虑单独一个样本的情况）， 如果这是从输入层开始的话，其中n_H_prev * n_W_prev就可以看成输入图片的长宽， 那么n_C_prev在图片是彩色的情况下就等于3，也就是RGB三个通道，如果是灰度图，那它就为1。 从英文上来说就是Height、Weight、Channel。 从视频中可以看到这个卷积层中有两个Filter，它们之间的运算完全没有关系，对于一个Filter来说，它在输入矩阵上从左上角开始， 左右滑动求取到值，每一次滑动就计算得到一个对应的值。在视频中它计算完毕后就得到一个3x3的输出。 通常情况下，所有Filter的维度都是一样的，视频中Filter的维度为f * f * n_C_prev，它的深度要与输入的Channel数一致 才能正常的进行计算，所有这里它的第三个维度为n_C_prev。设Filter对应的输入矩阵上的小格子为 $X$，Filter表示为$F$， 那么它们之间的计算公式为： output = bias + \sum_i^f \sum_j^f \sum_k^{C} X_{(i,j,k)} * F_{(i,j,k)}简单来说，就是两个矩阵做点积，然后求和，最后加上一个bias。那么对于一个Filter来说， 它就有 $f \times f \times C + 1$ 个参数。 最后将每一个Filter计算得到的结果作为一个Channel，叠在一起，就组成了最后的输出结果。 Padding通过上面的计算步骤，只要Filter的长宽不是1，输出矩阵的长与宽就一定比输入矩阵的长与宽要小，并且对于矩阵角上的值来说， 它们参与运算的次数是没有矩阵内部的值参与的次数多的。 为了不让矩阵长与宽一直变小，让所有值相对公平的参与计算，就可以使用到Padding操作。 它的操作十分简单，就是在矩阵四周补零就可以了。 通常的深度学习框架中，例如TensorFlow，它在卷积层的参数中可以直接设置padding，它有两个取值valid和same， 其中valid就表示不进行padding，same则是表示进行padding，并且padding的大小刚好保证输入输出的维度大小相同。 stride如上面所说，Filter在图片上进行滑动，那么滑动的距离就由stride来定义，通常它取值为1或2。 输出维度进过上面的一顿操作，输出的维度为多少呢？随便就可以推出来，计算输出的维度公式如下： n_H = \left \lfloor \frac{n_{H_{prev}} - f + 2 \times pad }{ stride } \right \rfloor + 1POOL层POOL层总是接在卷积层的后面，那么它到的是在干嘛的，它为什么要取名字叫做POOL？在学习卷积神经网络之前一直以为这一层很复杂， 但是实际上，这一层的操作十分简单。 如上图所示，左边是Max Pool，右边是Average Pool，它和卷积层的操作几乎一样，也是滑动，每滑动一次计算出一个值。 它与卷积层操作的区别是它每次取得是区域内的最大值或者是平均值，也就是它计算出的结果只与输入矩阵有关系， 因为只有输入矩阵的值会参与运算。另外，它没有通道的概念，也就是它只有一个通道，所以它不是同时对输入矩阵的多个通道进行操作， 而是对输入的每个通道分别进行操作。 所以，输入与输出的维度进行比较的话，长宽通常会发生变化，而深度不会发生变化。 实际上，通常把POOL层理解为降低数据维度的方法。 卷积层在做神马？在图像中，可以把每一个Filter看做一个特征的识别器（毕竟人家名字就叫做Filter），或许某个Filter的作用就是识别 一条直线，或者又是识别一个直角等。总之可以把一个Filter看成一个特征的识别。 事实上，全连接网络也能完成卷积层的作用，但是全连接层会带来太多的参数，另外，对于一个卷积层，不知要多少个全连接的网络层才能 完成与它相同的功能。所以卷积层的出现，带来了图像识别方面的巨大进步。 反向传播其实卷积层与池化层的反向传播并不复杂。 首先注意到卷积层，对于它的一个输出值来说，它来源于输入的某一块与其中一个Filter运算后得到的结果，那么它对Filter 的求导，就能得到这个Filter的一个求导值。 这样一个一个求取，就能得到每个Filter的导数值，进而就能去更新Filter的权值。 对于POOL层，它的求导也十分简单，首先对于Max Pool，它的一个输出值来自与输入的一个值，那么直接把这个值传回就好。 对于Average Pool，它的一个输出值来源于很多个输入值，并且这些输入值对它的贡献是一样的，所以，将这个输出值平均传回就好。 未完待续？]]></content>
      <categories>
        <category>coursera</category>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>study</tag>
        <tag>mechine learning</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java使用poi读写excel文件]]></title>
    <url>%2F2017%2F10%2F10%2F2017-10-10-java-poi%2F</url>
    <content type="text"><![CDATA[在java中使用poi来读取excel文件是一种比较普遍的做法，这里记录一下代码，免得以后又忘记如何读写excel文件。 下载poi直接取官网下载poi包，Apache POI - Download Release Artifacts。 这里下载的是最新的3.17版本。 可以看到里面有很多jar包。 读取excel文件在读excel文件的过程中，只需要用到poi-3.17.jar和poi-ooxml-3.17.jar两个jar包就够了。 在官方的 Quick Guide 中有读取文件以及读取单元格的例子，依照它就可以写出读取excel文件的代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105import org.apache.poi.openxml4j.exceptions.InvalidFormatException;import org.apache.poi.ss.usermodel.*;import java.io.File;import java.io.FileInputStream;import java.io.FileNotFoundException;import java.io.IOException;import java.util.ArrayList;import java.util.List;/** * 将Excel按照行解析 * 希望第一行是标题，重第二行开始是数据，格式不要乱啊 * 解析给定的 excel 文件，然后返回 excel 的所有内容 * @author long * @date 2017-10-10 * @version v1.0 */public class PoiWithExcel &#123; /** * 输入excel文件的路径（.xls结尾的文件），输出文件内容，第一行是 title，剩下的行是内容。 * 注意，这里认为文件只有一个 sheet 。 * @param path * @return */ public List&lt;String[]&gt; parseExcelData(String path) throws FileNotFoundException &#123; File input = new File(path); FileInputStream fis = new FileInputStream(input); List&lt;String[]&gt; contents = new ArrayList&lt;&gt;(); Workbook workbook = null; Sheet sheet = null; try &#123; workbook = WorkbookFactory.create(fis); sheet = workbook.getSheetAt(0); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (InvalidFormatException e) &#123; e.printStackTrace(); &#125; int row_num = sheet.getPhysicalNumberOfRows(); //获取总行数 /************* 遍历第一行，获取 title ***********/ Row row = sheet.getRow(0); if ( row == null ) return contents; int col_num = row.getPhysicalNumberOfCells(); //获取总列数 String[] content = new String[col_num]; for (int col = 0 ; col &lt; col_num ; col++ ) &#123; Cell cell = row.getCell(col); content[col] = myGetCellValue(cell); &#125; contents.add(content); /************* 遍历剩余行，读取内容 ***********/ for ( int r = 1; r &lt; row_num; r++ ) &#123; row = sheet.getRow(r); content = new String[col_num]; for ( int col = 0; col &lt; col_num; col++ ) &#123; Cell cell = row.getCell(col); content[col] = myGetCellValue(cell); &#125; contents.add(content); &#125; try &#123; workbook.close(); fis.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return contents; &#125; /** * 按照 单元格 的 类型 来解析它其中的内容 * @param cell * @return */ private String myGetCellValue(Cell cell) &#123; if ( cell == null ) return ""; String cellValue = null; // 在官方的说明中，它使用的是 cell.getCellType(), 但是这个方法是过时的， // 所以这里使用 cell.getCellTypeEnum() 来取代了它。 switch(cell.getCellTypeEnum()) &#123; case STRING: //文本 cellValue = cell.getStringCellValue(); break; case NUMERIC: //数字、日期 if (DateUtil.isCellDateFormatted(cell)) &#123; cellValue = cell.getDateCellValue().toString(); &#125; else &#123; cellValue = String.valueOf(cell.getNumericCellValue()); &#125; break; case BOOLEAN: //布尔型 cellValue = String.valueOf(cell.getBooleanCellValue()); break; case FORMULA: //公式 cellValue = cell.getCellFormula(); break; default: cellValue = ""; &#125; return cellValue; &#125;&#125;]]></content>
      <categories>
        <category>guide</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python pip 安装小指南]]></title>
    <url>%2F2017%2F10%2F03%2F2017-10-03-pip-install%2F</url>
    <content type="text"><![CDATA[原始命令首先，在python安装目录下的Scripts目录下，有pip和pip3两个命令。这两个命令的区别是， 如果电脑上安装了python2，那么pip安装的包会在python2下面，而pip3会在python3下面。 当然如果没有python2，那么它们是没有区别的。 安装命令： 123$ pip install package$ pip3 install package 卸载命令： 1$ pip uninstall package 升级命令： 1$ pip install --upgrade package 查看可以更新的包： 1$ pip list --outdated 查看某个已经安装的包： 1$ pip show --files SomePackage 国内源上面安装默认用的不是国内源，可能会很慢，那么就要用到国内源。 一般使用豆瓣源或者清华源， http://pypi.douban.com/simple/https://pypi.tuna.tsinghua.edu.cn/simple 在pip命令中加入-i 源地址就行，例如， 1$ pip3 install -i http://pypi.douban.com/simple/ package （今天清华源莫名奇妙崩了，不知道为何） 安装失败问题今天在安装scripy时，不知道出了什么问题，安装不上，百度后发现这个直接安装就是不行， 先看官网说明Installing via pip 其中有一句： pip does not work well for Windows because the standard pip package index site, PyPI, does not yet have Windows wheels for some packages, such as SciPy. wtf，那么windows该怎么安装啊，靠。 百度一下，发现通过以下网站来解决问题， Unofficial Windows Binaries for Python Extension Packages 安装它的说明，先安装wheel，然后下载numpy安装，再下载scipy安装就行了。 1$ pip3 install wheel 然后将对应版本的安装包下载下来进行安装，我这里选择的是scipy-1.0.0rc1-cp36-cp36m-win_amd64.whl， 下载之后，到对应的文件目录执行下面的命令即可安装（numpy早就已经安装了）， 1$ pip3 install scipy-1.0.0rc1-cp36-cp36m-win_amd64.whl 然后sklearn还是不能用！！！ 所以如果使用whl安装，那么最好这几个依赖包都使用whl来安装，这样才能没有问题！！！ 于是，下载对应的numpy、scipy、matplotlib和scikit_learn的whl包来安装。 首先卸载： 1234$ pip3 uninstall scikit_learn$ pip3 uninstall matplotlib$ pip3 uninstall scipy$ pip3 uninstall numpy 然后安装： 1234$ pip3 install numpy-1.13.3+mkl-cp36-cp36m-win_amd64.whl$ pip3 install scipy-1.0.0rc1-cp36-cp36m-win_amd64.whl$ pip3 install matplotlib-2.1.0rc1-cp36-cp36m-win_amd64.whl$ pip3 install scikit_learn-0.19.0-cp36-cp36m-win_amd64.whl 终于ok了，我去。 安装失败问题安装的时候遇到了UnicodeDecodeError，提示UTF-8解码有问题。 这是因为windows的shell的编码问题，它使用的是gbk？ 总之切换一下shell的代码页就好了： 1chcp 65001]]></content>
      <categories>
        <category>guide</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode 85 Maximal Rectangle]]></title>
    <url>%2F2017%2F09%2F28%2F2017-09-28-leetcode_85%2F</url>
    <content type="text"><![CDATA[Description: Given a 2D binary matrix filled with 0’s and 1’s, find the largest rectangle containing only 1’s and return its area. For example, given the following matrix: 12341 0 1 0 01 0 1 1 11 1 1 1 11 0 0 1 0 Return 6. 题目理解： 在矩阵中去找最小的长方形，初步想法是尝试用动态规划来做，但是实际上是不行的， 因为原问题的最优解好像并不能由它的子问题的最优解来构成。 然后就想能不能用什么搜索的方法来寻找，通过逐步扩大搜索范围来求解，并不知道怎么做。 突然想到将两行加在一起，例如下面两行， 121 0 1 0 01 0 1 1 1 加起来得到， 12 0 2 1 1 那么这两行的最大长方形要不就是2，要不就是衡向的三个1，那么如果是三行呢？ 1231 0 1 0 01 0 1 1 11 1 1 1 1 加起来得到， 13 1 3 2 2 从这个结果可以看出，这里就将上面的矩阵表现成为了一个柱状图，值就是它的高度！！！ 那么这不正好就是上一题84题所做的东西，求柱状图中的最大长方形。 于是就变得十分简单，将上一题的代码直接拿过来，就解决了问题。 1234567891011121314151617181920212223242526272829303132333435363738394041class Solution &#123; public int maximalRectangle(char[][] matrix) &#123; if ( matrix == null || matrix.length == 0 ) return 0; int m = matrix.length, n = matrix[0].length, max = 0; int[] row_sum = new int[n]; for ( int i = 0; i &lt; m; i++ ) &#123; for ( int j = 0; j &lt; n; j++ ) row_sum[j] = ('0' == matrix[i][j]) ? 0 : row_sum[j] + 1; int area = largestRectangleArea(row_sum); if ( area &gt; max ) max = area; &#125; return max; &#125; private int largestRectangleArea(int[] heights) &#123; int max = 0, n = heights.length; int[] small_left = new int[n]; int[] small_right = new int[n]; small_left[0] = -1; small_right[n-1] = n; for ( int i = 1; i &lt; n; i++ ) &#123; int idx = i - 1; while ( idx &gt;= 0 &amp;&amp; heights[idx] &gt;= heights[i] ) idx = small_left[idx]; small_left[i] = idx; &#125; for ( int i = n - 2; i &gt;= 0; i-- ) &#123; int idx = i + 1; while ( idx &lt; n &amp;&amp; heights[idx] &gt;= heights[i] ) idx = small_right[idx]; small_right[i] = idx; &#125; for ( int i = 0; i &lt; n; i++ ) &#123; int area = (small_right[i] - small_left[i] - 1) * heights[i]; if ( area &gt; max ) max = area; &#125; return max; &#125;&#125; 由于上一题的代码复杂度为O(n)，那么这里的复杂度就是O(n^2)。运行时间：8ms。击败：95.90%]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>study</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode 84 Largest Rectangle in Histogram]]></title>
    <url>%2F2017%2F09%2F28%2F2017-09-28-leetcode_84%2F</url>
    <content type="text"><![CDATA[Descriptin: Given n non-negative integers representing the histogram’s bar height where the width of each bar is 1, find the area of largest rectangle in the histogram. 题目理解：本题需要从柱状图中找出最大的长方形，求解当然不难，但是想要降低复杂度就很难。 代码一： 最简单的求解方法，遍历柱状图，对每一根柱，求它的最大长方形，所以也需要一个循环来做，于是复杂度O(n^2)。 12345678910111213141516class Solution &#123; public int largestRectangleArea(int[] heights) &#123; int max = 0, n = heights.length; for ( int i = 0; i &lt; n; i++ ) &#123; int left = i - 1, right = i + 1; while ( left &gt;= 0 &amp;&amp; heights[left] &gt;= heights[i] ) left--; while ( right &lt; n &amp;&amp; heights[right] &gt;= heights[i] ) right++; int area = (right - left - 1) * heights[i]; if ( max &lt; area ) max = area; &#125; return max; &#125;&#125; 结果是代码超时，最后一个用例全是1，卡在上面了。 代码二： 这个代码是自己想了很久写出来的，看了别人的才发现自己有些地方想偏了，所以才想了这么久。 想法是，由于在上面遍历过程中，老是重复一些东西，所以这里对已经遍历过的部分进行一定的处理，并且计算方式有所变化。 遍历到一个柱时，它前面的柱体一定是削平了的。 如下图， 在遍历到下标3时，把前面变成2削成了1，同时下标1的1值也不用再保存了， 因为后面的柱体和它组成的长方形一定没有与前面的“1”大。 同样的，遍历到下标5时会变成下面的形式， 前面只是保证了前面的柱体是削平的，那么时候去计算长方形的大小呢？当遇到下坡时就去计算， 比如当遍历到2的时候， 它前面的柱体呈现一个1,5,6的形式，由于5,6比2高，那么就要去把它们削到高度为2， 在削的时候，就去计算被削的柱体它能组成的长方形大小。 比如高度为6的柱体，因为它是被削平的，所以它到当前高度2的柱体之间如果有别的柱体，也一定是至少比6要高的。 所以就长方形的长度就等于6到2之间的间隔，乘上6。 这样每一个柱体只会计算一次长方形的大小，并且这个过程是没有查找的操作的。 所以复杂度为O(n)。 1234567891011121314151617181920212223242526272829303132class Solution &#123; public int largestRectangleArea(int[] heights) &#123; int max = 0, flen = 0, n = heights.length; int[][] front = new int[n][2]; for ( int i = 0; i &lt; n; i++ ) &#123; if ( flen == 0 || heights[i] &gt; front[flen-1][0] ) &#123; front[flen][0] = heights[i]; front[flen++][1] = i; &#125; else &#123; int j = flen - 1; while ( j &gt;= 0 &amp;&amp; front[j][0] &gt; heights[i] ) &#123; int area = (i - front[j][1]) * front[j][0]; if ( area &gt; max ) max = area; j--; &#125; if ( j == -1 || front[j][0] &lt; heights[i] ) j++; front[j][0] = heights[i]; flen = j + 1; &#125; &#125; int j = flen - 1; while ( j &gt;= 0 ) &#123; int area = (n - front[j][1]) * front[j][0]; if ( area &gt; max ) max = area; j--; &#125; return max; &#125;&#125; 代码三： 这个是看的别人的代码，这个思想很好，而且正好是从代码一最简单形式变化而来的，不得不佩服。 考虑到代码一每次都重复去寻找左右比当前柱体矮的柱体，它就相当于进行了预处理，先将一个柱体的左右比它矮的柱体找到了， 然后就可以快速的计算了。 12345678910int[] lessFromLeft = new int[height.length];for (int i = 1; i &lt; height.length; i++) &#123; int p = i - 1; while (p &gt;= 0 &amp;&amp; height[p] &gt;= height[i]) &#123; p = lessFromLeft[p]; &#125; lessFromLeft[i] = p;&#125; 最重要的就是上面的代码，它就是去寻找当前柱体左边的第一个比它矮的柱体，使用了一个数组进行保存柱体下标， 它的核心思想是，前面的柱体已经找到了比自己矮的柱体的位置，那么当前柱体就可以利用到这个信息， 快速的找到比自己矮的柱体。就如写这个代码的那个人所说，这里的思想有点像KMP。 这里的这种将问题拆分，然后进行求解的思想值得学习。 1234567891011121314151617181920212223242526private int largestRectangleArea(int[] heights) &#123; if ( matrix == null || matrix.length == 0 ) return 0; int max = 0, n = heights.length; int[] small_left = new int[n]; int[] small_right = new int[n]; small_left[0] = -1; small_right[n-1] = n; for ( int i = 1; i &lt; n; i++ ) &#123; int idx = i - 1; while ( idx &gt;= 0 &amp;&amp; heights[idx] &gt;= heights[i] ) idx = small_left[idx]; small_left[i] = idx; &#125; for ( int i = n - 2; i &gt;= 0; i-- ) &#123; int idx = i + 1; while ( idx &lt; n &amp;&amp; heights[idx] &gt;= heights[i] ) idx = small_right[idx]; small_right[i] = idx; &#125; for ( int i = 0; i &lt; n; i++ ) &#123; int area = (small_right[i] - small_left[i] - 1) * heights[i]; if ( area &gt; max ) max = area; &#125; return max;&#125; 运行时间：5ms。复杂度：O(n)。]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>study</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python argparse 模块笔记]]></title>
    <url>%2F2017%2F09%2F26%2F2017-09-26-python-argparse%2F</url>
    <content type="text"><![CDATA[argparse是python中的一个模块，参考官方文档来理解它 argparse — Parser for command-line options, arguments and sub-commands 第一句介绍就是The argparse module makes it easy to write user-friendly command-line interfaces， 所以这个模块是用于写命令行接口的，使得脚本友好的从命令行传入参数。 导入1import argparse 创建一个parser对象1parser = argparse.ArgumentParser() 添加参数12parser.add_argument('integers', metavar='N', type=int, nargs='+', help='an integer for the accumulator') 这里添加了一个叫做integers的参数，它的类型是int，metavar是用于在help里面来表示这个数的，本来是help里面会使用integers这个名字，这里就会被替换为N，nargs表示这个参数后面可以跟的输入的个数，这里的’+’表示一个或者多个，help是在打印help信息时给出的提示。 例如下面脚本： 1234567import argparseparser = argparse.ArgumentParser()parser.add_argument('--foo', nargs=2, metavar='f')parser.add_argument('bar', nargs=1)args = parser.parse_args()print(args) 先在命令行中调用帮助信息， 12345678910$ python .\test.py -husage: test.py [-h] [--foo f f] barpositional arguments: baroptional arguments: -h, --help show this help message and exit --foo f f 输入正确的参数个数和错误的参数个数， 12345678910111213$ python .\test.py --foo a b 1Namespace(bar=[&apos;1&apos;], foo=[&apos;a&apos;, &apos;b&apos;])$ python .\test.py --foo a busage: test.py [-h] [--foo FOO FOO] bartest.py: error: the following arguments are required: bar$ python .\test.py --foo a b c 2usage: test.py [-h] [--foo FOO FOO] bartest.py: error: unrecognized arguments: 2 另外有时候，可能需要使用下面的函数来返回输入参数， 1FLAGS, unparsed = parser.parse_known_args() 这里的意思就是说对于认识的参数，存到FLAGS里面，对于不认识的参数，存到unparsed里面。 参考argparser模块学习]]></content>
      <categories>
        <category>python modules</category>
      </categories>
      <tags>
        <tag>study</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow 笔记一]]></title>
    <url>%2F2017%2F09%2F26%2F2017-09-26-tensorflow-tutorial%2F</url>
    <content type="text"><![CDATA[Building Input Functions with tf.estimator直接从Building Input Functions with tf.estimator这一节开始，前面的就先不做笔记了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354from __future__ import absolute_importfrom __future__ import divisionfrom __future__ import print_functionimport itertoolsimport pandas as pdimport tensorflow as tftf.logging.set_verbosity(tf.logging.INFO)COLUMNS = ["crim", "zn", "indus", "nox", "rm", "age", "dis", "tax", "ptratio", "medv"]FEATURES = ["crim", "zn", "indus", "nox", "rm", "age", "dis", "tax", "ptratio"]LABEL = "medv"training_set = pd.read_csv("boston_train.csv", skipinitialspace=True, skiprows=1, names=COLUMNS)test_set = pd.read_csv("boston_test.csv", skipinitialspace=True, skiprows=1, names=COLUMNS)prediction_set = pd.read_csv("boston_predict.csv", skipinitialspace=True, skiprows=1, names=COLUMNS)feature_cols = [tf.feature_column.numeric_column(k) for k in FEATURES]regressor = tf.estimator.DNNRegressor(feature_columns=feature_cols, hidden_units=[10, 10], model_dir="/tmp5/boston_model")def get_input_fn(data_set, num_epochs=None, shuffle=True): return tf.estimator.inputs.pandas_input_fn( x=pd.DataFrame(&#123;k: data_set[k].values for k in FEATURES&#125;), y = pd.Series(data_set[LABEL].values), num_epochs=num_epochs, shuffle=shuffle)regressor.train(input_fn=get_input_fn(training_set), steps=5000)ev = regressor.evaluate(input_fn=get_input_fn(test_set, num_epochs=1, shuffle=False))loss_score = ev["loss"]print("Loss: &#123;0:f&#125;".format(loss_score))y = regressor.predict( input_fn=get_input_fn(prediction_set, num_epochs=1, shuffle=False)) # .predict() returns an iterator of dicts; convert to a list and print predictionspredictions = list(p["predictions"] for p in itertools.islice(y, 6))print("Predictions: &#123;&#125;".format(str(predictions))) 一段一段的理解： 123from __future__ import absolute_importfrom __future__ import divisionfrom __future__ import print_function 这里从__future__这个包下引入了absolute_import，division，print_function。 首先看future官方API， 简单来说就是将python的后面版本新特性导入到当前版本，使得当然版本也可以使用后面版本的语法。 看文档最后的表就知道，这里我使用的python3.6.1，所以这里的代码可以不写，如果是python较低版本，比如2.xxx，那就能需要导入一下了。 123import itertoolspredictions = list(p["predictions"] for p in itertools.islice(y, 6)) 同样的，来查看itertools官方API， 它是一个迭代的工具类，提供了各种迭代的方法。 这里的， 1itertools.islice(y, 6) 创建了一个迭代器，从y集合的第1个元素开始到第6个元素结束。 1import pandas as pd 一样的官方10 Minutes to pandas， pandas是基于numpy的，在做数据分析时，用它来表示矩阵向量可能更加方便快捷。 在安装pandas时，直接使用pip3 install pandas会很慢，基本等于失败，换源后效果显著， 1pip install -i https://pypi.tuna.tsinghua.edu.cn/simple some-package 这里换成清华源来安装，速度很快。 1tf.logging.set_verbosity(tf.logging.INFO) 打开这个过后，在进行训练时，会打印很多中间过程的信息， 12345678910111213INFO:tensorflow:Restoring parameters from /tmp5/boston_model\model.ckpt-15000INFO:tensorflow:Saving checkpoints for 15001 into /tmp5/boston_model\model.ckpt.INFO:tensorflow:loss = 1842.78, step = 15001INFO:tensorflow:global_step/sec: 702.316INFO:tensorflow:loss = 3675.04, step = 15101 (0.143 sec)INFO:tensorflow:global_step/sec: 779.157INFO:tensorflow:loss = 2918.1, step = 15201 (0.128 sec)INFO:tensorflow:global_step/sec: 667.102INFO:tensorflow:loss = 4292.76, step = 15301 (0.150 sec)INFO:tensorflow:global_step/sec: 704.81INFO:tensorflow:loss = 2817.55, step = 15401 (0.142 sec)INFO:tensorflow:global_step/sec: 727.968... 这里每100次迭代打印一次当前损失。 12345COLUMNS = ["crim", "zn", "indus", "nox", "rm", "age", "dis", "tax", "ptratio", "medv"]FEATURES = ["crim", "zn", "indus", "nox", "rm", "age", "dis", "tax", "ptratio"]LABEL = "medv" 定义三个list，第一个是读文件时需要读取的列，第二个是样本的属性，第三个是标签列。定义这个方便对属性和标签进行拆分。 12regressor = tf.estimator.DNNRegressor(feature_columns=feature_cols, hidden_units=[10, 10], model_dir="/tmp5/boston_model") 定义了一个深度学习回归模型，[10, 10]指定了模型共两层，每层10个神经元，model_dir=&quot;/tmp5/boston_model&quot;表示了模型存储的位置。 注意，这里的/tmp5/boston_model在windows下面表示是在根目录下的tmp5文件夹，比如程序是在F盘下执行的， 那么它建立一个F:/tmp5/目录，如果需要当前目录下的tmp5目录，那就要使用tmp5/boston_model路径。 1feature_cols = [tf.feature_column.numeric_column(k) for k in FEATURES] 这里先将FEATURES转化成回归模型所需的格式，然后传入模型。 12345678def get_input_fn(data_set, num_epochs=None, shuffle=True): return tf.estimator.inputs.pandas_input_fn( x=pd.DataFrame(&#123;k: data_set[k].values for k in FEATURES&#125;), y = pd.Series(data_set[LABEL].values), num_epochs=num_epochs, shuffle=shuffle)regressor.train(input_fn=get_input_fn(training_set), steps=5000) 注意这里get_input_fn中调用的tf.estimator.inputs.pandas_input_fn(...)返回的是一个函数，因为train()里面接受的是一个函数对象。 这里调用时按照格式来的，num_epochs代表数据集可以过几遍，对训练集当然没有限制，所以输入数None，对于验证集或者测试集， 这里的num_epochs就要设置为1，因为一个样本只需要过一次，同样的，shuffle表示是否随机读取样本，也只有训练集需要随机读取操作。 1234ev = regressor.evaluate(input_fn=get_input_fn(test_set, num_epochs=1, shuffle=False))loss_score = ev["loss"]print("Loss: &#123;0:f&#125;".format(loss_score)) 训练完成后，使用验证集进行验证，这里打印出损失分数。 123456y = regressor.predict( input_fn=get_input_fn(prediction_set, num_epochs=1, shuffle=False))# .predict() returns an iterator of dicts; convert to a list and print predictionspredictions = list(p["predictions"] for p in itertools.islice(y, 6))print("Predictions: &#123;&#125;".format(str(predictions))) 这里就是测试预测的结果，想要的效果是， 1Predictions: [33.480186, 18.6161, 23.09123, 34.338253, 16.050083, 19.354153] 实际的结果是， 123Predictions: [array([ 33.83599091], dtype=float32), array([ 17.83500481], dtype=float32), array([ 24.12747383], dtype=float32), array([ 35.41732025], dtype=float32), array([ 15.54900551], dtype=float32), array([ 17.97283173], dtype=float32)] 说明这里predict()返回的是一个列表，但是列表中的元素是array对象，所以这里要简单改动一下， 1predictions = list(p["predictions"][0] for p in itertools.islice(y, 6)) TensorBoard: Visualizing Learning同样的，直接阅读代码，来看看整个流程到底发生了什么，mnist_with_summaries.py。 首先，看入口函数: 1234567891011121314151617181920212223242526if __name__ == '__main__': parser = argparse.ArgumentParser() parser.add_argument('--fake_data', nargs='?', const=True, type=bool, default=False, help='If true, uses fake data for unit testing.') parser.add_argument('--max_steps', type=int, default=1000, help='Number of steps to run trainer.') parser.add_argument('--learning_rate', type=float, default=0.001, help='Initial learning rate') parser.add_argument('--dropout', type=float, default=0.9, help='Keep probability for training dropout.') parser.add_argument( '--data_dir', type=str, default=os.path.join(os.getenv('TEST_TMPDIR', '/tmp'), 'tensorflow/mnist/input_data'), help='Directory for storing input data') parser.add_argument( '--log_dir', type=str, default=os.path.join(os.getenv('TEST_TMPDIR', '/tmp'), 'tensorflow/mnist/logs/mnist_with_summaries'), help='Summaries log directory') FLAGS, unparsed = parser.parse_known_args() tf.app.run(main=main, argv=[sys.argv[0]] + unparsed) 第一句， 1if __name__ == '__main__': 表示下面的代码只有在这个文件被当做脚本执行时才会执行。 下一句， 1234parser = argparse.ArgumentParser()parser.add_argument('--fake_data', nargs='?', const=True, type=bool, default=False, help='If true, uses fake data for unit testing.') 参考argparse笔记来理解， 那么这里也就是定义了输入的可选参数，同时也设定了默认值。 然后关注这一句， 12default=os.path.join( os.getenv('TEST_TMPDIR', '/tmp'), 'tensorflow/mnist/input_data') 其中os.getenv(&#39;TEST_TMPDIR&#39;, &#39;/tmp&#39;)是查询系统信息的函数，它去查询系统中&#39;TEST_TMPDIR&#39;的值， 显然这里没有对&#39;TEST_TMPDIR&#39;定义，于是它就会返回这里的设置默认值&#39;/tmp&#39;，否则返回null。 然后，对于os.path.join()它就是一个将路径合并的函数，例如， 123$ os.path.join('/hello/','good/boy/','doiido')'/hello/good/boy/doiido' 但是如果是在windows下面执行，它就会变成下面这样， 123$ os.path.join( '/tmp', 'tensorflow/mnist/input_data')'/tmp\\tensorflow/mnist/input_data' 我去，所以在windows下面执行的时候老是路径报错。 接着就是main函数的最后两句， 12FLAGS, unparsed = parser.parse_known_args()tf.app.run(main=main, argv=[sys.argv[0]] + unparsed) 随便百度就可以看到它的源代码，它所进行的操作就是传入main函数，然后再传入参数，然后运行。 这里FLAGS里面的参数我们已经使用了，所以将剩下的参数传入，其中sys.argv[0]是当前文件的路径位置， unparsed就是剩下未使用的参数。 其次看main函数， 12345def main(_): if tf.gfile.Exists(FLAGS.log_dir): tf.gfile.DeleteRecursively(FLAGS.log_dir) tf.gfile.MakeDirs(FLAGS.log_dir) train() 这里的log_dir就是日志文件，也就是TensorBoard画图时所需要的文件。 操作就是删除旧的日志文件，然后重新建一个文件夹，再调用train()函数。 这里来到train()函数，剩下的所有代码都在这里面。 12345from tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True, fake_data=FLAGS.fake_data) 这个部分会读取本地的mnist数据集，如果没有它就会先将数据集下载下来，放到指定的数据目录里面。 1sess = tf.InteractiveSession() 这一句使用了InteractiveSession()来建立了会话，它与Session()的不同之处在于，使用了它之后， 调用时run()变成了tf.global_variables_initializer().run()。 1234# Input placeholderswith tf.name_scope('input'): x = tf.placeholder(tf.float32, [None, 784], name='x-input') y_ = tf.placeholder(tf.float32, [None, 10], name='y-input') 这里定义了两个输入的占位符。使用name_scope()使得在TensorBoard上它们会同处于&#39;input&#39;这个命名之下。 123with tf.name_scope('input_reshape'): image_shaped_input = tf.reshape(x, [-1, 28, 28, 1]) tf.summary.image('input', image_shaped_input, 10) 这里将输入重新reshape变成方阵（图片本来的形式），然后使用tf.summary.image()将它记录到日志里。 12345678910# We can't initialize these variables to 0 - the network will get stuck.def weight_variable(shape): """Create a weight variable with appropriate initialization.""" initial = tf.truncated_normal(shape, stddev=0.1) return tf.Variable(initial)def bias_variable(shape): """Create a bias variable with appropriate initialization.""" initial = tf.constant(0.1, shape=shape) return tf.Variable(initial) 定义了一个权重初始化的函数，输入需要初始化权重的shape，然后第一句进行了一个truncated_normal()， 它进行正态初始化，但是对于超出正态一定范围的值进行丢弃，返回的是一个tensor。 然后使用tf.Variable()将它变成一个变量。 同样的，将bias初始为0.1。 下面的函数是专门用来对变量进行记录的，提供给TensorBoard去使用， 1234567891011def variable_summaries(var): """Attach a lot of summaries to a Tensor (for TensorBoard visualization).""" with tf.name_scope('summaries'): mean = tf.reduce_mean(var) tf.summary.scalar('mean', mean) with tf.name_scope('stddev'): stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean))) tf.summary.scalar('stddev', stddev) tf.summary.scalar('max', tf.reduce_max(var)) tf.summary.scalar('min', tf.reduce_min(var)) tf.summary.histogram('histogram', var) 记录下变量的均值，标准差，最大值，最小值，柱状图。 接着的一个函数用来构建神经网络层， 12345678910111213141516171819202122def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu): """Reusable code for making a simple neural net layer. It does a matrix multiply, bias add, and then uses ReLU to nonlinearize. It also sets up name scoping so that the resultant graph is easy to read, and adds a number of summary ops. """ # Adding a name scope ensures logical grouping of the layers in the graph. with tf.name_scope(layer_name): # This Variable will hold the state of the weights for the layer with tf.name_scope('weights'): weights = weight_variable([input_dim, output_dim]) variable_summaries(weights) with tf.name_scope('biases'): biases = bias_variable([output_dim]) variable_summaries(biases) with tf.name_scope('Wx_plus_b'): preactivate = tf.matmul(input_tensor, weights) + biases tf.summary.histogram('pre_activations', preactivate) activations = act(preactivate, name='activation') tf.summary.histogram('activations', activations) return activations 初始化权重并记录，初始化偏置并记录。计算通过激活函数前的输出并记录，计算输出并记录，最后返回输出。 下面开始构建神经网络， 1hidden1 = nn_layer(x, 784, 500, 'layer1') 首先构建了第一层隐藏层，神经元数量500。 1234with tf.name_scope('dropout'): keep_prob = tf.placeholder(tf.float32) tf.summary.scalar('dropout_keep_probability', keep_prob) dropped = tf.nn.dropout(hidden1, keep_prob) 下一步则是在第一层之后，加入了一个dropout层，这里的keep_prob使用了占位符，以便调整。同样也将概率进行记录。 12# Do not apply softmax activation yet, see below.y = nn_layer(dropped, 500, 10, 'layer2', act=tf.identity) 这里建立了神经网络的第二层，但是激活函数这里传入的是一个tf.identity，这个函数的意思是， 传入什么数，它就传出什么数…那么这里就相当于是没有激活函数。 123456789101112131415with tf.name_scope('cross_entropy'): # The raw formulation of cross-entropy, # # tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.softmax(y)), # reduction_indices=[1])) # # can be numerically unstable. # # So here we use tf.nn.softmax_cross_entropy_with_logits on the # raw outputs of the nn_layer above, and then average across # the batch. diff = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y) with tf.name_scope('total'): cross_entropy = tf.reduce_mean(diff)tf.summary.scalar('cross_entropy', cross_entropy) 这里就像它注释里面写的，它先计算了整体的交叉熵，然后取了一下均值，最后进行记录。 123with tf.name_scope('train'): train_step = tf.train.AdamOptimizer(FLAGS.learning_rate).minimize( cross_entropy) 调用AdamOptimizer()来进行优化。 123456with tf.name_scope('accuracy'): with tf.name_scope('correct_prediction'): correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)) with tf.name_scope('accuracy'): accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))tf.summary.scalar('accuracy', accuracy) 首先，这里的y是一个行向量，所以使用tf.argmax(y, 1)将它每一行的最大值得下标找出来。同时， tf.equal()将返回一个bool值组成的tensor。 使用tf.reduce_mean()计算一下均值，就得到了当前的准确率，然后将它进行记录。 123456# Merge all the summaries and write them out to# /tmp/tensorflow/mnist/logs/mnist_with_summaries (by default)merged = tf.summary.merge_all()train_writer = tf.summary.FileWriter(FLAGS.log_dir + '/train', sess.graph)test_writer = tf.summary.FileWriter(FLAGS.log_dir + '/test')tf.global_variables_initializer().run() 将所有的日志合并，进行共同的操作。然后定义日志写入的位置。最后将变量初始化。 最后就到了训练阶段，首先定义了feed_dict函数，用来给占位符赋值， 123456789def feed_dict(train): """Make a TensorFlow feed_dict: maps data onto Tensor placeholders.""" if train or FLAGS.fake_data: xs, ys = mnist.train.next_batch(100, fake_data=FLAGS.fake_data) k = FLAGS.dropout else: xs, ys = mnist.test.images, mnist.test.labels k = 1.0 return &#123;x: xs, y_: ys, keep_prob: k&#125; 如果是训练阶段，那么就mnist.train中取出一个batch给x和y，如果不是训练阶段， 在这里肯定就是测试阶段了，那么就把整个测试集传给x和y。 下面就是训练代码： 123456789101112131415161718192021222324# Train the model, and also write summaries.# Every 10th step, measure test-set accuracy, and write test summaries# All other steps, run train_step on training data, &amp; add training summaries for i in range(FLAGS.max_steps): if i % 10 == 0: # Record summaries and test-set accuracy summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(False)) test_writer.add_summary(summary, i) print('Accuracy at step %s: %s' % (i, acc)) else: # Record train set summaries, and train if i % 100 == 99: # Record execution stats run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE) run_metadata = tf.RunMetadata() summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True), options=run_options, run_metadata=run_metadata) train_writer.add_run_metadata(run_metadata, 'step%03d' % i) train_writer.add_summary(summary, i) print('Adding run metadata for', i) else: # Record a summary summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True)) train_writer.add_summary(summary, i)train_writer.close()test_writer.close() 首先，每运行10步，就计算一下当前的准确度，并将日志数据写入test日志文件。 然后，每隔100步，加入tf.RunOptions()和tf.RunMetadata()，这好像是一些原信息，类似运行时间什么的。 剩下的步骤就是正常的训练，每次训练的日志都写入train日志文件。 最后关闭train_writer和test_writer。]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>study</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Anaconda 使用记录]]></title>
    <url>%2F2017%2F09%2F18%2F2017-09-18-anaconda%2F</url>
    <content type="text"><![CDATA[Anacanda是什么？目前把它看成一个python的发行版，它的方便之处在于它自带了很多第三方包，另外它可以方便的进行python的版本管理。 安装使用环境 window10。 官网上面下载安装即可anaconda下载。 配置环境变量将安装目录加入环境变量即可，如果之前安装配置过python，需要把之前的path给删了，不然用的还是以前的python。 打开cmd，输入python， ok！ 安装TensorFlowanaconda它本身不带TensorFlow，需要进行安装， 执行anaconda命令需要使用anaconda prompt来打开cmd（可能需要管理员身份运行，另外最好先不要开代理）。 首先更换镜像源， 12$ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/$ conda config --set show_channel_urls yes 使用conda info可以看到当前的一些配置信息， 上面可以看到源以及更换成功。 然后搜索一下TensorFlow版本， 1$ anaconda search -t conda tensorflow 这里选择了anaconda/tensorflow 1.2.1这个版本，输入下面命令， 123456789101112131415$ anaconda show anaconda/tensorflowUsing Anaconda API: https://api.anaconda.orgName: tensorflowSummary: TensorFlow is a machine learning libraryAccess: publicPackage Types: condaVersions: + 0.10.0rc0 + 1.0.1 + 1.1.0 + 1.2.1To install this package with conda run: conda install --channel https://conda.anaconda.org/anaconda tensorflow 然后就输入上面给出的conda install --channel https://conda.anaconda.org/anaconda tensorflow来进行安装， 这里速度非常慢，因为清华源上面没有这个版本，安装N次失败了。 最后换了dhirschfeld/tensorflow这个版本，终于它在清华源上面有，安装就快多了， 12345678910111213141516171819$ conda install --channel https://conda.anaconda.org/dhirschfeld tensorflowThe following NEW packages will be INSTALLED: backports.weakref: 1.0rc1-py36_0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free libprotobuf: 3.2.0-vc14_0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free [vc14] markdown: 2.6.9-py36_0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free protobuf: 3.2.0-py36_0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free tensorflow: 1.2.1-py36_0 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/freeconda-env-2.6. 100% |###############################| Time: 0:00:00 994.18 kB/svs2015_runtime 100% |###############################| Time: 0:00:01 1.27 MB/slibprotobuf-3. 100% |###############################| Time: 0:00:08 1.12 MB/sbackports.weak 100% |###############################| Time: 0:00:00 0.00 B/shtml5lib-0.999 100% |###############################| Time: 0:00:00 1.02 MB/sprotobuf-3.2.0 100% |###############################| Time: 0:00:00 808.75 kB/stensorflow-1.2 100% |###############################| Time: 0:00:12 1.72 MB/sconda-4.3.13.p 100% |###############################| Time: 0:01:52 5.10 kB/sconda-4.3.13.p 100% |###############################| Time: 0:00:48 11.97 kB/s 打开python，可以看到TensorFlow已经安装完成， 123456789101112131415$ C:\Users\龙\Desktop&gt; pythonPython 3.6.1 |Anaconda custom (64-bit)| (default, May 11 2017, 13:25:24) [MSC v.1900 64 bit (AMD64)] on win32Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; import tensorflow as tf&gt;&gt;&gt; a = tf.constant(2, name=&apos;a&apos;)&gt;&gt;&gt; b = tf.constant(3, name=&apos;b&apos;)&gt;&gt;&gt; c = a * b + b&gt;&gt;&gt; d = tf.placeholder(tf.int32, name = &apos;d&apos;)&gt;&gt;&gt; f = d + c&gt;&gt;&gt; init = tf.global_variables_initializer()&gt;&gt;&gt; with tf.Session() as session:... session.run(init)... print(session.run(f, feed_dict = &#123;d: 100&#125;))...109 同样的，使用conda list命令也可以看到安装的包中出现了TensorFlow， 1$ conda list 注：最终也没能用anaconda装上TensorFlow_1.3.0，最后用回了原始的python3.6.1，使用pip3装的TensorFlow… 参考Anaconda+Tensorflow环境安装与配置]]></content>
      <categories>
        <category>guide</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode 76 Minimum Window Substring]]></title>
    <url>%2F2017%2F09%2F15%2F2017-09-15-leetcode_76%2F</url>
    <content type="text"><![CDATA[Description: Given a string S and a string T, find the minimum window in S which will contain all the characters in T in complexity O(n). For example,S = “ADOBECODEBANC”T = “ABC”Minimum window is “BANC”. Note:If there is no such window in S that covers all characters in T, return the empty string “”. If there are multiple such windows, you are guaranteed that there will always be only one unique minimum window in S. 题目理解： 给定一个字符集合T，要找到字符串S中包含这些字符的最小子串，字符可以重复，要求复杂度为O(n)。 想了很久也不知道如何O(n)，只能想到O(mn)。 代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253class Solution &#123; private class CharInt &#123; char c; int idx; CharInt next = null; public CharInt ( char c, int idx ) &#123; this.c = c; this.idx = idx; &#125; &#125; public String minWindow(String s, String t) &#123; char[] ss = s.toCharArray(), tt = t.toCharArray(); CharInt head = new CharInt('0', -1), tail, relay, temp; relay = new CharInt(tt[0], -1); head.next = relay; for ( int i = 1; i &lt; tt.length; i++ ) &#123; relay.next = new CharInt(tt[i], -1); relay = relay.next; &#125; tail = relay; int min = Integer.MAX_VALUE, l = -1; for ( int i = 0; i &lt; ss.length; i++ ) &#123; relay = head; while ( relay.next != null ) &#123; if ( relay.next.c == ss[i] ) &#123; temp = relay.next; temp.idx = i; if ( temp != tail ) &#123; relay.next = temp.next; temp.next = null; tail.next = temp; tail = temp; &#125; int ll = head.next.idx; if ( ll != -1 ) &#123; int len = i - ll + 1; if ( len &lt; min ) &#123; min = len; l = ll; &#125; &#125; break; &#125; relay = relay.next; &#125; &#125; if ( l == -1 ) return ""; return new String(ss, l, min); &#125;&#125; 复杂度：O(mn)。运行时间：165ms。击败：3.94% 显然复杂度太高，别人都在10ms内，这代码辣鸡。 代码2： 看了别人的代码，总于明白他们是如何做到O(n)。 首先，这里的字符其实只有Ascii码里面的前128个字符，其次java里面的char是可以之间当成int来使用的， 那么这里在匹配字符的时候，就可以直接使用散列表的思想，申请一个128长的int数组就好了。 另外，即使能在O(1)的时间将一个字符匹配到它该在的位置上，那么怎么去找这么一个最小窗口又是一个大问题。 这里的思路实在是巧，首先统计字符集合之后，int[128]上就记录了每一个字符的个数。要想在O(n)的时间内完成， 那么肯定又要用到滑动窗的思路来统计。那么定义这个窗口，窗口的左边界到右边界内的字符能够满足字符集合，并且左边界一定不能再向右移动。 同30题很像，但是这里更加巧妙，只需要在这么个int[128]的数组上进行操作就行： 定义距离dis，窗口里面每少一个字符，距离加一，初始化距离为字符集合的总大小。 右边界右移，输入一个字符，int[128]上对应位置值减1，然后判断： 如果这个位置的值大于等于0，说明这个位置是集合中有的字符，并且现在窗中个数还不足，所以dis减1。 如果这个位置的值小于0，那么这个位置： 本来就不是集合中的字符，不操作。 是集合中的字符，但是窗中这个字符太多，以至于为负，同样不需要操作。 当前距离dis为0，说明窗中包含了集合中所有字符，但是可能有多的，于是尝试移动左边界，并保持dis为0： 设左边界对应字符A： 如果此时A在int[128]上对应位置值等于0，说明此字符是集合需要的字符（否则这个位置的值该为负），并且现在的个数刚刚好，那么左边界不移动。 否则int[128]上对应位置值减1。 dis等于0，尝试更新最小窗口。 通过上面的方法，代码变得非常简单，并且复杂度O(n)，额外操作也非常少，甚至空间开销都是固定的。 123456789101112131415161718192021class Solution &#123; public String minWindow(String s, String t) &#123; int[] counts = new int[128]; for ( int i = 0, bound = t.length(); i &lt; bound; i++ ) counts[t.charAt(i)]++; int dis = t.length(), min = Integer.MAX_VALUE, l = -1; for ( int i = 0, ll = 0, bound = s.length(); i &lt; bound; i++ ) &#123; counts[s.charAt(i)]--; if ( counts[s.charAt(i)] &gt;= 0 ) dis--; while ( dis == 0 &amp;&amp; counts[s.charAt(ll)] != 0) counts[s.charAt(ll++)]++; if ( dis == 0 &amp;&amp; (i - ll + 1) &lt; min ) &#123; min = i - ll + 1; l = ll; &#125; &#125; return l == -1 ? "" : s.substring(l, l + min); &#125;&#125; 复杂度：O(n)。运行时间：4ms。击败：90.25%]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>study</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode 30 Substring with Concatenation of All Words]]></title>
    <url>%2F2017%2F09%2F15%2F2017-09-15-leetcode_30%2F</url>
    <content type="text"><![CDATA[Description: You are given a string, s, and a list of words, words, that are all of the same length. Find all starting indices of substring(s) in s that is a concatenation of each word in words exactly once and without any intervening characters. For example, given:s: “barfoothefoobarman”words: [“foo”, “bar”] You should return the indices: [0,9].(order does not matter). 题目理解： 乍一看这是一个字符串匹配问题，但是其实这样想的话就完全错误了，这比字符串匹配问题要复杂多了。 还好的是它所给的单词都是一样长的，这就已经简化了很多的问题，但是它可以给重复的单词， 那么这个问题又不那么简单了，因为这些单词之间可以随意的排列组合，重复的单词会带来很多问题。 那么首先一定要去除重复，不然会对重复的单词进行重复的匹配，并且对排列组合很不利。通常想到的肯定是使用 HashMap来做，这样很快能统计出重复的单词，以及这个单词重复的次数。 因为总长度固定，那么只需要在一个固定长度的窗内去检测是否匹配就好，那么就可以将这个窗从左向右进行滑动， 来一个单词一个单词的匹配。 那么如何进行单词的匹配呢？首先想到的是字符串匹配问题，可以使用KMP算法来匹配字符串与每一个单词，记录下所有匹配的位置， 注意到每个单词都不同（去除重复之后），单词长度一样，那么它们之间匹配的位置就不会重复。 所有可以直接使用一个与字符串同样长的数组来存储这些下标。 如何查看是否匹配？这时就需要发挥想象力，首先滑动窗是肯定的，在滑动窗中统计很有技巧，这里： 设置一个总距离dis，表示窗口中单词与字典之间的距离。一个单词的距离就是1，所以初始的时候距离为字典的总单词数。 滑动窗口，从字符串上切割出一个新的单词，如果这个单词与字典中的单词不匹配，则不操作。否则匹配上单词A： 单词A现在的计数超过所需个数，距离加一。 单词A现在的计数不超过所需个数，距离减一。 滑动窗口，需要将最左端的一个单词弹出，进行与上面相反的操作。 代码一： 注：这里没有使用HashMap来除去重复。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990public class Solution &#123; private int[] matchs; public List&lt;Integer&gt; findSubstring(String s, String[] words) &#123; List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); if ( s == null || words == null || ( words[0].length()*words.length &gt; s.length() ) ) return res; if ( words[0].equals("") ) &#123; for ( int i = 0 ; i &lt;= s.length() ; i++ ) res.add(i); return res; &#125; char[] ss = s.toCharArray(); matchs = new int[ss.length]; Arrays.fill( matchs, -1 ); int[] counts = new int[words.length]; String[] words_wipe = new String[words.length]; boolean[] used = new boolean[words.length]; int counts_len = 0; for ( int i = 0; i &lt; words.length; i++ ) &#123; if ( !used[i] ) &#123; words_wipe[counts_len] = words[i]; counts[counts_len] = 1; for ( int j = i + 1; j &lt; words.length; j++ ) &#123; if ( words[i].equals(words[j]) ) &#123; counts[counts_len]++; used[j] = true; &#125; &#125; counts_len++; &#125; &#125; for ( int i = 0; i &lt; counts_len; i++ ) &#123; KMP(ss, words_wipe[i].toCharArray(), i); &#125; int step = words_wipe[0].length(), dis = words.length, total_len = dis * step; int[] sub_count = new int[counts_len]; for ( int i = 0; i &lt; step; i++ ) &#123; Arrays.fill(sub_count, 0); dis = words.length; for ( int j = i; j &lt; ss.length; j += step ) &#123; int head = j - total_len; if ( head &gt; -1 &amp;&amp; matchs[head] != -1 ) &#123; dis = sub_count[matchs[head]] &lt;= counts[matchs[head]] ? dis + 1 : dis - 1; sub_count[matchs[head]]--; &#125; if ( matchs[j] == -1 ) continue; dis = sub_count[matchs[j]] &gt;= counts[matchs[j]] ? dis + 1 : dis - 1; sub_count[matchs[j]]++; if ( dis == 0 ) res.add(j - total_len + step); &#125; &#125; return res; &#125; private void KMP(char[] t, char[] p, int widx) &#123; int[] pai = computePai(p); int match_idx = -1; for ( int i = 0; i &lt; t.length; i++ ) &#123; while ( match_idx &gt; -1 &amp;&amp; p[match_idx + 1] != t[i] ) match_idx = pai[match_idx]; if ( p[match_idx + 1] == t[i] ) match_idx++; if ( match_idx == p.length - 1 ) &#123; matchs[i - p.length + 1] = widx; match_idx = pai[match_idx]; &#125; &#125; &#125; private int[] computePai(char[] p) &#123; int[] pai = new int[p.length]; pai[0] = -1; int k = -1; for ( int idx = 1; idx &lt; p.length; idx++ ) &#123; while ( k &gt; -1 &amp;&amp; p[k+1] != p[idx] ) k = pai[k]; if ( p[k+1] == p[idx] ) k = k + 1; pai[idx] = k; &#125; return pai; &#125;&#125; 复杂度：O(mn)?运行时间：50ms击败：57.51% 代码还是很垃圾，所以和别人的差距在哪里，实在是想不出来，查看别人代码后发现，别人使用的是HashMap的Hash功能来匹配单词的， 这样通过Hash来进行两个个单词的匹配，复杂度是O(1)，额，原来Hash是这样使用的… 另外这里还要翻转一下思维，上面是用单词去匹配字符串，这里由于先建好了一个单词的HashMap，所以直接将字符串切割， 然后使用切出的单词使用HashMap查询就能知道它的次数。 12345678910111213141516171819202122232425262728293031323334353637383940414243public class Solution &#123; public List&lt;Integer&gt; findSubstring(String s, String[] words) &#123; List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); int m = s.length(), n = words.length, step = words[0].length(), bound = m - step, total_len = step * n; // Count the repeated words Map&lt; String, Integer&gt; words_list = new HashMap&lt;&gt;(n); int[][] counts = new int[2][n]; Integer index = 0, temp; for ( int i = 0 ; i &lt; n ; i++ ) &#123; if ( ( temp = words_list.get(words[i])) != null ) counts[0][temp]++; else &#123; words_list.put( words[i], index); counts[0][index++] = 1; &#125; &#125; // from s.start to s.end, match all the substring int[] matchs = new int[bound + 1]; for ( int i = 0; i &lt;= bound; i++ ) &#123; String s_sub = s.substring(i, i + step); matchs[i] = ((temp = words_list.get(s_sub)) != null) ? temp : -1; &#125; // move window from head to end, dis = 0 means the current window contains all the words for ( int i = 0; i &lt; step; i++ ) &#123; Arrays.fill(counts[1], 0); int dis = n; for ( int j = i; j &lt;= bound; j += step ) &#123; int head = j - total_len; if ( head &gt; -1 &amp;&amp; matchs[head] != -1 ) &#123; dis = counts[1][matchs[head]] &lt;= counts[0][matchs[head]] ? dis + 1 : dis - 1; counts[1][matchs[head]]--; &#125; if ( matchs[j] == -1 ) continue; dis = counts[1][matchs[j]] &gt;= counts[0][matchs[j]] ? dis + 1 : dis - 1; counts[1][matchs[j]]++; if ( dis == 0 ) res.add(j - total_len + step); &#125; &#125; return res; &#125;&#125; 复杂度：O(n)。运行时间：27ms。击败：92.37%]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>study</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Using Databases with Python 笔记]]></title>
    <url>%2F2017%2F09%2F12%2F2017-09-12-using-databases-with-python%2F</url>
    <content type="text"><![CDATA[Object Oriented Python课程首先讲了一下Python的类的定义，它也是很通常的定义， 123456789101112131415161718192021222324252627282930313233class PartyAnimal: x = 0 def __init__(self): print('I am constructed') def party(self): self.x = self.x + 1 print('So far', self.x) def __del__(self): print('I am destructed', self.x)class cat(PartyAnimal): name = 'None' def __init__(self, name): self.name = name print('I am constructed') def eat(self): print(self.name, 'eat')p = PartyAnimal()p.party()p = 102print("eeeeeeeeeeeeeeeeeeeeeeeeeeeeee")cc = cat("XIAOBAI")cc.eat()cc = 134 I am constructedSo far 1I am destructed 1eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeI am constructedXIAOBAI eatI am destructed 0 所以新建类，使用class，继承类就在后面加上(要继承的类)，也就是class name(Inheritance)。 继承会继承成员变量和成员方法，以及构造器和析构器，当然也可以重写方法，总体就是十分正常的继承规则。 Basic Structured Query Language这一部分将了基本的数据库语法，然后在python中书写sql，需要注意的是python中自带的是sqlite， 是一个轻量级的sql的库，所以它可以直接做成一个包来调用。 12345678910111213141516171819202122232425262728293031323334353637import sqlite3import reconn = sqlite3.connect('emaildb.sqlite')cur = conn.cursor()cur.execute('''DROP TABLE IF EXISTS Counts''')cur.execute('''CREATE TABLE Counts (org TEXT, count INTEGER)''')fname = input('Enter file name: ')if (len(fname) &lt; 1): fname = 'mbox-short.txt'fh = open(fname)for line in fh: if not line.startswith('From: '): continue all_org = re.findall("@(.+)\s", line) org = all_org[0] cur.execute('SELECT count FROM Counts WHERE org = ? ', (org,)) row = cur.fetchone() if row is None: cur.execute('''INSERT INTO Counts (org, count) VALUES (?, 1)''', (org,)) else: cur.execute('UPDATE Counts SET count = count + 1 WHERE org = ?', (org,))conn.commit()# https://www.sqlite.org/lang_select.htmlsqlstr = 'SELECT org, count FROM Counts ORDER BY count DESC LIMIT 10'for row in cur.execute(sqlstr): print(str(row[0]), row[1])cur.close() 上面是一个建表并且更新或增加表项的过程，这个过程就会在当前目录下新建一个emaildb.sqlite的sql文件。 需要注意的是第29行的commit()，只有在执行它之后才会将结果写入文件，所以放到循环外面执行会快很多。 Data Models and Relational SQL这一部分讲了数据库的一些使用技巧。 在使用数据库时，数据量小的时候自然是无所谓，但是在数据量很大的时候，要遵循一个原则，那就是对于经常会重复出现的字符串，我们不能让它重复，因为它这样会浪费空间。 所以存储时，对于重复出现的字符串，将它单独建立为一个表，别的表使用它的下标来进行索引。但是这个索引是我们自己做的，所以查询时要有技巧。 例如音乐的专辑和歌手。显然一个歌手平均不止一张专辑，那么就将专辑和歌手拆成两个表， 序号 歌手名 1 周杰伦 2 林俊杰 3 Avril Lavigne 序号 专辑名 歌手名 1 八度空间 1 2 范特西 1 3 第二天堂 2 4 Let go 3 5 叶惠美 1 6 Under My Skin 3 这样也就是在专辑表中，我们就节约了存储空间，更为重要的是这样可以得到数据之间清晰的关系。 那么查询的时候该怎么办呢？可以使用下面语句来查询， 1SELECT 专辑.专辑名, 歌手.歌手名 FROM 专辑 JOIN 歌手 ON 专辑.歌手名 = 歌手.序号 其中JOIN关键字的任务是将两个表合在一起查询，ON关键字限定了条件，进行筛选。 这就能查询得到下面的结果: 专辑名 歌手名 八度空间 周杰伦 范特西 周杰伦 第二天堂 林俊杰 Let go Avril 叶惠美 周杰伦 Under My Skin Avril 下面是作业中使用python脚本建立数据库的过程，注意其中的关键字IGNORE，它的作用是如果当期数据存在，那就不插入，否则插入。在这个地方十分有用，因为索引不能随意变化。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394import xml.etree.ElementTree as ETimport sqlite3conn = sqlite3.connect('trackdb.sqlite')cur = conn.cursor()# Make some fresh tables using executescript()cur.executescript('''DROP TABLE IF EXISTS Artist;DROP TABLE IF EXISTS Album;DROP TABLE IF EXISTS Track;DROP TABLE IF EXISTS Genre;CREATE TABLE Artist ( id INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE, name TEXT UNIQUE);CREATE TABLE Genre ( id INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE, name TEXT UNIQUE);CREATE TABLE Album ( id INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE, artist_id INTEGER, title TEXT UNIQUE);CREATE TABLE Track ( id INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE, title TEXT UNIQUE, album_id INTEGER, genre_id INTEGER, len INTEGER, rating INTEGER, count INTEGER);''')fname = input('Enter file name: ')if ( len(fname) &lt; 1 ) : fname = 'Library.xml'# &lt;key&gt;Track ID&lt;/key&gt;&lt;integer&gt;369&lt;/integer&gt;# &lt;key&gt;Name&lt;/key&gt;&lt;string&gt;Another One Bites The Dust&lt;/string&gt;# &lt;key&gt;Artist&lt;/key&gt;&lt;string&gt;Queen&lt;/string&gt;def lookup(d, key): found = False for child in d: if found : return child.text if child.tag == 'key' and child.text == key : found = True return Nonestuff = ET.parse(fname)all = stuff.findall('dict/dict/dict')print('Dict count:', len(all))for entry in all: if ( lookup(entry, 'Track ID') is None ) : continue name = lookup(entry, 'Name') artist = lookup(entry, 'Artist') album = lookup(entry, 'Album') count = lookup(entry, 'Play Count') rating = lookup(entry, 'Rating') length = lookup(entry, 'Total Time') genre = lookup(entry, 'Genre') if name is None or artist is None or album is None or genre is None: continue print(name, artist, album, genre, count, rating, length) cur.execute('''INSERT OR IGNORE INTO Artist (name) VALUES ( ? )''', ( artist, ) ) cur.execute('SELECT id FROM Artist WHERE name = ? ', (artist, )) artist_id = cur.fetchone()[0] cur.execute('''INSERT OR IGNORE INTO Genre (name) VALUES ( ? )''', ( genre, ) ) cur.execute('SELECT id FROM Genre WHERE name = ? ', (genre, )) genre_id = cur.fetchone()[0] cur.execute('''INSERT OR IGNORE INTO Album (title, artist_id) VALUES ( ?, ? )''', ( album, artist_id ) ) cur.execute('SELECT id FROM Album WHERE title = ? ', (album, )) album_id = cur.fetchone()[0] cur.execute('''INSERT OR REPLACE INTO Track (title, album_id, genre_id, len, rating, count) VALUES ( ?, ?, ?, ?, ?, ? )''', ( name, album_id, genre_id, length, rating, count ) )conn.commit() Many-to-Many Relationships in SQL这一节很简单，讲了数据存在多对多关系时如何存储，这时的存储方式就是将数据分别存储，然后将它们之间的关系建成一个关系表。 代码省略。 Databases and Visualization调用google的Geocoding这个API（这是google map的api，可以查询地理位置信息），获取到原始数据， 然后将数据进行一下处理，写入where.js，然后调用html脚本进行画图。 后一课程最后的第五个课程完全是一些小项目，但是他都给好了代码，相当于他带你简单过一遍这些项目的代码。 很多地方现在还理解不了，先不记录了，以后可以再回来重看。]]></content>
      <categories>
        <category>coursera</category>
        <category>python for everybody</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode 41 First Missing Positive]]></title>
    <url>%2F2017%2F09%2F10%2F2017-09-10-leetcode_41%2F</url>
    <content type="text"><![CDATA[Description: Given an unsorted integer array, find the first missing positive integer. For example,Given [1,2,0] return 3,and [3,4,-1,1] return 2. Your algorithm should run in O(n) time and uses constant space. 题目理解：问题倒是十分的简单，所以这道题hard的原因在于固定的额外空间，也就是要原址操作。 如果不要求原址操作，那就是easy，但是原址操作的确也是不太好想，最后一瞬间想到了散列表，就想到这题的解法了， 也就是将一个数放到它应该在的位置上，比如数字5，那么它就该在数组的4位置上，那么通过直接索引加上交换， 就能实现题目要求。 PS:之前写的没有考虑固定空间，辣鸡。 代码： 123456789101112131415161718public class Solution &#123; public int firstMissingPositive(int[] nums) &#123; int right = nums.length - 1, i = 0; while ( i &lt;= right ) &#123; int idx = nums[i] - 1; if ( idx == i ) &#123; i++; &#125; else if ( idx &gt; right || idx &lt; 0 || nums[idx] == nums[i] ) &#123; nums[i] = nums[right]; right--; &#125; else &#123; nums[i] = nums[idx]; nums[idx] = idx + 1; &#125; &#125; return right + 2; &#125;&#125; 额外空间只使用了三个int，循环每次都操作一个数，或是放到最后，或者正确位置，并且同一个位置最多会被交换一次，所以复杂度是O(n)的。 复杂度：O(n)。运行时间：12ms。击败：56.77%]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>study</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode 37 Sudoku Solver]]></title>
    <url>%2F2017%2F09%2F09%2F2017-09-09-leetcode_37%2F</url>
    <content type="text"><![CDATA[Description: Write a program to solve a Sudoku puzzle by filling the empty cells. Empty cells are indicated by the character &#39;.&#39;. You may assume that there will be only one unique solution. excemple: solve: 题目理解： 数独应该每个人都玩过，它规则很简单，但是做起来很难，就是因为它可能性太多了。 那么这个问题要求解数独问题，首先肯定是需要使用递归来解，然后就可以想能不能动态规划，好像不能，那么就只能强行递归，复杂度不想计算，肯定是指数级，但是它是 9x9的格子，所以还好。 代码： 这里其实挺考验的，如何组织数据是一个大问题，直接影响到整个代码的结构，一定要想得简单一点。 下面代码也是进过很多次调整的，思路比最开始清晰多了。 另外，一个清晰的思路十分重要，不要上来就写，要先想清楚。 代码虽然很长，但是思路还是很清晰的。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091public class Solution &#123; /** * 第一，计算需要填写的格子的剩余可用的格子 * 第二，取出当前操作数最少的格子 * 第三，递归 */ /** * 负责初始化 */ public void solveSudoku(char[][] board) &#123; boolean[][] r_used = new boolean[9][9]; // 行已使用的数字 boolean[][] c_used = new boolean[9][9]; // 列已使用的数字 boolean[][][] b_used = new boolean[3][3][9]; // 3x3 的大格子已使用的数字 List&lt;int[]&gt; ems = new ArrayList&lt;&gt;(); // 空的格子的下标 for ( int i = 0; i &lt; 9; i++ ) &#123; for ( int j = 0; j &lt; 9; j++ ) &#123; if ( board[i][j] == '.' ) &#123; int[] empty = &#123;i, j, 0&#125;; // 行 + 列 + 它可以使用的数字的个数 ems.add(empty); &#125; else &#123; int num = board[i][j] - 49; // char 转 int 这里多减一个 1 ， 因为 '1' 对应 0 下标 myAdjuster( i, j, num, true, r_used, c_used, b_used ); &#125; &#125; &#125; int min_idx = findMin( ems, r_used, c_used, b_used ); mySolver ( board, ems, r_used, c_used, b_used, min_idx); &#125; /** * 递归主体，对输入的 ems.get(idx) 这个格子填数字 */ private boolean mySolver ( char[][] bd, List&lt;int[]&gt; ems, boolean[][] r_used, boolean[][] c_used, boolean[][][] b_used, int idx ) &#123; if ( idx == -1 ) return true; int[] em = ems.get(idx); ems.remove(em); // 将这个格子先移除集合，因为现在要填数字 int i = em[0], j = em[1]; for ( int num = 0; num &lt; 9; num++ ) &#123; if ( !r_used[i][num] &amp;&amp; !c_used[j][num] &amp;&amp; !b_used[i/3][j/3][num] ) &#123; // 可以使用的数字 myAdjuster( i, j, num, true, r_used, c_used, b_used ); // 调整所使用的数字 int min_idx = findMin( ems, r_used, c_used, b_used ); // 找到下一个最小 if ( mySolver ( bd, ems, r_used, c_used, b_used, min_idx) ) &#123; // 递归 bd[i][j] = (char)(num + 49); // 成功表示问题已经解决 return true; &#125; myAdjuster( i, j, num, false, r_used, c_used, b_used ); // 恢复调整 &#125; &#125; ems.add(em); // 添加回集合 return false; &#125; /** * 调整或者恢复 使用的下标 */ private void myAdjuster( int i, int j, int num, boolean add, boolean[][] r_used, boolean[][] c_used, boolean[][][] b_used ) &#123; if (add) &#123; r_used[i][num] = true; c_used[j][num] = true; b_used[i/3][j/3][num] = true; &#125; else &#123; r_used[i][num] = false; c_used[j][num] = false; b_used[i/3][j/3][num] = false; &#125; &#125; /** * 找到最小可操作的格子坐标 */ private int findMin( List&lt;int[]&gt; ems, boolean[][] r_used, boolean[][] c_used, boolean[][][] b_used ) &#123; for ( int[] em : ems ) &#123; int i = em[0], j = em[1]; em[2] = 0; // 将这个格子的计数先置零 for ( int num = 0; num &lt; 9; num++ ) &#123; if ( !r_used[i][num] &amp;&amp; !c_used[j][num] &amp;&amp; !b_used[i/3][j/3][num] ) // 使用可用这个数字 em[2]++; &#125; &#125; int min = 1000, idx = -1; for ( int i = 0; i &lt; ems.size(); i++ ) &#123; // 找到最小 int[] em = ems.get(i); if ( em[2] &lt; min ) &#123; min = em[2]; idx = i; &#125; &#125; return idx; &#125;&#125; 运行时间：8ms。复杂度：指数。击败：94.23% 代码二： 上面的代码每次去找最下操作的格子的时候显得比较复杂，那么将更新与格子的可操作数维护放在一起做，就得到下面的代码， 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104public class Solution &#123; /** * 第一，计算需要填写的格子的剩余可用的格子 * 第二，取出当前操作数最少的格子 * 第三，递归 */ private class Ep &#123; boolean[] used = new boolean[9]; int count = 9; int row, col, block; public Ep(int row, int col, int block) &#123; this.row = row; this.col = col; this.block = block; &#125; public boolean set(int num, boolean flag) &#123; if ( used[num] != flag ) &#123; count = true == flag ? count - 1 : count + 1; used[num] = flag; return true; &#125; return false; &#125; &#125; /** * 负责初始化 */ public void solveSudoku(char[][] board) &#123; List&lt;Ep&gt; ems = new ArrayList&lt;&gt;(); // 空的格子的下标 for ( int i = 0; i &lt; 9; i++ ) &#123; for ( int j = 0; j &lt; 9; j++ ) &#123; if ( board[i][j] == '.' ) &#123; Ep empty = new Ep(i, j, (i/3)*3 + j/3 + 1); // 行 + 列 + 格子 ems.add(empty); &#125; &#125; &#125; int min_idx = -1; boolean[] temp = new boolean[ems.size()]; for ( int i = 0; i &lt; 9; i++ ) &#123; for ( int j = 0; j &lt; 9; j++ ) &#123; if ( board[i][j] != '.' ) &#123; int num = board[i][j] - 49; // char 转 int 这里多减一个 1 ， 因为 '1' 对应 0 下标 min_idx = myAdjuster( ems, i, j, num, true , temp); &#125; &#125; &#125; mySolver ( board, ems, min_idx); &#125; /** * 递归主体，对输入的 ems.get(idx) 这个格子填数字 */ private boolean mySolver ( char[][] bd, List&lt;Ep&gt; ems, int idx ) &#123; if ( idx == -1 ) return true; Ep em = ems.get(idx); ems.remove(idx); // 将这个格子先移除集合，因为现在要填数字 boolean[] adjust = new boolean[ems.size()]; for ( int num = 0; num &lt; 9; num++ ) &#123; if ( false == em.used[num] ) &#123; // 可以使用的数字 int min_idx = myAdjuster( ems, em.row, em.col, num, true, adjust ); // 调整所使用的数字 if ( mySolver ( bd, ems, min_idx) ) &#123; // 递归 bd[em.row][em.col] = (char)(num + 49); // 成功表示问题已经解决 return true; &#125; myAdjuster( ems, em.row, em.col, num, false, adjust ); // 恢复调整 &#125; &#125; ems.add(idx, em); // 添加回集合 return false; &#125; /** * 调整或者恢复 使用的下标 */ private int myAdjuster( List&lt;Ep&gt; ems, int i, int j, int num, boolean flag, boolean[] adjust) &#123; if ( flag == true ) &#123; int block = (i/3)*3 + j/3 + 1; int min = Integer.MAX_VALUE, idx = -1; for ( int ii = 0, bound = ems.size(); ii &lt; bound; ii++ ) &#123; Ep em = ems.get(ii); if ( em.row == i || em.col == j || em.block == block ) adjust[ii] = em.set(num, flag); if ( min &gt; em.count ) &#123; min = em.count; idx = ii; &#125; &#125; return idx; &#125; else &#123; for ( int ii = 0; ii &lt; adjust.length; ii++ ) &#123; if ( adjust[ii] ) &#123; ems.get(ii).set(num, false); adjust[ii] = false; &#125; &#125; return -1; &#125; &#125;&#125; 上面的维护操作直接与格子绑定在一起，将它建立成为一个格子对象，这样耗费了更多的空间，换来的就是可以维护格子的操作数。 复杂度：指数。运行时间：4ms。击败：99.15%]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>study</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Using Python to Access Web Data 笔记]]></title>
    <url>%2F2017%2F09%2F08%2F2017-09-08-using-python-to-access-web-data%2F</url>
    <content type="text"><![CDATA[前两个子课程的笔记就不记了，因为非常的基础。 Regular Expressions正则表达式之前就没好好学，这里正好可以学一下。 首先python中的正则表达式和java中基本一致，也可以说所有编程语言中的正则表达式其实都是差不多的，还可以说正则表达式是一个独立的特征，这些语言都要支持这个特征。所有学习的是哪一门语言的正则表达式并不关键。 首先，官方文档Regular Expression HOWTO。 课程资料中所列： character mean ^ Matches the beginning of a line $ Matches the end of the line . Matches any character \s Matches whitespace \S Matches any non-whitespace character * Repeats a character zero or more times *? Repeats a character zero or more times (non-greedy) + Repeats a character one or more times +? Repeats a character one or more times (non-greedy) [aeiou] Matches a single character in the listed set XYZ Matches a single character not in the listed set [a-z0-9] The set of characters can include a range ( Indicates where string extraction is to start ) Indicates where string extraction is to end 使用正则表达式需要导包： 1import re 函数以及语法直接看官方文档吧,Regular expression operations 直接上题： 题目：extract all the numbers in the file and compute the sum of the numbers 代码： 12345678910111213import refile = open('test.txt') # 测试文件one_line = file.read() # 把文件读成一行nums = re.findall('[^0-9]*([0-9]+)[^0-9]*', one_line) # 取出所有整数print(len(nums))total = 0for num in nums: total += int(num) print('$',num)print(total) 这里的正则表达式为[^0-9]*([0-9]+)[^0-9]*，也就是中间数字，前后存在除了数字之外的任意字符。 虽然上面的逻辑十分简单并且正确，但是最开始我并不是这样写的，我最开始想的是[^0-9]([0-9]+)[^0-9]， 数字前后一定存在一个非数字字符，but对于例子： 12345678Why should you learn to write programs? 774612 1929 8827Writing programs (or programming) is a very creative7 and rewarding activity. You can write programs formany reasons, ranging from making your living to solving8837 a difficult data analysis problem to having fun to helping 128someone else solve a problem. This book assumes thateveryone needs to know how to program ... 输出为： 5$ 7746$ 1929$ 7$ 8837$ 12818647 正常输出应该为： 7$ 7746$ 12$ 1929$ 8827$ 7$ 8837$ 12827486 那么这里就少了12以及8827。难道这里是因为在匹配7746的时候，匹配的是7746\n，所以到了匹配12的时候，前面的\n被匹配了，所以它不能再使用。同样的，匹配1929时，它匹配的是1929，导致后面8827前面没有字符能够匹配了？ 可能在findall()的时候就是使用的字符不再使用吧… 未完待续… Networks and Sockets感觉我自己是个假的学网络的，对于网络的了解还是太窄太片面了，以后恶补一下。 在python里面使用socket十分的简单， 下面的代码就能访问课程提供的一个网页，并且将这个html文件给打印出来， 1234567891011121314import socketmysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)mysock.connect(('data.pr4e.org', 80))cmd = 'GET http://data.pr4e.org/intro-short.txt HTTP/1.0\r\n\r\n'.encode()mysock.send(cmd)while True: data = mysock.recv(512) if (len(data) &lt; 1): break print(data.decode())mysock.close() 这里直接使用域名就能进行连接（之前学习socket都是用的ip，毕竟局域网，然而现在学的都退回去了）， 80端口就是web端口，然后可以直接加上文件来进行访问，这后面的HTTP/1.0\r\n\r\n现在还不能明白。 encode()和decode()到底进行了什么操作也还不能明白。 另外注意这里的域名协议是http，它和https好像是有一些区别的。 Programs that Surf the Web这一节主要讲了一下python与网络中的一些编码。 首先解释了为什么在上面的socket中，要在发出request之前进行encode()，在接收信息后要先进行decode()。 第一，现在网络编码绝大多数都是UTF-8，甚至可以默认一定是它，毕竟UTF-8包含了所有的常用语言文字。 第二，在python内部，所有的字符都是unicode编码，那么问题就来了，unicode与UTF-8并不能直接通用，所有在字符出入的时候就要加上encode()与decode()。 关于UTF-8与unicode。 首先快速参考一下知乎Unicode 和 UTF-8 有何区别? 首先，unicode是为了解决之前编码只考虑英文字符的问题而出现的，因为以前的ascii码只使用一个字节来表示字符，所以它最多也只能表示256个字符， 如果只使用英文是够用的，但是事实是世界要发展，所以出现各种字符，那么它就不够用了。于是就出现了许多的编码方式，例如中国就出了GBK编码。 但是这是不适于国家与国家之间的交流的，于是ISO就指定了unicode这个编码标准。 unicode现在通常是使用两个字节来表示一个字符，但是其实它可以被看成一个字符集，它将所有字符都定义了一个唯一的ID，这样网络就能有一个统一的字符表，不再出现之前的需要相互转化的问题。 那么新的问题就是直接使用unicode来表示字符时，它有时候会浪费空间，在编码表中靠前的字符，例如英文字符，它前一字节就是0000，后一字节才是它真正的序号。 于是，在网络传输中，由于网络带宽并没有这么的理想，大家肯定就会嫌弃unicode编码浪费带宽，所以，于1992年创建，由Ken Thompson创建了UTF传输标准，它的全名是Unicode Transformation Format，这个全名就能明白了UTF的意思。 UTF是针对unicode的一种网络传输标准，按照我们通信的人来说，它就是一种针对unicode的编码方式。 它现在有UTF-7、UTF-7.5、UTF-8、UTF-16、UTF-32几种格式，当然现在最为流行的就是其中的UTF-8。 它是一种变长的编码方式，这样就能减少网络传输中的数据量，所以在网络传输中，大家都用它。 下面直接看作业代码： 作业的目标是将一个网页中的span标签中的整数进行求和，当然可以使用socket读取到网页，然后再使用正则表达式来去到整数。 但是这个作业规定使用BeautifulSoup来进行这个过程，它能够直接把网页给解析了，不过这个不是BeautifulSoup的功能， 在python中原有就有支持这个功能的函数，BeautifulSoup主要是将网页中一些不规范的表达的地方的雷点给你踩了。也就是它总结了网页中很多不规范的奇葩写法，然后过滤这些，使得Soup更加好喝。 另外BeautifulSoup是一个额外的模块，可以使用pip install BeautifulSoup来安装。 注： 主要是html语言太强了，很多奇葩的写法，它也不报错… 12345678910111213141516171819202122232425262728from urllib.request import urlopenfrom bs4 import BeautifulSoupimport ssl# Ignore SSL certificate errorsctx = ssl.create_default_context()ctx.check_hostname = Falsectx.verify_mode = ssl.CERT_NONEurl = input('Enter - ')html = urlopen(url, context=ctx).read()# html.parser is the HTML parser included in the standard Python 3 library.# information on other HTML parsers is here:# http://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parsersoup = BeautifulSoup(html, "html.parser")# Retrieve all of the anchor tagstags = soup('span')count = 0sum = 0for tag in tags: # Look at the parts of a tag sum += int(tag.contents[0]) count += 1print("Count", count)print("Sum", sum) 代码就是调用方式，网页http://py4e-data.dr-chuck.net/comments_29102.html。 其中格式为&lt;tr&gt;&lt;td&gt;Modu&lt;/td&gt;&lt;td&gt;&lt;span class=&quot;comments&quot;&gt;90&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;。 这里就是去取其中的90。 代码二： 123456789101112131415161718192021222324import urllib.request, urllib.parse, urllib.errorfrom bs4 import BeautifulSoupimport ssl# Ignore SSL certificate errorsctx = ssl.create_default_context()ctx.check_hostname = Falsectx.verify_mode = ssl.CERT_NONEurl = input('Enter URL: ')c = input('Enter count: ')p = input('Enter position: ')count = int(c)posi = int(p)for i in range(count+1): print("Retrieving: ", url) html = urllib.request.urlopen(url, context=ctx).read() soup = BeautifulSoup(html, 'html.parser') # Retrieve all of the anchor tags tags = soup('a') url = tags[posi-1].get('href', None) 输入链接，跳转次数，从第几个链接跳转，这就像一个网络爬虫一样。 具体的细节完全都不懂，现在只知道表象。 Web Services and XML这一节将的主要是XML语言，以及python对XML的一个解析，内容很简单。 直接上代码: 123456789101112131415161718192021222324import urllib.request, urllib.parse, urllib.errorimport xml.etree.ElementTree as ETaddress = input('Enter location: ')if len(address) &lt; 1: print('invalid location! Enter a location like \"http://py4e-data.dr-chuck.net/comments_42.xml\"') exit()url = addressprint('Retrieving', url, '...')uh = urllib.request.urlopen(url)data = uh.read()print('Retrieved', len(data), 'characters')# print(data.decode())tree = ET.fromstring(data)items = tree.findall('.//count')print("Count", len(items))total = 0for item in items: total += int(item.text)print("Sum:", total) 输入一个地址，使用urllib库来进行连接，然后调用xml.etree.ElementTree来解析其中的data， 解析得到的就是一个XML树结构。然后将其中所有count节点的值加起来就行。 JSON and the REST Architecture这一节其实主要讲了JSON的解析，它和XML类似，但是JSON更加轻量级，更加简单，可以使用python解析得到字典或者列表等数据结构。 123456789101112131415161718192021222324252627282930import urllib.request, urllib.parse, urllib.errorimport json# Note that Google is increasingly requiring keys# for this APIserviceurl = 'http://py4e-data.dr-chuck.net/geojson?'while True: address = input('Enter location: ') if len(address) &lt; 1: break url = serviceurl + urllib.parse.urlencode( &#123;'address': address&#125;) print('Retrieving', url) uh = urllib.request.urlopen(url) data = uh.read().decode() print('Retrieved', len(data), 'characters') try: js = json.loads(data) except: js = None if not js or 'status' not in js or js['status'] != 'OK': print('==== Failure To Retrieve ====') print(data) continue print("Place_id", js["results"][0]["place_id"]) 这里使用了google提供的api，来查询到输入地址的位置信息。当然这里实际上调用的是py4e的接口， 因为地址的位置信息可能会变，为了批改作业，它就把它改成了py4e的接口，这样就不会有变动。 可以看到json的解析十分简单，这是它的一大优势。]]></content>
      <categories>
        <category>coursera</category>
        <category>python for everybody</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode 44 Wildcard Matching]]></title>
    <url>%2F2017%2F09%2F06%2F2017-09-06-leetcode_44%2F</url>
    <content type="text"><![CDATA[Implement wildcard pattern matching with support for ‘?’ and ‘*‘. ‘?’ Matches any single character.‘*‘ Matches any sequence of characters (including the empty sequence). The matching should cover the entire input string (not partial). The function prototype should be:bool isMatch(const char *s, const char *p) Some examples:isMatch(“aa”,”a”) → falseisMatch(“aa”,”aa”) → trueisMatch(“aaa”,”aa”) → falseisMatch(“aa”, “*“) → trueisMatch(“aa”, “a*“) → trueisMatch(“ab”, “?*“) → trueisMatch(“aab”, “c*a*b”) → false 题目理解： 经典的字符串匹配问题。 首先想到的方法，顺着走，但是第一次写得非常复杂，这里忽略不计（但是速度还可以）。 今天所想的方法，动态规划，自顶向下，或者自底向上。 看了别人的顺着走的算法，原来思路也是很简单的。 所以不是看到这种问题就想着动态规划，也许顺着想就能简单解决。 另外，这道题的java运行时间十分不稳定，重复提交可以差30ms。 代码一： 自顶向下。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class Solution &#123; public boolean isMatch(String s, String p) &#123; char[] ss = s.toCharArray(), pp = p.toCharArray(); // 为了先合并连续的'*' char[] ppp = new char[pp.length]; int ppp_idx = 0; boolean last_not_star = true; for ( int i = 0; i &lt; pp.length; i++ ) &#123; if ( pp[i] == '*' ) &#123; if ( last_not_star ) ppp[ppp_idx++] = pp[i]; last_not_star = false; &#125; else &#123; last_not_star = true; ppp[ppp_idx++] = pp[i]; &#125; &#125; char[] pppp = new char[ppp_idx]; for ( int i = 0; i &lt; ppp_idx; i++ ) pppp[i] = ppp[i]; pp = pppp; char[][] matchs = new char[ss.length + 1][pp.length + 1]; for ( int i = 0; i &lt; ss.length; i++ ) matchs[i][pp.length] = '0'; matchs[ss.length][pp.length] = '1'; return myMatch( ss, pp, 0, 0, matchs) == '1'; &#125; private char myMatch(char[] a, char[] b, int ai, int bi, char[][] matchs) &#123; if ( matchs[ai][bi] != '\0' ) return matchs[ai][bi]; if ( ai == a.length ) &#123; if ( b[bi] == '*' ) matchs[ai][bi] = myMatch(a, b, ai, bi + 1, matchs); else matchs[ai][bi] = '0'; return matchs[ai][bi]; &#125; if ( b[bi] == '?' ) &#123; matchs[ai][bi] = myMatch(a, b, ai + 1, bi + 1, matchs); &#125; else if ( b[bi] == '*' ) &#123; matchs[ai][bi] = myMatch(a, b, ai, bi + 1, matchs); if ( matchs[ai][bi] != '1' ) matchs[ai][bi] = myMatch(a, b, ai + 1, bi, matchs); &#125; else &#123; if ( a[ai] == b[bi] ) matchs[ai][bi] = myMatch(a, b, ai + 1, bi + 1, matchs); else matchs[ai][bi] = '0'; &#125; return matchs[ai][bi]; &#125; &#125; 复杂度：O(mn)？？？运行时间：大约100ms。击败：大约16.73% 代码二： 自底向上。 1234567891011121314151617181920212223242526public class Solution &#123; public boolean isMatch(String s, String p) &#123; char[] ss = s.toCharArray(), pp = p.toCharArray(); int m = ss.length, n = pp.length; boolean[][] matchs = new boolean[m + 1][n + 1]; for ( int i = 0; i &lt;= m; i++ ) matchs[i][0] = false; for ( int i = 0; i &lt; n &amp;&amp; pp[i] == '*'; i++ ) matchs[0][i+1] = true; matchs[0][0] = true; for ( int i = 1; i &lt;= m; i++ ) &#123; for ( int j = 1; j &lt;= n; j++ ) &#123; int ii = i - 1, jj = j - 1; if ( pp[jj] == '?' ) matchs[i][j] = matchs[ii][jj]; else if ( pp[jj] == '*' ) matchs[i][j] = matchs[ii][j] || matchs[i][jj]; else &#123; if ( matchs[ii][jj] ) matchs[i][j] = (ss[ii] == pp[jj]); &#125; &#125; &#125; return matchs[m][n]; &#125; &#125; 复杂度：O(mn)。运行时间：大约75ms。击败：大约50% 代码三： 顺序扫描。 12345678910111213141516171819202122public class Solution &#123; public boolean isMatch(String s, String p) &#123; char[] ss = s.toCharArray(), pp = p.toCharArray(); int m = ss.length, n = pp.length, i = 0, j = 0, star_idx = -1, match = -1; while ( i &lt; m ) &#123; if ( j &lt; n &amp;&amp; pp[j] == '*' ) &#123; match = i; star_idx = j; j++; &#125; else if ( j &lt; n &amp;&amp; ( pp[j] == '?' || pp[j] == ss[i] ) ) &#123; i++; j++; &#125; else if ( star_idx != -1 ) &#123; i = match++; j = star_idx + 1; &#125; else return false; &#125; while ( j &lt; n &amp;&amp; pp[j] == '*' ) j++; return j == n; &#125;&#125; 复杂度：应该介于O(mn)到O(m + n)之间。运行时间： 大约60ms。击败：大约80%]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>study</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode 72 Edit Distance]]></title>
    <url>%2F2017%2F09%2F06%2F2017-09-06-leetcode_72%2F</url>
    <content type="text"><![CDATA[Description: Given two words word1 and word2, find the minimum number of steps required to convert word1 to word2. (each operation is counted as 1 step.) You have the following 3 operations permitted on a word:a) Insert a characterb) Delete a characterc) Replace a character 题目理解，就是求将第一个字符串转化到第二个字符串所需要的最少步数，可以使用插入、删除、替换，三种策略。 这其实就是算法导论第15章，动态规划的思考题15-5，但是这道题会更加简单， 因为书上还有旋转操作。反正就是说这道题如果不使用动态规划，那么一定是过不了的。 思路一：先写一个递归算法，将它转化为自顶向上的动态规划。 思路二：自底向上。 代码一： 先考虑递归的情况，也就是取两个字符串的头字符$a_0$和$b_0$比较，然后按照： $a_0$和$b_0$匹配，向下递归$a_1$和$b_1$。 $a_0$和$b_0$不匹配， 使用replace，也就是将$a_0$替换为$b_0$，向下递归$a_1$和$b_1$。 使用Insert，也就是在$a_0$前面插入$b_0$，向下递归$a_0$和$b_1$。 使用Delete，也就是删除$a_0$，向下递归$a_1$和$b_0$。 上面的递归就可以求出少的操作次数，但是这样的递归将是指数复杂度的，所以更改其变为动态规划，减少子问题的重复计算。 123456789101112131415161718192021222324252627282930class Solution &#123; public int minDistance(String word1, String word2) &#123; char[] a = word1.toCharArray(), b = word2.toCharArray(); int m = a.length, n = b.length; int[][] dis = new int[m + 1][n + 1]; for ( int i = 0; i &lt; m; i++ ) Arrays.fill(dis[i], -1); for ( int i = 0; i &lt;= m; i++ ) dis[i][n] = m - i; for ( int j = 0; j &lt;= n; j++ ) dis[m][j] = n - j; return myDis( a, b, 0, 0, dis ); &#125; private int myDis( char[] a, char[] b, int i, int j, int[][] dis ) &#123; if ( dis[i][j] != -1 ) return dis[i][j]; if ( a[i] == b[j] ) &#123; dis[i][j] = myDis( a, b, i + 1, j + 1, dis ); &#125; else &#123; int dis1 = myDis( a, b, i, j + 1, dis ); int dis2 = myDis( a, b, i + 1, j, dis ); int dis3 = myDis( a, b, i + 1, j + 1, dis ); int min = dis1 &lt; dis2 ? dis1 : dis2; min = min &lt; dis3 ? min : dis3; dis[i][j] = min + 1; &#125; return dis[i][j]; &#125;&#125; 复杂度O(mn)。运行时间8ms。击败97.43% 代码二： 自底向上。 对照P223，定理15的定义法，先清晰一下思路。 令$a=(a_1,a_2,…,a_m)$和$b=(b_1,b_2,…,b_n)$两个序列，将第一个序列转化到第二个，那么： 如果$a_m == b_n$，则次数等于$(a(m-1),b(n-1))$。 如果$a_m \neq b_n$，则次数等于$(a(m-1),b(n-1))$与$(a(m),b(n-1))$与$(a(m-1),b(n))$的最小值加一。 1234567891011121314151617181920212223242526272829class Solution &#123; public int minDistance(String word1, String word2) &#123; char[] a = word1.toCharArray(), b = word2.toCharArray(); int m = a.length, n = b.length; int[][] dis = new int[m + 1][n + 1]; for ( int i = 0; i &lt;= m; i++ ) dis[i][0] = i; for ( int j = 0; j &lt;= n; j++ ) dis[0][j] = j; for ( int i = 1; i &lt;= m; i++ ) &#123; for ( int j = 1; j &lt;= n; j++ ) &#123; if ( a[i - 1] == b[j - 1] ) &#123; dis[i][j] = dis[i-1][j-1]; &#125; else if ( dis[i-1][j] &gt; dis[i][j-1] ) &#123; if ( dis[i][j-1] &gt; dis[i-1][j-1] ) dis[i][j] = dis[i-1][j-1] + 1; else dis[i][j] = dis[i][j-1] + 1; &#125; else &#123; if ( dis[i-1][j] &gt; dis[i-1][j-1] ) dis[i][j] = dis[i-1][j-1] + 1; else dis[i][j] = dis[i-1][j] + 1; &#125; &#125; &#125; return dis[m][n]; &#125; &#125; 复杂度O(mn)。运行时间10ms。击败92.12% 注： 自顶向上比自底向上快？？？想了一下，应该是因为在自顶向下的过程中，并不需要去将所有的子问题都计算出来。]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>study</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Introduction to ML Strategy]]></title>
    <url>%2F2017%2F09%2F06%2F2017-09-06-coursera-ml-strategy%2F</url>
    <content type="text"><![CDATA[Deep_Learning第三个子课程《Introduction to ML Strategy》笔记。这一子课程一共分了2周的课程，没有讲实质的算法，讲的是在实际中如果去解决一个机器学习问题， 或者说是在实现算法过程中，该如何一步一步去提升算法，它又应该基于一个怎么样的策略。 实话说，这一部分听着比较无聊，一是因为自己没有这样的一个经历，不能感同身受，二是这一部分主要是一个intuition上的东西， 比较的玄，三是这一部分确实有一点无聊。 由于以上原因，这一部分笔记先简单记一下，待到之后或许会重新来看一遍视频。 正交化意思是参数的正交化，也就是尽量使得要调整的参数正交化，这样参数调整起来才不会互相影响。（暂时并不能太理解具体的操作） 模型的目标对于整个机器学习系统来说，提高算法精度是肯定的，但是还可能需要考虑到很多其它的问题，例如计算速度、内存占用、假真率啊等等。 有的目标是越小/大越好，有的是只要满足就行，这就叫做Optimizing metric and Satisficing。 另外要注意到train set与dev set以及test set它们之间的分布关系，如果分布区别很大，或者说训练的目标就不是我们本来的目标的话（不同的分布导致不同的目标）， 显然算法的表现将不会很好。 Avoidable bias and variance首先对于一个机器学习问题，它存在着一个精度的上限，叫做Bayes Error，它从数学上证明了这个问题它的一个精度上限，没有任何算法或者人能超过它。 第二在这么一个问题上，可能有一个人类的精度上限human-level performance，代表着直接让人来做的误差。 然后在train set上的误差就是training error，dev set上的误差就把它叫做dev error。 那么，对于Bayes Error，没法直接知道，所以通常使用human-level performance来表示我们算法想要达到的一个精度目标， 所以training error与human-level performance之间的差值就叫做avoidable bias，也就是算法还差多少拟合能力。 training error与dev set之间的差值就叫做variance，表示模型的泛化能力。 这两个精度差就可以指示我们，当前的模型问题出在哪个地方。 后一周不好总结，需要时可以再去看一遍…]]></content>
      <categories>
        <category>coursera</category>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>study</tag>
        <tag>mechine learning</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode 10 Regular Expression Matching]]></title>
    <url>%2F2017%2F09%2F05%2F2017-09-05-leetcode_10%2F</url>
    <content type="text"><![CDATA[Description: ‘.’ Matches any single character.‘*‘ Matches zero or more of the preceding element. The matching should cover the entire input string (not partial). The function prototype should be:bool isMatch(const char *s, const char *p) Some examples:isMatch(“aa”,”a”) → falseisMatch(“aa”,”aa”) → trueisMatch(“aaa”,”aa”) → falseisMatch(“aa”, “a*“) → trueisMatch(“aa”, “.*“) → trueisMatch(“ab”, “.*“) → trueisMatch(“aab”, “c*a*b”) → true 题目理解：给两个字符串，用第二个字符串去匹配第一个字符串，意思就是第一个字符串中的字符不转义， 只有第二个字符串才需要转义。 需要注意的是，一个字符后面如果跟上*，那么这个字符一定要转义，也就是例如a*与a*不能匹配，因为第一个a*不转义， 第二个a*转义，所以第一个a*中的*将匹配不上。 思路：几个月之前，显然不知道动态规划，就直接递归，非常二。当然也能击败一半人… 刚看了动态规划，这个问题典型的动态规划问题，使用自顶向下的策略，速度得到提升。 代码一：直接递归版本 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class Solution &#123; public boolean isMatch(String s, String p) &#123; char[] s_array = s.toCharArray(); char[] p_array = p.toCharArray(); int si = 0; int pi = 0; while ( pi &lt; p_array.length ) &#123; if ( pi + 1 != p_array.length &amp;&amp; p_array[ pi + 1 ] == '*' ) &#123; int si_back = s_array.length - 1; int pi_back = p_array.length - 1; while ( pi_back &gt; pi + 1 &amp;&amp; si_back &gt;= si ) &#123; if ( p_array[pi_back] == '*' ) break; if ( p_array[pi_back] != s_array[si_back] &amp;&amp; p_array[pi_back] != '.' ) return false; pi_back--; si_back--; &#125; pi_back++; si_back++; String p_sub = p.substring( pi + 2 , pi_back ); if ( p_array[pi] == '.' ) &#123; while ( si &lt;= si_back ) &#123; if ( isMatch( s.substring( si , si_back ) , p_sub ) ) return true; si++; &#125; return false; &#125; else &#123; while ( si &lt; si_back &amp;&amp; s_array[si] == p_array[pi] ) &#123; if ( isMatch( s.substring( si , si_back ) , p_sub ) ) return true; si++; &#125; return isMatch( s.substring( si , si_back ) , p_sub ); &#125; &#125; else &#123; if ( p_array[pi] == '.' ) &#123; if ( si == s_array.length ) return false; pi++; si++; &#125; else &#123; if ( si == s_array.length || s_array[si] != p_array[pi] ) return false; pi++; si++; &#125; &#125; &#125; return si == s_array.length; &#125;&#125; 复杂度：指数级别。运行时间：33ms。击败：44.25%。 代码二：动态规划（自顶向下） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class Solution &#123; public boolean isMatch(String s, String p) &#123; char[] ss = s.toCharArray(); char[] pp = p.toCharArray(); char[][] matchs = new char[ss.length + 1][pp.length + 1]; for ( int i = 0; i &lt; ss.length; i++ ) matchs[i][pp.length] = '0'; matchs[ss.length][pp.length] = '1'; return myMatch( matchs, ss, pp, 0, 0) == '1'; &#125; private char myMatch( char[][] matchs, char[] s, char[] p, int si, int pi ) &#123; if ( matchs[si][pi] == '1' || matchs[si][pi] == '0' ) return matchs[si][pi]; if ( pi + 1 &lt; p.length &amp;&amp; p[pi + 1] == '*' ) &#123; matchs[si][pi] = myMatch( matchs, s, p, si, pi + 2 ); if ( matchs[si][pi] == '1' ) return matchs[si][pi]; if ( p[pi] == '.' ) &#123; for ( int i = si + 1; i &lt;= s.length; i++ ) &#123; matchs[si][pi] = myMatch( matchs, s, p, i, pi + 2 ); if ( matchs[si][pi] == '1' ) return matchs[si][pi]; &#125; &#125; else &#123; int i = si; while ( i &lt; s.length &amp;&amp; s[i] == p[pi] ) &#123; matchs[si][pi] = myMatch( matchs, s, p, i + 1, pi + 2 ); if ( matchs[si][pi] == '1' ) return matchs[si][pi]; i++; &#125; &#125; &#125; else if ( p[pi] == '.' ) &#123; if ( si == s.length ) matchs[si][pi] = '0'; else matchs[si][pi] = myMatch( matchs, s, p, si + 1, pi + 1 ); &#125; else &#123; if ( si == s.length || s[si] != p[pi] ) matchs[si][pi] = '0'; else matchs[si][pi] = myMatch( matchs, s, p, si + 1, pi + 1 ); &#125; return matchs[si][pi]; &#125; &#125; 复杂度：O(n^2)???运行时间：25ms。击败：99.48%]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>study</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow 笔记一]]></title>
    <url>%2F2017%2F09%2F02%2F2017-09-02-tensorflow%2F</url>
    <content type="text"><![CDATA[一个十分简单的TensorFlow笔记。 导入python里面直接按照module来导入就行， 1import tensorflow as tf 常量1a = tf.constant(1, name='a') 或者 1a = tf.constant(1) 它也可以是矩阵， 1X = tf.constant(np.random.randn(3,1), name = "X") 变量例如， 1234y_hat = tf.constant(36, name='y_hat')y = tf.constant(39, name='y')loss = tf.Variable((y - y_hat)**2, name='loss') 可以看到变量是由一些常量进行一些运算后得到的。 当然直接写也是可以的， 1loss = (y - y_hat)**2 TensorFlow对基本运算进行了重写，它会自动识别出这里的运算步骤的。 初始化变量1init = tf.global_variables_initializer() 还不明白它的作用。 占位符1x = tf.placeholder(tf.int64, name = 'x') 占位符可以看成一个变量，它可以重复的赋值，赋值时要使用feed_dict{}， 1sess.run(2 * x, feed_dict = &#123;x: 3&#125;) 会话（session）TensorFlow的运行需要建立一个会话任务，然后通过它来运行， 123with tf.Session() as session: session.run(init) print(session.run(loss)) 要通过session才能将计算run()起来，这里也可以使用下面的方式， 1234sess = tf.Session()sess.run(init)print(sess.run(loss))sess.close() 目前看来，上面结构的好处就是可以自动进行close()。 十分简单的程序1234567891011121314151617import tensorflow as tf # 导包a = tf.constant(2, name='a') # 常量b = tf.constant(3, name='b') # 常量c = a * b + b # 变量d = tf.placeholder(tf.int32, name = 'd') # 占位符f = d + c # 变量init = tf.global_variables_initializer() # 初始化器with tf.Session() as session: # 建立会话 session.run(init) # 执行初始化 print(session.run(f, feed_dict = &#123;d: 100&#125;)) # 运行变量f 109 甚至怀疑应该把variable看成为一个运算式子，所以才会去run()这么一个式子。]]></content>
      <categories>
        <category>coursera</category>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>study</tag>
        <tag>mechine learning</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode 5 Longest Palindromic Substring]]></title>
    <url>%2F2017%2F09%2F01%2F2017-09-01-longest-palindromic-substring%2F</url>
    <content type="text"><![CDATA[Description: Given a string s, find the longest palindromic substring in s. You may assume that the maximum length of s is 1000. Example: Input: “babad”Output: “bab”Note: “aba” is also a valid answer. 想法，最简单的，就是以每个字母为中心，向两边扩散，这样的复杂度就是O(n^2)。 那么下面就是第一份代码，就是简单的向两边扩充。 123456789101112131415161718192021222324252627282930313233343536373839404142public class Solution &#123; public String longestPalindrome(String s) &#123; char[] s_split = s.toCharArray(); int p, q; int index1 = 0; int index2 = 0; for (int i = 0 ; i &lt; s_split.length ; i++) &#123; p = i - 1; q = i + 1; while ( p &gt;= 0 &amp;&amp; q &lt; s_split.length ) &#123; if ( s_split[p] == (s_split[q]) ) &#123; p--; q++; &#125; else &#123; break; &#125; &#125; if ( ( q - p - 1 ) &gt; ( index2 - index1 + 1 ) ) &#123; index1 = p + 1; index2 = q - 1; &#125; if ( i - 1 &gt;= 0 &amp;&amp; s_split[i] == s_split[i-1] ) &#123; p = i - 2; q = i + 1; while ( p &gt;= 0 &amp;&amp; q &lt; s_split.length ) &#123; if ( s_split[p] == s_split[q] ) &#123; p--; q++; &#125; else &#123; break; &#125; &#125; &#125; if ( ( q - p - 1 ) &gt; ( index2 - index1 + 1 ) ) &#123; index1 = p + 1; index2 = q - 1; &#125; &#125; return ( new String(s_split, index1, index2 - index1 + 1)); &#125;&#125; 击败38.60%，从图上就可以看出来，很垃圾。 时隔不知道多少个月，重新回来发现这么垃圾，索性修改了一下，现在的想法就比较复杂了。 首先，对于一个回文串，它的构成是例如abcdcba的形式，当去掉两边的a时，它依旧是一个回文串bcdcb。 那么如果想要降低复杂度，那么循环到一个字符时，一定要用到之前的信息，这样就能降低复杂度。 通过这种信息论的想法，加上上面回文串的特点，下面是新的算法思路： 循环一遍字符串。 以前一个字符所在的回文串，看以当前字符串为右边界能否去扩张这些字符串。 查看当前字符与前两个字符能否组成回文串，查看能否与前一个字符组成回文串。 保存下当前字符所在的回文串。 当前回文串的长度是否超过之前保存的最长，是则保存。 例如字符串sabckcbaga，假如现在循环到了后一个b，也就是现在前面的字符串是sabckcb，那么前一个字符就是c， 它所在的一个回文串就是ckc，那么现在就尝试用b去扩充它，发现bckcb，可以扩充，所以最长支付串保存为bckcb， 当前所在回文串保存bckcb。再查看kcb是否回文串，不是，查看cb是否回文串，也不是。 那么当前回文串就是bckcb。 循环到下一个字符是a，尝试用a扩充bckcb，扩充成功，当前字符串保存abckcba，最长长度更新， 查看cba不是回文串，查看ba不是回文串。当前回文串abckcba。 再循环到g，尝试扩充abckcba失败，bag、ag也不是回文串，当前回文串没有。 循环到a，上一个回文串没有，查看aga是回文串，ga不是回文串。当前回文串aga。 再例如字符串abababa。 循环到第二个b，它不能扩充前一个回文串aba，bab是回文串，ab不是。当前回文串bab。 循环到a，它能扩充bab，为ababa，同时aba也是回文串，ba不是。当前回文串ababa、aba。 循环到b，它不能扩充ababa，它能扩充aba为babab，同时bab是回文串，ab不是。当前回文串babab、bab。 … 代码里面还考虑了重复，例如aaaaaa，按照上面的操作到第5个a时，它的当前回文串会有aaaaa、aaaa、aaa、aa，这样很不好， 实时上，这里只需要aaaaa，在存一个是否连续的标志位。 例如循环到最后一个a，它上一个回文串为aaaaa，同时连续标志位为true，首先它不能扩充aaaaa，当连续标志位为true时， 看它能否右扩充aaaaa，也就是aaaaa + a是否为为回文串，这里显然aaaaaa是回文串。同时由于连续标志位为true，这里不再需要查看aaa和aa是否为回文串。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class Solution &#123; private int begin = 0; private int max_len = 1; private int p_idx = 0; private int t_idx = 0; private void addLen( int[] a, int i, int len ) &#123; if ( max_len &lt; len ) &#123; max_len = len; begin = i; &#125; if ( i - 1 &gt;= 0 ) a[t_idx++] = i - 1; &#125; public String longestPalindrome(String s) &#123; if ( s == null || s.length() == 0 ) return ""; char[] ss = s.toCharArray(); int succ_idx = -1; int[] p = new int[ss.length]; for ( int i = 1; i &lt; ss.length; i++ ) &#123; for ( int j = 0; j &lt; p_idx ; j++ ) if ( ss[p[j]] == ss[i] ) addLen(p, p[j], i - p[j] + 1); if ( succ_idx != -1 ) &#123; if ( ss[succ_idx] == ss[i] ) addLen(p, succ_idx, i - succ_idx + 1 ); else succ_idx = -1; &#125; else &#123; if ( i - 2 &gt;= 0 &amp;&amp; ss[i - 2] == ss[i] ) addLen(p, i - 2, 3); if ( i - 1 &gt;= 0 &amp;&amp; ss[i - 1] == ss[i] ) &#123; succ_idx = i - 1; addLen(p, i - 1, 2); &#125; &#125; p_idx = t_idx; t_idx = 0; &#125; return ( new String(ss, begin, max_len)); &#125; &#125; 期望复杂度O(n)？反正最差复杂度O(n^2)。 击败84.26%，我觉得还行。我甚至觉得如果用例再多一些，可以击败更多的人！！！]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>study</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Softmax Regression 总结]]></title>
    <url>%2F2017%2F08%2F30%2F2017-08-30-softmax%2F</url>
    <content type="text"><![CDATA[Softmax Regression使用神经网络进行多分类任务时，通常在最后一层会使用一个softmax层，个人感觉它与sigmoid函数类似，都是将结果转变成一个类似概率的东西， 这样会更加有利于计算当前损失，提升模型学习的效果。 首先对于一个输入$a^{[L-1]}$，先线性运算， z^{[L]} = w^{[L]}a^{[L-1]} + b^{[L]}前面的操作每一层都一样，但是激活函数就不一样了， t = e^{(z^{[L]})} a^{[L]} = \frac{e^{(z^{[L]})}}{\sum_{i=1}^{n^{[L]}} t_i}首先将输出转成$t$，这样能够保证所有数值不为负，然而以它们在总和中所占的比例，作为它们当前的概率， 也就是$a^{[L]_i}$就是分类为第$i$个分类的概率。 举例，对于一个$z^{[L]}$取值如下， z^{[L]} = \begin{bmatrix} 5 \\ 2 \\ -1 \\ 3 \end{bmatrix}求$t$为， t = \begin{bmatrix} e^5 \\ e^2 \\ e^{-1} \\ e^3 \end{bmatrix}那么$a^{[L]}$为， a^{[L]} = \frac{t}{\sum_{i=1}^{n^{[L]}} t_i} = \begin{bmatrix} e^5 / (e^5 + e^2 + e^{-1} + e^3) \\ e^2 / (e^5 + e^2 + e^{-1} + e^3) \\ e^{-1} / (e^5 + e^2 + e^{-1} + e^3) \\ e^3 / (e^5 + e^2 + e^{-1} + e^3) \end{bmatrix} = \begin{bmatrix} 0.842 \\ 0.042 \\ 0.002 \\ 0.114 \end{bmatrix}这里的到的$a^{[L]}$就可以看作一个概率值。 Loss_function：由于softmax可以看成是Logistic Regression的推广，那么这里对损失函数的计算也对照Logistic Regression来推。 Logistic Regression的代价函数，这里把上面的$a^{[L]}$写为$\hat y$： J(W,b) = \frac{1}{m} \sum_{i=1}^m L(\hat y^{(i)},y^{(i)} ) = - \frac{1}{m} \sum_{i=1}^m y^{(i)}log \ \hat y^{(i)} + (1-y^{(i)})log(1 - \hat y^{(i)})首先，考虑单个样本，那么在Logistic Regression中单个样本的概率函数就是， P(y) = (\hat y)^y (1 - \hat y)^{1-y}同样的，在softmax中单个样本的概率函数以同样的形式可以写为， P(y) = \prod_{i = 1}^{n^{[L]}} (\hat y_i)^{y_i}进行对数似然， L(P(y)) = \sum_{i = 1}^{n^{[L]}} y_i log(\hat y_i)改为最小化似然函数，加上一个负号， L(P(y)) = -\sum_{i = 1}^{n^{[L]}} y_i log(\hat y_i)那么现在就可以考虑多样本输入的情况，直接在前面加上一个求和就行， L(P(y)) = -\sum_{i = 1}^{m}\sum_{j = 1}^{n^{[L]}} y_j^{(i)} log(\hat y_j^{(i)}) 梯度代价函数就是上面的负对数似然，同样先考虑单样本， J(w,b) = -\sum_{j = 1}^{n^{[L]}} y_j log(\hat y_j)将$\hat y_j$换一下，忽略上标， J(w,b) = -\sum_{j = 1}^{n} y_j log(\frac{e^{z_j}}{\sum_{i=1}^{n} e^{z_i}}) = -\sum_{j = 1}^{n} y_j (log(e^{z_j}) - log(\sum_{i=1}^{n} e^{z_i})) = -\sum_{j = 1}^{n} y_j log(e^{z_j}) + \sum_{j = 1}^{n} y_j log(\sum_{i=1}^{n} e^{z_i}) = -\sum_{j = 1}^{n} y_j z_j + log(\sum_{j=1}^{n} e^{z_j})先求一个$z_i$的导数， \frac{\partial J}{\partial z_i} = - y_i + \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}} = \hat y_i - y_i所以，对于一个样本的导数就可以写为， \frac{\partial J}{\partial z} = \hat y - y当然，多个样本的形式也和上面一模一样，所以反向传播时，其实计算是十分简单的。 注: 在cs231n课程里面，提到这里还有一个地方需要注意，因为中间会去求e的幂次， 这很容易会造成上溢，所以通常对这个地方要做一个额外的处理， \frac{e^z}{\sum_{j=1}^{n} e^{z_j}} = \frac{Ce^z}{C\sum_{j=1}^{n} e^{z_j}} = \frac{e^{z+logC}}{\sum_{j=1}^n e^{z_j + logC}}这里的$C$，一般取值为， logC = -\max_j z_j那么为什么要叫做softmax呢？ 它其实对应的是hard max，hard max的操作就是直接将$z^{[L]}$按照取值大小， 直接硬转为0、1，所以这里取了一个soft… z^{[L]} = \begin{bmatrix} 5 \\ 2 \\ -1 \\ 3 \end{bmatrix} \Rightarrow \begin{bmatrix} 1 \\ 0 \\ 0 \\ 0 \end{bmatrix}]]></content>
      <categories>
        <category>coursera</category>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>study</tag>
        <tag>mechine learning</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hyperparameter tuning, Batch Norm and TensorFlow]]></title>
    <url>%2F2017%2F08%2F29%2F2017-08-29-dnn_4%2F</url>
    <content type="text"><![CDATA[Hyperparameter tuning调参是训练神经网络中十分重要的一项技能，很有可能为了训练出一个好的模型，一大半的时间需要花费在调参上面。 首先调参时要分清楚哪些是重要参数，也就是对结果影响较大的参数，着重去调整它们，例如下列的参数： $\alpha$ $\lambda$ $\beta_1,\beta_2,\epsilon$ $layers$ $hidden \ units$ $learning \ rate \ decay$ $mini \ batch \ size$ 显然，其中比较重要的参数是$\alpha,\lambda,layers,learning \ rate \ decay$，所以在调参的时候着重去调整它们， 剩下的参数随意一点，可调可不调，看着具体的问题来选择。 通常会认为调参是下面这种形式： 这种形式就是先选好固定的距离，然后进行搜索，这样并没有太大问题，但就上图来说，如果参数一重要，参数二不重要， 这样就会浪费一些搜索机会，比如先固定了参数一，参数二变化五次，这五次模型的性能并不会有什么变化。 所以，可以进行随机选点： 这样就不会太浪费选点的机会。 另外可以第一轮先在一个较大的范围去选点，然后再缩小到一个小一点的范围进行更加精确的搜索。 随机选点： 随机选点可不是random就可以了，有时还需要考虑一下尺度问题， 首先，假如是一个50 ~ 100范围内的随机，那么随机选就可以了，没什么问题， 但是如果是一个0.0001 ~ 1的范围，那么随机就会显得很蠢， 那么90%的概率点会选在0.1 ~ 1的区间里面，显然不行。如下图所示，才是这里的”均匀”。 为了产生这种效果，就把它变成一个对数坐标，在python中就像下面这样来写就行了， 12r = -4 * np.random.rand() # -4 ~ 0a = 10**r # 10^-4 ~ 10^0 相当于变成对数坐标，在对数坐标上面进行随机，然后再返回来。 如果是一个0.9 ~ 0.999的范围，先按上面的方式随机一个0.1 ~ 0.001的范围，再用1来减就行了。 另外例如范围0.9000 ~ 0.9005与0.999 ~ 0.9995，看起来它们之间没有什么不同，但是实际上，后一个范围它很接近1， 这个变化在机器学习中是会有质变的，所以第二个范围在进行搜索时，需要更加的细致。 另外视频中还给提供了在模型训练时间很长的时候（几天以上），两种调参方式的有趣的名字： Pandas：一次一胎，也就是同时只训练一个模型，然后你看着代价变化曲线，每天调一调参。（计算资源较少） Caviar：一次n胎，同时训练n个模型，几天后选择其中那个最好的。（计算资源很多） Batch Norm就像前面将输入进行标准化一样，可以在神经网络中任意一层去应用类似的标准化方法，这样的标准化对于梯度爆炸和梯度消失问题的缓解都很有好处。 如下图， Batch norm一般是在$z^{[i]}$上进行，也就是在激活函数之前进行， \mu^{[l]} = \frac{1}{n^{[l]}} \sum_{i = 1}^{n^{[l]}} z^{[l]}_i (\sigma^{[l]})^2 = \frac{1}{n^{[l]}} \sum_{i = 1}^{n^{[l]}} (z^{[l]}_i - \mu^{[l]})^2 z^{[l]}_{norm} = \frac{z^{[l]} - \mu^{[l]}}{\sqrt{(\sigma^{[l]})^2 + \epsilon}} \tilde{z}^{[l]}_{norm} = \delta z^{[l]}_{norm} + \beta这里的$\epsilon$就是防止出现除以零的错误，另外，这里不直接使用标准化后的$z^{[l]}_{norm}$， 而是加入了两个参数$\delta,\beta$，这两个参数不是定死的，它是随着更新而更新的，加这两个参数的原因， 是为了让模型可以在标准化的基础上进行一下小变动，甚至可以返回标准化之前的数值，例如， \delta = \sqrt{(\sigma^{[l]})^2 + \epsilon} \beta = \mu那么带进去计算就会发现，数值又返回原始的状态了。 在这里，使用了Batch norm的层就多了两个参数$\delta,\beta$，在反向传播时，同样要去计算这两个参数的梯度，然后更新。 同样的也可以使用Adam方法来更新这两个参数。 另外需要注意的是， z^{[l]} = w^{[l]}a^{[l-1]} + b^{[l]}那么在对$z^{[l]}$进行batch norm的时候，$b^{[l]}$就会显得很多余，因为它会被零均值化掉， 所以在施加了batch norm的神经层中，就不再需要$b^{[l]}$这个参数。 为什么Batch norm会起到作用？ 吴恩达在这里给了一些直觉来使得它make sense，例如下面训练一个简单的神经元， 训练的数据如下图左边，测试时变为了下图右边， 显然由于数据的分布发生了变化，模型的准确率就会下降，不可能指望左边真的能够学习到那条绿色的判决线。 同样的道理，在多层的神经网络中，可以认为某一层的输入就有一个疯狂变化的分布， 那么，这时受到分布变化的影响，这一层就很难去学习到一个比较好的参数。当加上Batch norm时， 就能限制一下这个分布的变化，减轻影响。 测试样本怎么办？ 当训练时，使用的都是mini-batch，自然可以计算出它们的均值、方差，但是如果在测试时，一次只有一个样本通过， 那么怎么Batch norm呢？ 通常情况下，追踪训练中的每一个mini-batch在该层上得到的均值与方差，使用exponentially weighted的方法来估计它们的均值， \mu_{all}^{[l]} = \beta \mu_{all}^{[l]} + (1 - \beta) \mu^{\{i\}[l]}这里的$i$代表第$i$个mini-batch。 注意这样求出来的总的均值、方差只是在test阶段使用，并不用在训练过程中。 梯度怎么求？ 这个现在先避开，感觉上就不好算，按吴恩达的说法，通常不会去手写这个的，让学习框架来就好。 Softmax RegressionSoftmax Regression 总结 深度学习框架不知道视频是什么时间做的，总之上面列出了很多深度学习的框架，但就现在这个时间点来说，TensorFlow肯定是主流的， 当然，课程里面用的也是TensorFlow。 当神经网络的结构变得复杂的时候，手写代码就真的会比较辛苦，还容易出现bug，这个时候，就需要框架来帮忙了。 TensorFlow笔记一]]></content>
      <categories>
        <category>coursera</category>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>study</tag>
        <tag>mechine learning</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mini-batch 和 权重更新策略]]></title>
    <url>%2F2017%2F08%2F29%2F2017-08-29-dnn_3%2F</url>
    <content type="text"><![CDATA[Mini-batch在训练时，对样本的输入有三种策略， 一次输入一个样本，Stochastic gradient descent。 一次输入多个样本，Mini-batch gradient descent。 一次输入所有样本，Batch gradient descent。 对于一次输入所有样本，也就是Batch gradient descent，那么每次梯度下降都会向着全局最优/局部最优去前进， 这样的缺点就是计算更新很慢，因为一次更新都需要计算整个训练集，在训练集非常大的时候甚至是不可能的。 另外，这样更新是很难跳出局部最优的，虽然局部最优也不差，但是总是有差的局部最优。 对于一次输入一个样本，也就是Stochastic gradient descent，那就是向另外一个极端去前进，每次梯度下降都会向着这一个样本的全局最优去前进， 虽然这样计算块，更新快，但是随机性也太大了。 对于一次输入多个样本，也就是Mini-batch gradient descent，这就是一个折中的解决方案， 每次计算不会太复杂，也能降低一点随机性。 在Mini-batch大小的选择中，一般选取2的幂次的大小，这与CPU/GPU的计算位数相关，所以一般情况， 选取64、128、256、512。 权重更新在梯度下降的基础上，对更新策略更新一下改动，不再是单纯的减去梯度，而是用带有平均的一种思想去更新。 Exponentially weighted averages: 如下图是一个气温随日期变换的坐标图，其中蓝色的点就是某一天的气温，这里总共有一年的数据， 其中红色和绿色的线是根据下面的公式画出来的， v_t = \beta v_{t-1} + (1 - \beta)\theta_t其中$\theta_t$表示的就是第$t$天的气温，当$\beta = 0.9$时，画出的就是红色的线， 当$\beta = 0.98$时，画出的就是绿色的线。上面的式子就相当于是在给气温做平均。 大概平均的天数为： days \approx \frac{1}{1 - \beta}于是红色的线大概是10天的平均，绿色的线大概是50天的平均，所以绿色的线看起来会有一点偏右。 计算时，就一步一步运算就行了， v_\theta = 0 v_\theta := \beta v + (1 - \beta)\theta_1 v_\theta := \beta v + (1 - \beta)\theta_2 ...但是，在实际操作中，按照上面的式子得到的绿色的线其实应该在紫色的线的位置上，这是因为初始值为0的原因， 所以，这里需要加入Bias correction， v_t = \frac{v_t}{1 - \beta^t}这里的$\beta^t$是$\beta$的$t$次幂，刚开始的时候分子会很小，然后逐渐变大，事实上，第一轮迭代，$v_1$就等于$\theta_1$。 这样，就能矫正这个初始值太低的问题。 Momentum: 属于权重更新的初级魔法： V_{dw} = \beta V_{dw} + (1 - \beta)dw V_{db} = \beta V_{db} + (1 - \beta)db w := w - \alpha V_{dw} b := b - \alpha V_{db}通常取$\beta = 0.9$，而且这里不需要矫正。也可以去掉$(1 - \beta)$，变为， V_{dw} = \beta V_{dw} + dw V_{db} = \beta V_{db} + db这样相当于参数调整基本交给$\alpha$来做。 RMSprop: 属于权重更新的中级魔法： S_{dw} = \beta S_{dw} + (1 - \beta)dw^2 S_{db} = \beta S_{db} + (1 - \beta)db^2 w := w - \alpha \frac{dw}{\sqrt{S_{dw}}} b := b - \alpha \frac{db}{\sqrt{S_{db}}}这里的$dw^2$就是$dw$的平方，当然也是element-wise的。这里的$\beta$通常取0.999，而且也不用矫正。 为了防止出现分母为0的错误，通常要在分母上加上一个很小的数$\epsilon$， w := w - \alpha \frac{dw}{\sqrt{S_{dw}} + \epsilon} b := b - \alpha \frac{db}{\sqrt{S_{db}} + \epsilon}Adam: 属于权重更新的高级魔法，它将Momentum与RMSprop相结合： Init:V_{dw} = 0, S_{dw} = 0, V_{db} = 0, S_{db} = 0 V_{dw} = \beta_1 V_{dw} + (1 - \beta_1) dw V_{db} = \beta_1 V_{db} + (1 - \beta_1) db S_{dw} = \beta_2 S_{dw} + (1 - \beta_2) dw^2 S_{db} = \beta_2 S_{db} + (1 - \beta_2) db^2显然这里的$V_{dw}$属于Momentum， 这里的$S_{dw}$属于RMSprop， 另外这里还要用到矫正： V_{dw}^{corrected} = \frac{V_{dw}}{(1 - \beta_1^t)} V_{db}^{corrected} = \frac{V_{db}}{(1 - \beta_1^t)} S_{dw}^{corrected} = \frac{S_{dw}}{(1 - \beta_2^t)} S_{db}^{corrected} = \frac{S_{db}}{(1 - \beta_2^t)}将它们结合起来，权重的更新策略就如下： w := w - \alpha \frac{V_{dw}^{corrected}}{\sqrt{S_{dw}^{corrected}} + \epsilon} b := b - \alpha \frac{V_{db}^{corrected}}{\sqrt{S_{db}^{corrected}} + \epsilon}通常将参数设置为$\beta_1 = 0.9,\beta_2 = 0.999,\epsilon = 10^{-8}$，一般情况不需要去调整它们的取值，默认的就足够了。 （它名字的全称其实是Adaptive moment estimation，所以其实和Adam没有半毛钱关系…） Learning rate decay在权值更新的过程中逐渐减小学习速率，能够使模型收敛到更好的解， 其中蓝色的线是没有减小学习速率的，它最后会在最优解附近一个较宽的范围徘徊， 另外绿色的线就是逐渐减小学习速率的，它会收敛到一个较窄的范围。 需要注意的是，这里逐渐减小学习速率的单位是epoch，一个epoch就是过一遍数据。 首选的策略是： \alpha = \frac{1}{1 + decayRate * epochNum} \alpha_0其它的一些策略： \alpha = 0.95^{epochNum} \alpha_0 \alpha = \frac{k}{\sqrt{epochNum}} \alpha_0或者， \alpha = \frac{k}{\sqrt{t}} \alpha_0这里的$t$就是总的迭代次数了，上面的$k$是一个固定的常量。 以上的这些decay的方法其实都可以，选一个用就好。 关于局部最优通常对于局部最优的印象是这样的， 就会觉得很容易陷入局部最优，但是实际上在高维空间中，这样的直觉是不准确的， 它更像是下面的马鞍面， 那么在遇到一个梯度为0的点的时候，实际上更可能是马鞍面中间的那个点， 模型因为各种措施，它也并不会卡在这个点上。 因为这样的原因，神经网络几乎不可能卡在一个很差的局部最优点上，通常都能收敛到一个较好的局部最优点，距离全局最优也差不了多远。]]></content>
      <categories>
        <category>coursera</category>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>study</tag>
        <tag>mechine learning</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[神经网络的数据集划分、正则化、Dropout、输入标准化以及梯度检查等等]]></title>
    <url>%2F2017%2F08%2F25%2F2017-08-25-dnn_2%2F</url>
    <content type="text"><![CDATA[数据集的拆分在一个数据集上去应用机器学习算法时，你可能可以选择不同的算法，同样的， 相同的算法也可以有不同的参数，例如神经网络中的网络层数，神经元个数等等。 这个时候当然就得一个一个算法去试，一个一个参数去调，来选出所认为的表现最好的模型。 这时就会一个问题，如何去评判哪一个模型在这里最好？ 通常情况（默认这里是监督学习），首先需要拿一部分数据出来训练模型，然后在训练完毕后， 让训练好的模型去预测另一部分数据，用这部分数据来评判这个模型的性能， 这样就能选出最合适的模型。 但是这里得到模型准确率是不能作为模型的真实准确率的，因为这里相当于用同一份数据去在一堆模型中选出最好的模型， 这样就可能是因为这个模型刚好符合这部分数据而已，所以需要用“新”的数据来测试这个模型的准确率。 综上，数据的划分为训练集 + 交叉验证集 + 测试集，视频中的原英文是Training Set + Development Set/Hold-out Cross Validation Set + Test Set。 这三部分的划分比例一般为70% + 20% + 10%，但是这个比例并不是一定的，只要能实现各个部分的功能就行，例如有一百万的样本，也许一万个样本对于交叉验证集来说已经足够了。 正则化正则化可以说是神经网络必不可少的一个部分，因为神经网络的拟合能力十分强大，所以它的过拟合能力也是十分的强大。 事实上，神经网络中很多的技巧都是为了去减少过拟合，正则化是其中最基础的一个。 J(w^{[1]},b^{[1]},...,w^{[L]},b^{[L]}) = \frac{1}{m} \sum_{i=1}^m L(\hat y^{(i)}, y^{(i)}) + \frac{\lambda}{2m} \sum_{l = 1}^L ||w^{[l]}||_F^2上面的公式就是加了正则化的代价函数，也就是多了一项权重的2-范数，这样就会将权重的取值大小考虑到代价函数里面。 由于优化目标是最小化代价函数，那么正则化项的存在就会迫使权重的取值变小，权重的取值不再那么自由， 它就会选择一个折中，不再过分的变化某些权重来迎合训练数据，达到减轻过拟合的目的。 另外这里多了一个参数$\lambda$，这个参数就控制着正则化的强度，也就多了一个训练时需要调整的超参数。 加上正则化项对于反向传播的求导也没有什么影响，在之间的基础上加上正则化项的导数就行， dw^{[l]} = (from \ backprop) + \frac{\lambda}{m}w^{[l]}对于偏置项$b^{[l]}$来说，可以对它正则化，也可以不正则化，不重要。这里具体的原因还不知道，感觉它取值其实是受到权重影响的， 在对权重加上偏置之后，它加不加偏置就变得不重要了。 注：正则化又叫weight decay。 Drop OutDrop-out是一个很神奇的东西，它同样可以减小神经网络过拟合，提高模型的泛化能力。 它的操作就是给予每个神经元一定的几率使它会被“丢弃”掉，如下图， 图中以50%的几率使得一些隐藏层的神经元抑制，也就是不往后传播。 直观意义上来说，由于每次都会有不同的神经元被抑制，所以模型不能只依靠某些神经元，这里如果第一层也使用Drop-out， 那么就意味着输入的一些属性也会被抑制掉， 这就迫使每一个神经元都得有作用，或者不能依赖样本的某一个属性，而且它们的功能上也要有冗余。于是模型就不会过分的去拟合当前数据， 也就达到了减小过拟合的目的。（输出层当然不能加Drop-out） 注意： 加入Drop-out后的梯度运算就和ReLU函数类似，被抑制的神经元就不再反向传播， 所以抑制矩阵也需要在前向传播中缓存下来，因为在反向传播时将会用到它。 另外使得每一层的期望输出不会变小，对于没有被“drop-out”的神经元的输出，将它除以drop-out的概率， 例如作业中的代码， 1234D1 = np.random.rand(A1.shape[0], A1.shape[1]) # drop-out矩阵D1 = ( D1 &lt; keep_prob ) # 变为0，1A1 = A1 * D1 # drop-outA1 = A1 / keep_prob # 除以概率 因为这里乘上了系数$\frac{1}{keep-prob}$，那么在反向传播求导时，这个系数同样的存在， 12dA1 = dA1 * D1 dA1 = dA1 / keep_prob 另外，在对测试样本进行分类时，关闭Drop-out，也不再乘系数$\frac{1}{keep-prob}$。 其它的一些防止过拟合的方式Data Augmentation: 不知道如何翻译，总之思想很简单，就是将现有的数据集中的样本进行一下处理，得到一些变化后的样本， 将这些变化后的样本也加入训练集，这样也能够提升模型的泛化性能，减轻过拟合。 如下图的图片分类问题， 其中左边是原始的图片，将图片安装当前分类的性质，可以进行对称，旋转，噪声等操作，这样就能在原样本上得到新的一些样本， 并且这些样本也是在实际中真实会遇到的一些情况。 Early Stopping 直接看图， 图中的蓝色线就是训练集在模型中的误差，它随着训练逐渐减小。紫色线是dev set在模型中的误差，它随着模型的训练也是逐渐减小， 但是当迭代到一定次数的时候，模型就开始过拟合了，于是它的误差就会逐渐增高。 所以Early Stopping的意思就是去提前结束训练，争取不让模型过拟合，所以在训练过程中可以去画这个曲线，提早停止训练。 输入标准化Normalizing Input，输入标准化是在数据预处理中非常重要的一步，由于原始数据每个属性的取值范围不一样， 如果不做处理的话，可能在进行梯度下降时会遇到一些困难，例如， x_1 \in \{1-100\} x_2 \in \{1-10000\}这时，两个属性之间的差距就会变得很大，如下图， 它的样本分布就与左边的图类似，比较狭长。 中间的图是零均值化后的分布。 最右边的图是再进行方差归一化的分布。 公式如下： \mu = \frac{1}{m} \sum_{i = 1}^m x^{(i)} \sigma ^2 = \frac{1}{m} \sum_{i = 1}^m (x^{(i)})^2 x := x - \mu x /= \sigma ^2这里的$x^{(i)}$就是一个样本，它是一个向量，但是这里的操作都是element-wise的。 需要注意的是，这里是先进行零均值化，然后再对零均值化后的样本进行方差归一化，所以方差里面省去了均值0。 下面是输入标准化之前和之后的代价函数的取值空间对比： 显然在输入标准化之后，能更好的进行优化。 注意： 直接对整个训练集进行标准化，所以能够得到整个训练集的均值和方差，那么在test阶段， 输入的样本同样的按照这个均值和方差去标准化就行。 Vanishing/exploding gradients梯度消失和梯度爆炸是深度神经网络中会遇到的问题，假设一个深度神经网络，如下图： 假设这个网络的激活函数是一个线性激活函数，并且没有偏置， z^{[l]} = w^{[l]}z{[l-1]}那么$\hat y$就可以写为， \hat y = w^{[L]}w^{[L-1]}...w^{[2]}w^{[1]}x假设中间每一层的神经元权值都相同， \hat y = w^{[L]}(w^{[1]})^{(L-1)}x那么当$w^{[1]}$取下列值得时候 w^{[1]} = \begin{bmatrix} 0.5 & 0 \\ 0 & 0.5 \end{bmatrix} \ \ \ \ or \ \ \ \ w^{[1]} = \begin{bmatrix} 1.5 & 0 \\ 0 & 1.5 \end{bmatrix}在经历$(w^{[1]})^{(L-1)}$之后，显然值要不会非常小，要不就会非常大。 这样，要不就是数据太大，直接计算错误，要不就是数据太小，一次迭代和没有迭代一样， 这就是问题所在。 （所以和梯度有什么直接关系，为何要叫做梯度消失、梯度爆炸？？？） 权重的初始化为了减轻上面的梯度爆炸或者梯度消失的问题，在权重初始化时，可以施加一定的规则。 先看一个神经元中$z$的计算， z = w_1x_1 + w_2x_2 + ... + w_nx_n为了让$z$既不会太大也不会太小，那么让$w$的均值为0，方差为$\frac{1}{n}$，会有一定的帮助。 对于ReLU激活函数， w^{[l]} = np.random.randn(shape)*np.sqrt(\frac{2}{n^{[l-1]}})这里分子变为了2（为1叫做Xavier Initialization，为2时叫做He Initialization），暂时只知道这是一个经验值。 对于tanh激活函数，分子不用变成2， w^{[l]} = np.random.randn(shape)*np.sqrt(\frac{1}{n^{[l-1]}})另外这里的也可以选择为， \sqrt{\frac{2}{n^{[l-1]}+ n^{[l]}}}最后，初始化只是给了一个起始点而已，它并不是一个完美的方法。 梯度检查在实现神经网络的过程中，可能不注意就写出了一个bug，有时候可能发现不了，那么这时候最好先用梯度检查来看代码是否存在问题， 当不存在问题时，再关闭梯度检查。 对于导数，可以利用导数的定义来进行计算， 这样，取一个很小的$\epsilon$，就能使用下面公式去近似到$f(\theta)$在点$\theta$的导数。 {f}'(\theta) = \frac{f(\theta + \epsilon) - f(\theta - \epsilon)}{2 \epsilon}implementation: 在实现梯度检查时，首先将所有的参数$W^{[1]},b^{[1]},…,W^{[L]},b^{[L]}$合并成一个超长的向量$\theta$， 同样的，也将反向传播得到的各个梯度$dW^{[1]},db^{[1]},…,dW^{[L]},db^{[L]}$合并成一个向量$d\theta$。 因为所有参数的导数都是由代价函数$J$求导得来的，那么在求他们梯度的近似的时候，当然要从代价函数入手， d\theta_{approxi}^{[i]} = \frac {J(\theta_1,\theta_2,...,\theta_i + \epsilon,...) - J(\theta_1,\theta_2,...,\theta_i - \epsilon,...)} {2\epsilon}求出所有参数的近似梯度，将它们与反向传播得到的梯度相比较， check \ \ \ \frac{||d\theta_{approxi} - d\theta||_2}{||d\theta_{approxi}||_2 + ||d\theta||_2}从经验直觉上，当这个比值在$10^{-7}$往下时，就可以认为这里的梯度计算没有问题； 当在$10^{-5}$左右时，就可能会有问题了；当在$10^{-3}$往上时，几乎可以认为一定有问题。 注意事项： Don’t use in training - only to debug.（梯度检查是一件很费时间的事情，每改一个参数就得计算一次前向传播） If algorithm fails grad check, look at components to try to identify bug.（定位问题所在） Remember regularization. Doesn’t work with dropout.（会变得不好计算，先关闭dropout，检查完毕再开启） Run at random initialization; perhaps again after some training.]]></content>
      <categories>
        <category>coursera</category>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>study</tag>
        <tag>mechine learning</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Coursera Improving Deep Neural Networks 笔记]]></title>
    <url>%2F2017%2F08%2F25%2F2017-08-25-dnn%2F</url>
    <content type="text"><![CDATA[第二个专项课程的完整题目是 Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization. 通过标题就可以看出课程的主要内容就是在第一个课程的基础上， 对深度神经网络进行加固。 一共三周的课程，每周课程题目分别为： Practical aspects of Deep Learning Optimization algorithms Hyperparameter tuning, Batch Normalization and Programming Frameworks 第一周讲了划分数据集、正则化、权值初始化以及梯度检查。 第二周讲了mini-batch以及RMSprop等参数更新方法。 第三周讲了超参数的选择、标准化以及TensorFlow。 第一周神经网络的数据集划分、正则化、Dropout、输入标准化以及梯度检查 第二周Mini-batch 和 权重更新策略 第三周Hyperparameter tuning, Batch Norm and TensorFlow]]></content>
      <categories>
        <category>coursera</category>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>study</tag>
        <tag>mechine learning</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[写博客的注意事项]]></title>
    <url>%2F2017%2F08%2F23%2F2017-08-23-my-write-blog-guide%2F</url>
    <content type="text"><![CDATA[引导页面NexT主题说明 Markdown语法 MathJax基本语法 在线公式编辑器，快速知道语法 创建文件可以使用hexo new &quot;博客名&quot;命令，它会在source/_posts文件夹下面创建了一个新的md文件， 会自带一点内容，例， 123$ hexo new &quot;wakaka&quot;INFO Created: I:\nut\git\blog\source\_posts\wakaka.md 12345---title: wakakadate: 2017-08-23 23:50:45tags:--- 创建好之后，就可以接着往下写。当然可以自己在source/_posts文件夹下创建md文件， 这样文件名与title就可以不一样，随便写，至于时间信息，在中文输入法中拼sj就可以。 文件头部官方头部说明 这篇文章的头部如下， 12345678910---title: 写博客的注意事项date: 2017-08-23 23:42:49categories: - guide - hexotags: - guidedescription: 写一篇博客需要注意的问题，作为以后写博客的参考。--- 这里hexo这个分类，在页面上会显示为guide类的一个子项， 如果要写公式，因为在配置文件里面设置了per_page = true， 所以需要在头部加上mathjax标签，这样可以加快不用公式页面的渲染速度。 1mathjax: true 图片图片放在source/images文件夹下就行，直接相对路径引用， 1![image](/images/head/head_bigfan.png) 插入图片大小的调整，并且居中： 12&lt;div align="center"&gt;&lt;img src="/images/head/head_bigfan.png"style="zoom:50%" title="大范" /&gt;&lt;/div&gt; 也可以用， 12&lt;div align="center"&gt;&lt;img src="/images/head/head_bigfan.png"width="200" height="200" title="大范" /&gt;&lt;/div&gt; 公式行内用\$...\$，行间用\$\$...\$\$，注意去掉前面的所有斜杠&quot;\&quot;， 由于公式的一些问题，一定要正确的使用格式，不然会可能会出问题。 目录hexo会自动安装Markdown语法中标题的#，##，###，####来生成目录。 gif和引用图片一样的操作， 1234![image](/images/head/wzx.gif)&lt;div align="center"&gt;&lt;img src="/images/head/wzx.gif"style="zoom:150%"/&gt;&lt;/div&gt; 视频支持的网站直接可以复制到链接， 例，b站视频： 123&lt;embed height="415" width="544" quality="high" allowfullscreen="true" type="application/x-shockwave-flash" src="//static.hdslb.com/miniloader.swf" flashvars="aid=13483112&amp;page=1" pluginspage="//www.adobe.com/shockwave/download/download.cgi?P1_Prod_Version=ShockwaveFlash"&gt;&lt;/embed&gt; 音乐例，网易云音乐，进入网页版，获取链接： 12&lt;iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&amp;id=33894145&amp;auto=0&amp;height=66"&gt;&lt;/iframe&gt; 注意链接中有一个auto属性，0表示不自动播放，1表示自动播放。 颜色HTML颜色表 123&lt;font color='#FF0000'&gt;这是红色！！！&lt;/font&gt;&lt;br/&gt;&lt;font color='#FFD306'&gt;这是黄色！！！&lt;/font&gt;&lt;br/&gt;&lt;font color='#0072E3'&gt;这是蓝色！！！&lt;/font&gt;&lt;br/&gt; 这是红色！！！ 这是黄色！！！ 这是蓝色！！！]]></content>
      <categories>
        <category>guide</category>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>guide</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo中mathjax公式与回车换行问题]]></title>
    <url>%2F2017%2F08%2F23%2F2017-08-23-hexo-mathjax%2F</url>
    <content type="text"><![CDATA[初始问题页面公式一部分能够渲染，另外一部分不能渲染，变成斜体。 这个问题显然是致命的，不解决基本没得救。 搜索了一下，发现这是因为Markdown与公式之间的不友好性，导致它会把公式中的&#39;_&#39;字符看出是斜体的语法， 导致公式渲染成了斜体。 另外这个问题与Next主题完全没有关系，就是Hexo它自己的问题。 Hexo默认的Markdown渲染引擎是marked，就是这个引擎的锅。 解决方法我看到的解决方法第一个是使用rawblock保护代码块，我还不知道它是什么， 反正意思是我所以得文档都得改，不可能。 第二个方法是将引擎换成pandoc，它也是一个Markdown引擎， 但是试了一下不知道卡哪了，反正不成功。 看到的第三个解决办法是修改marked引擎的渲染规则，试了一下，照样产生混乱的公式。 最终的解决方式，换成kramed引擎！！！去看看官网怎么说的kramed， 其中开头就是， marked hasn’t been evolving as much as it could be lately and due to our needs with GitBook, we need features such as robust mathjax support and want to strive closer to the rising kramdown standard. 它加强了对mathjax的支持，执行如下命令来安装它， 12$ npm uninstall hexo-renderer-marked --save$ npm install hexo-renderer-kramed --save 第一条命令卸载掉marked渲染引擎，第二条命令装上kramed渲染引擎。 果断的解决了问题。 后续突然发现Markdown规则变成了回车换行，显然不能忍，这样根本没法排版， 这个问题和公式问题一样难以接受。 继续搜索，踩了无数坑终于明白了，对于marked引擎，要实现空格+空格+回车换行， 只要在_config.yml文件里面添加下面字段， 123marked: gfm: true breaks: false 因为这里渲染引擎换成了kramed，所以上面的字段当然不管用了，但是， 只要原样修改就好，改成下面， 123kramed: gfm: true breaks: false 总于解决了这个让我差点弃坑的两个严重的问题，o(￣▽￣)o]]></content>
      <categories>
        <category>guide</category>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>guide</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo使用next主题建站全过程]]></title>
    <url>%2F2017%2F08%2F23%2F2017-08-23-build-hexo%2F</url>
    <content type="text"><![CDATA[心路历程之前搭好了github + jekyll的个人博客， 搭建jekyll博客的一些回顾， 写了几天，并没有什么问题，写得还挺开心的。 昨天发现我写在笔记里面的公式会造成本地搜索功能无法使用，整了好久也没能发现问题， 不能忍了，决定把博客搬到github + hexo，毕竟使用的主题next就是Hexo上搬过来的。 jekyll的优点在于它是和github官方结合的，所以只需要按照一定的文件结构建立好仓库， github自己会将它生成变成网站。但是Hexo是本地建立成网站，github不对它进行加工。 所以用jekyll的好处就是你直接传markdown文件就能变成博客上的一篇文章，也就是本地可以不需要任何环境， Hexo必须在本地建立好所需的环境，每次改动都需要本地来重新生成整个网站。但就是因为jekyll的主题丑， 问题有点多，所以试一试Hexo。 本来以为很简单，现在我觉得建立Hexo比jekyll麻烦多了，弄了一天，总算是弄好了， 这里一定要做个总结，坑太多了。 window 10 + github + git + node.js + Hexo 各部分的功能首先，我在github上的用户名叫做isadamu。 github: 托管整个博客，按照仓库https://github.com/isadamu/isadamu.github.io里面的文件， 在域名用户名.github.io下面发布到网络， 也就是可以用户名.github.io这个域名来访问到仓库里的网站。 git: 用于管理仓库，随时来获取仓库里面的文件或者更新仓库里面的文件。 node.js: Hexo运行时需要的环境，同时可以直接使用它来安装hexo。 Hexo: 专门的一个建立博客的工具，将本地的文件转化成对应的html文件，建立起网站所需的所有文件结构， 然后结合git，将这些文件传到仓库。 建立github的仓库首先要有一个github账号，github官网。 然后建立一个叫做用户名.github.io的仓库，这样的一个仓库， github会在它上面启动github pages的功能， 总之，有这样一个功能，我们就能在这个仓库里面建立静态网站了， 也就是可以建立自己的博客，good。 安装配置git先下载一个git安装包，git 安装时就有一个安装路径选一下，别的不用管。 配置一下git的全局变量，不然每次要用git自带的命令行。 将安装路径里面bin文件夹的目录加到全局变量D:\develop\Git\bin。 配置好后，执行： 123$ git --versiongit version 2.14.1.windows.1 出现版本信息表示配置完成，然后就要配置一下ssh key， 用来与github建立起连接，免得以后频繁输入密码。 先配置一下git的用户名和邮箱，改成github账号的用户名和邮箱， 12$ git config --global user.name &quot;isadamu&quot;$ git config --global user.email &quot;myemail@qq.com&quot; 然后输入git config --list可以看到配置成功， 123456789$ git config --list......credential.helper=manageruser.name=isadamuuser.email=myemail@qq.compush.default=simplecredential.helper=wincred 然后开始生成ssh-key，这里需要打开git-bash.exe来输入命令， 它就在git的安装目录下面，输入命令， 1ssh-keygen -t rsa -C &quot;邮件地址&quot; 然后三连回车，意思就是用这个key连接时不需要密码，这时秘钥就生成在 C:\Users\你的用户名\.ssh\id_rsa.pub文件里面，复制文件里面的所有内容， 打开github设置，粘贴，确定。 然后输入， 1$ ssh -T git@github.com 出现， 1Are you sure you want to continue connecting (yes/no)? 输入yes，回车后出现， 1Hi isadamu! You&apos;ve successfully authenticated, but GitHub does not provide shell access. 这就表示链接已经建立成功了。 安装node.js首先，下载node.js 安装同样可以选择一下安装路径，其它不用管。 测试是否安装成功，输入node -v与npm -v， 1234567$ node -vv6.11.2$ npm -v3.10.10 出现版本号说明安装成功。 安装Hexo在命令行里直接输入下面命令开始安装， 1npm install -g hexo 但是所有这类命令的通病，不换源可能很慢，在命令后面加上淘宝源， --registry=https://registry.npm.taobao.org 也就是输入命令， 1npm install -g hexo --registry=https://registry.npm.taobao.org 命令运行完毕之后，输入， 1234567$ hexo -vhexo-cli: 1.0.3...ares: 1.10.1-DEV...openssl: 1.0.2l 出现版本号就表示Hexo安装成功。 初始化博客并发布首先，需要建立一个博客文件夹，这个文件夹以后就是博客的本地所在了。 例如选择F:\nut\git\blog文件夹（文件夹名字无所谓）。 在打开文件夹，在空白处Shift + 鼠标右键，打开命令行， 输入， 1$ hexo init 这个命令会生成一系列文件，这就是一个博客了，它使用了默认的主题landscape， 它在当前目录下的themes文件夹下面。 接着需要输入， 1$ npm install 它会安装一下Hexo所需的依赖，接着输入， 1$ hexo g 这个命令就是按照当前的文件，生成网站的文件结构，接着再输入， 1$ hexo s 这样本地服务器就启动起来了，在浏览器输入localhost:4000就可以看到这个默认模板的样子了， 这样博客就启动成功，另外也可以使用hexo s --debug来启动本地服务器， 这样可以在cmd显示当前的调试信息。 现在就差把博客传送到github上了，打开目录下面的_config.yml文件，在文件中找到 deploy字段，进行配置， 1234deploy: type: git repository: git@github.com:isadamu/isadamu.github.io.git branch: master 这样，Hexo就能够知道该把文件部署到哪里了，但是现在还少一个插件，在命令行里面输入， 1$ npm install hexo-deployer-git --save 安装好这个插件之后，就可以传到github上去了， 1$ hexo d 完成之后，可能要等几分钟，浏览器进入用户名.github.io，出现博客就说明成功了。 选择Next主题注意到Hexo现在它使用的默认主题landscape，那么当然可以自己选择喜欢的模板了。 官方主题库 当然也可到处搜索一下，知乎有哪些好看的 Hexo 主题？. 这里选择了Next模板，毕竟就是因为它才换到Hexo上面来。 到themes文件夹下，clone主题， 1$ git clone https://github.com/iissnan/hexo-theme-next.git 这样就将它clone了下来，注意它clone下来的文件夹名字叫做hexo-theme-next， 这里将它改名为next，因为在配置主题选择字段时，主题名字与主题的文件夹名字对应。 Next主题配置直接先参考官方的说明文档，NexT说明文档。 这里首先需要注意的是配置文件有两个，一个是Hexo的配置文件， 另外一个是主题自己的配置文件，配置时一定要注意。 第二是关于标签和分类页面的添加，一定要注意参考官方说明文档所说， 例如添加标签，先在博客主目录执行， 1$ hexo new page tags 它会在source文件夹下面建立一个tags文件夹，里面会有一个index.md文件， 在这文件里面安照参考文档改就行了，不然就会出现404。 另外添加公益404页面需要在主题的source文件夹下建一个404.html，在里面写上 123456789101112131415161718&lt;!DOCTYPE HTML&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv="content-type" content="text/html;charset=utf-8;"/&gt; &lt;meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" /&gt; &lt;meta name="robots" content="all" /&gt; &lt;meta name="robots" content="index,follow"/&gt; &lt;link rel="stylesheet" type="text/css" href="https://qzone.qq.com/gy/404/style/404style.css"&gt;&lt;/head&gt;&lt;body&gt; &lt;script type="text/plain" src="http://www.qq.com/404/search_children.js" charset="utf-8" homePageUrl="/" homePageName="回到我的主页"&gt; &lt;/script&gt; &lt;script src="https://qzone.qq.com/gy/404/data.js" charset="utf-8"&gt;&lt;/script&gt; &lt;script src="https://qzone.qq.com/gy/404/page.js" charset="utf-8"&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 就可以了。 另外CNAME文件（绑定域名）放在主目录的source文件下面就行。 注意如果在配置文件里面配置了两个相同的字段，会报错，在配置字段时可以先用搜索， 来确定目前没有这个字段。 最重要的公式问题与回车换行问题，愣是整了几个小时，总结如下， Hexo中mathjax公式与回车换行问题 Next主题背景修改要修改Next的主题背景，一个是next/source/css/_custom/custom.styl文件， 在里面加入， 1234body &#123; background: #FFFFFF; background-attachment: fixed;&#125; 其中background就是博客的背景字段，可以使用上面的方式使用纯色，第二个字段使得背景不随页面滑动而滑动， 背景也可以使用图片， 1background:url(/images/backGround.jpg ) 这样就可以引入想要添加的背景图片。 我使用的是Mist，在next/source/css/_schemes/Mist文件夹下面，同样的方式可以修改各个地方。 这里还加入了动态线条，打开next/layout/_layout.swig文件， 在 &lt; /body&gt;之前添加代码， 123&#123;% if theme.canvas_nest %&#125;&lt;script type=&quot;text/javascript&quot; src=&quot;//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js&quot;&gt;&lt;/script&gt;&#123;% endif %&#125; 配置主题的配置文件里面的字段， 1canvas_nest: true 完成。可以参考这个文件的官方文档 部署所需注意的问题部署之前，先停止本地的Hexo服务器。然后，先执行， 1$ hexo clean 先清除一下，这样免得待会上传和本地预览的不一样，然后， 123$ hexo g$ hexo d 等待部署完成就行，如果hexo d执行有问题，就换用git-bash来执行。 不同设备的更新问题如果在另外一台设备上也要写博客，需要安装配置好git + node.js + hexo，配置方法和上面一样， 然后就可以同样的操作了，hexo安装好本体就行，不需要别的操作。 插入网易云音乐的外链到主题中先上效果图， 需要将网易云音乐的外链插入到next/layout/_macro/sidebar.swig里面， 注意插入到图中的位置，也就是Blogroll下面。按道理可以将这个插入到主题的各种地方，只要看得明白主题的布局…]]></content>
      <categories>
        <category>guide</category>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>guide</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Logistic Regression 简单总结]]></title>
    <url>%2F2017%2F08%2F22%2F2017-08-22-logistic-regression%2F</url>
    <content type="text"><![CDATA[Logistic Regression把它看成只有一个神经元的神经网络，激活函数就是sigmoid函数。 模型： \hat y = \sigma (W^T x + b)其中， \sigma(Z) = \frac{1}{1+e^{-Z}} 代价函数： J(W,b) = \frac{1}{m} \sum_{i=1}^m L(\hat y^{(i)},y^{(i)} ) = - \frac{1}{m} \sum_{i=1}^m y^{(i)}log \ \hat y^{(i)} + (1-y^{(i)})log(1 - \hat y^{(i)})视频中好像没有具体说这个损失函数的一个推导，这里自己简单推一推。 首先对于$\sigma(Z)$这个函数，它其实就是经常说到的sigmoid函数， 它的函数图像如下： 可以看出sigmoid函数的值域范围是0~1，所以通常把它当成一个概率来看待， 那么，可以写出y的概率表达式为： P(y) = (\hat y)^y (1 - \hat y)^{1-y}其中$\hat y$就是预测$y$为1的概率，区分两种情况， 上面的式子分开写就是下面的式子，合并在一起有利于公式化： P(y) = \begin{cases} \hat y \ \ \ \ \ \ \ \ \ \ ,y = 1 \\ 1 - \hat y \ \ \ ,y = 0 \end{cases}所以我们采用极大似然法来进行对$W$的求解，似然函数就可以写为： \prod_{i=1}^m (\hat y^{(i)})^{y^{(i)}}(1 - \hat y^{(i)})^{1 - y^{(i)}}对数似然写为： L(\hat y) = \sum_{i=1}^m y^{(i)}log \ \hat y^{(i)} + (1-y^{(i)})log(1 - \hat y^{(i)})加上$- \frac{1}{m}$就得到了上面的代价函数，同时也从最大化变成了最小化。 梯度：首先需要对代价函数进行求导，下面进行推导，令： Z^{(i)} = W^Tx^{(i)} + b \\ \alpha^{(i)}=\sigma (W^Tx^{(i)} + b)单独看一个样本(x^{(i)},y^{(i)})，它的代价函数为， J^{(i)} = y^{(i)}log(\alpha^{(i)})+ (1-y^{(i)})log(1 - \alpha^{(i)})对其中一个W_i进行求导,这里先忽略上标(i)： \frac{\partial J}{\partial W_i}= \frac{y}{a} \frac{\partial \alpha}{\partial W_i} - \frac{1-y}{1-\alpha}\frac{\partial \alpha}{\partial W_i} = ( \frac{y}{a} - \frac{1-y}{1-\alpha} )\frac{\partial \alpha}{\partial W_i}然后其中的$\frac{\partial \alpha}{\partial W_i}$继续计算： \frac{\partial \alpha}{\partial W_i}= \frac{e^{-Z}}{(1+e^{-Z})^2} \frac{\partial Z}{\partial W_i} = \frac {1}{1+e^{-Z}}\frac{e^{-Z} + 1 - 1}{1+e^{-Z}} \frac{\partial Z}{\partial W_i} = \alpha(1 - \alpha)\frac{\partial Z}{\partial W_i}继续对$\frac{\partial Z}{\partial W_i}$进行计算： \frac{\partial Z}{\partial W_i} = x_i将上面的合起来，写为： \frac{\partial J}{\partial W_i} = ( \frac{y}{a} - \frac{1-y}{1-\alpha} )\alpha(1 - \alpha)x_i = (y - \alpha)x_i注意到代价函数为： J(W,b) = - \frac{1}{m} \sum_{i=1}^m y^{(i)}log(\alpha^{(i)}) + (1-y^{(i)})log(1 - \alpha^{(i)})所以考虑到向量化处理，同时加上前面的因子$-\frac{1}{m}$，最终可以写为： \begin{cases} \frac {\partial J}{\partial W} = \frac{1}{m}X(A - Y)^T \\ \frac {\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^m(\alpha^{(i)} - y^{(i)}) \end{cases}其中， A = [\alpha^{(1)},\alpha^{(2)},...,\alpha^{(m-1)},\alpha^{(m)}] \\ X = [x^{(1)},x^{(2)},...,x^{(m-1)},x^{(m)}] \\ Y = [y^{(1)},y^{(2)},...,y^{(m-1)},y^{(m)}] 参数更新权值的更新就按照梯度下降的原则更新就行， \begin{cases} W = W - \alpha \frac {\partial J}{\partial W} \\ b = b - \alpha \frac {\partial J}{\partial b} \end{cases}这里的$\alpha$代表学习因子。 存疑如果直接使用最小二乘损失函数呢？]]></content>
      <categories>
        <category>coursera</category>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>study</tag>
        <tag>mechine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jekyll博客顶端背景修改]]></title>
    <url>%2F2017%2F08%2F22%2F2017-08-22-head-color%2F</url>
    <content type="text"><![CDATA[由于NexT是Hexo上的主题，搬运到jekyll上面之后，目录结构完全不一样，所以网上的参考基本没什么用。 修改head颜色在_sass/_schemes/Mist/_header.scss下面修改， 1.header &#123; background: $whitesmoke; &#125; 为， 1.header &#123; background: #FFC78E; &#125; 然后就变成下面的颜色，]]></content>
      <categories>
        <category>guide</category>
        <category>jekyll</category>
      </categories>
      <tags>
        <tag>guide</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Coursera neural-networks-deep-learning 笔记]]></title>
    <url>%2F2017%2F08%2F22%2F2017-08-22-Andrew-Ng-DeepLearning_1%2F</url>
    <content type="text"><![CDATA[课程主页 学完了吴恩达在Coursera上新开的课程的第一部分neural networks deep learning， 是时候对课程进行一下总结了。 每周内容小回顾先对第一个专项课程所讲的内容进行简单的梳理，后面捡重点单独拿出来进行记录。 week 1第一周视频时间很短，主要就是介绍了神经网络的发展以及它的一些作用， 然后讲了课程的相关内容，如何讨论啊，作业形式啊什么的。 week 2第二周的课程首先引入了Logistic Regression，然后讲了如何对Logistic模型进行训练， 这样就引入了代价函数，梯度下降等等一些机器学习中的一些概念。 另外，后半部分又顺带引入了计算过程的向量化，所以就讲了与作业有关的Jupyter、numpy的相关操作。 week 3第三周主要讲了一个二层的神经网络，当然是先讲了它的前向传播，然后讲了如何向量化计算， 这里主要是为了讲清楚神经网络的层次结构。 然后讲了神经网络中的激活函数，最后给出了反向传播，加上权值的随机初始化。 week 4第四周开始更深层次的神经网络，然后讲了前向和反向传播，并且强调了神经网络每一层的 矩阵维度问题，在程序中要注意维度的正确性。 最后讲了参数设置问题。Parameters：通过网络学习得来的参数。 Hyperparameters：在训练前就要设置，需要多次重复训练来手动调整。 小结讲了构建一个多层的前向神经网络所需要的基本的知识。 Logistic RegressionLogistic Regression总结 初级 Deep Neural NetworksDeep Neural Networks 总结一 课程的一些问题好像没有ppt，笔记写起来很痛苦啊。]]></content>
      <categories>
        <category>coursera</category>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>study</tag>
        <tag>mechine learning</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DNN 笔记一]]></title>
    <url>%2F2017%2F08%2F22%2F2017-08-22-dnn%2F</url>
    <content type="text"><![CDATA[关于第一个专项课程的笔记。 这里先简单的定义为DNN就是层数很多的神经网络。 激活函数神经网络的激活函数一般有以下四种： 它们的方程写为： sigmoid = \frac{1}{1+e^{-Z}} tanh = \frac{e^x-e^{-x}}{e^x+e^{-x}} Relu = max(0,Z) Leaky Relu = max(0.01Z,Z)从图中可以看出，对于sigmoid函数，它的输出大于1，这对于神经网络来说是不利的。 tanh就能对sigmoid函数进行改进，使得输出值的范围在-1到1之间。 但是，对于sigmoid和tanh来说，当输出值Z过大或者过小时， 当我们在求取梯度的时候，这个地方的梯度值就几乎为0，这样就使得在网络层数较多时， 梯度值在某一层就会变为十分接近0，影响到梯度的反向传播。 所以，一般情况下，使用Relu或Leaky Relu来作为神经网络的激活函数， 它不仅能改善梯度的反向传播，还可以加快训练速度，因为作为函数，它的计算算是非常简单。 对它们进行求导，看它们的导函数长什么样： 对于$g(z) = \frac {1}{1+e^{-z}}$，它的导函数为： \frac{d}{dz}g(z) = g(z)(1 - g(z))对于$g(z) = tanh(z)$， \frac{d}{dz}g(z) = 1 - g(z)^2对于$g(z) = Rule(z) = max(0,z)$, \frac{d}{dz}g(z) = \begin{cases} 0 \ \ \ \ \ ,\mbox{if } z < 0 \\ 1 \ \ \ \ \ ,\mbox{if } z > 0 \end{cases}最后对于$g(z) = LeakyRelu(z) = max(0.01z, z)$ ， \frac{d}{dz}g(z) = \begin{cases} 0.01 \ \ \ ,\mbox{if } z < 0 \\ 1 \ \ \ \ \ \ \ \ ,\mbox{if } z > 0 \end{cases}看得出上面的导数其实都很简单，这也有利于快速的计算。 两层神经网络在week_3的作业中，需要构建一个两层的神经网络，这里将它拿出来先分析一下。 注意到这里的激活函数是tanh加上sigmoid，最后一层使用sigmoid的原因， 主要是为了输出一个0~1的值，就可以把它看成概率来处理。（由图中可以看出其中的上标 $[i]$就代表了层数） 参数初始化 这里需要初始化的参数有： 学习速率$\alpha$。 链接权值$w^{[i]}$。 偏置$b^{[i]}$。 对于学习速率$\alpha$，一般将其定义为一个确定的值即可，可能在0.01~1之间， 视情况而定，需要不断调整来获得它较优的取值。 对于偏置$b^{[i]}$，直接将它初始化为0就可以了。 对于权值$w^{[i]}$，这里就不能将它们初始化0，或者可以说不能将它们初始化为一个相同的值， 因为如果它们的初始值相同，它们之间的更新将不会有差异，这样它们会一直取到相同的值， 显然不行。 （如果使用drop-out策略，初始化为相同的值应该也无所谓） 对于权值的初始化，它的好坏对网络的训练也是会有很大影响的，后面再谈。 这里直接随机初始化就行， 123w = np.random.randn(dx,dy) * 0.01b = np.zeros((dx,1)) 这里权值$w$先以标准正态来随机取值，然后乘以0.01，防止权值过大造成计算出的数值过大。 偏置$b$直接初始化为0即可。 前向传播 对于一个输入$x^{(i)}$，它的前向传播过程如下： z^{[1] (i)} = W^{[1]} x^{(i)} + b^{[1] (i)}a^{[1] (i)} = \tanh(z^{[1] (i)})z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2] (i)}\hat{y}^{(i)} = a^{[2] (i)} = \sigma(z^{ [2] (i)}) y^{(i)}_{prediction} = \begin{cases} 1 \ \ \ \ \ \mbox{if } a^{[2](i)} > 0.5 \\ 0 \ \ \ \ \ \mbox{otherwise } \end{cases}这里的输出是一个$a^{[2]}$，它是$z^{[2]}$通过sigmoid函数求出来的， 输出的值可以看出一个概率值，所以这里使用一个交叉熵函数来估计最后的损失。 J = - \frac{1}{m} \sum_{i=0}^{m} (y^{(i)}log(a^{[2](i)}) + (1 - y^{(i)})log(1 - a^{[2](i)}))反向传播求其梯度： 最重要的，就是函数的反向传播过程了，这也是最难的地方， 但实际上它就是一个求取梯度的过程，但是由于函数嵌套太多， 所以容易引起困扰。 这里先忽略前面的系数，就当输入只有一个样本，所以去掉所有的上标$(i)$。 直接看一个最难的，结合上面的图与公式进行推导， \frac{\partial J}{\partial w^{[1]}} = \frac{\partial J}{\partial a^{[2]}} \frac{\partial a^{[2]}}{\partial z^{[2]}} \frac{\partial z^{[2]}}{\partial a^{[1]}} \frac{\partial a^{[1]}}{\partial z^{[1]}} \frac{\partial z^{[1]}}{\partial w^{[1]}}上面就是链式法则，显然要求$w^{[1]}$的偏导，首先要将前面的偏导数全部求出来， 一个一个的写出来。 首先对$a^{[2]}$进行求导，它是在上面的交叉熵函数里面， \frac{\partial J}{\partial a^{[2]}} = \frac{1 - y}{1 - a^{[2]}} - \frac{y}{a^{[2]}}再对$z^{[2]}$进行求导，它通过了一个sigmoid函数， \frac{\partial a^{[2]}}{\partial z^{[2]}} = \frac{\partial}{\partial z^{[2]}}(\frac {1}{1+e^{-z^{[2]}}}) = a^{[2]}(1 - a^{[2]})继续对$a^{[1]}$进行求导，它通过了一个线性运算， \frac{\partial z^{[2]}}{\partial a^{[1]}} = \frac{\partial }{\partial a^{[1]}} (w^{[2]}a^{[1]} + b^{[2]}) = w^{[2]}接着对$z^{[1]}$进行求导，这里通过了一个tanh运算， \frac{\partial a^{[1]}}{\partial z^{[1]}} = \frac{\partial}{\partial z^{[1]}} (\frac{e^{z^{[1]}} - e^{-z^{[1]}}}{e^{z^{[1]}} + e^{-z^{[1]}}}) = 1 - (a^{[1]})^2最后对$w^{[1]}$进行求导，这也是同样的一个线性运算， \frac{\partial z^{[1]}}{\partial w^{[1]}} = \frac{\partial}{\partial w^{[1]}}(w^{[1]}x + b^{[1]}) = x对于偏执$b^{[1]}, b^{[2]}$，它们的导数同理，可以写为， \frac{\partial J}{\partial b^{[2]}} = \frac{\partial J}{\partial a^{[2]}} \frac{\partial a^{[2]}}{\partial z^{[2]}} \frac{\partial z^{[2]}}{\partial b^{[2]}} \frac{\partial J}{\partial b^{[1]}} = \frac{\partial J}{\partial a^{[2]}} \frac{\partial a^{[2]}}{\partial z^{[2]}} \frac{\partial z^{[2]}}{\partial a^{[1]}} \frac{\partial a^{[1]}}{\partial z^{[1]}} \frac{\partial z^{[1]}}{\partial b^{[1]}}其中， \frac{\partial z^{[2]}}{\partial b^{[2]}} = 1 \frac{\partial z^{[1]}}{\partial b^{[1]}} = 1 综合上面的所有式子，可以将各个参数的导数写为， \frac{\partial J}{\partial a^{[2]}} = \frac{1 - y}{1 - a^{[2]}} + \frac{y}{a^{[2]}} \frac{\partial J}{\partial z^{[2]}} = \frac{\partial J}{\partial a^{[2]}} \frac{\partial a^{[2]}}{\partial z^{[2]}} = \frac{\partial J}{\partial a^{[2]}} a^{[2]}(1 - a^{[2]}) = a^{[2]} - y \frac{\partial J}{\partial w^{[2]}} = \frac{\partial J}{\partial z^{[2]}} \frac{\partial z^{[2]}}{\partial w^{[2]}} = \frac{\partial J}{\partial z^{[2]}} a^{[1]} = (a^{[2]} - y)a^{[1]} \frac{\partial J}{\partial b^{[2]}} = \frac{\partial J}{\partial z^{[2]}} \frac{\partial z^{[2]}}{\partial b^{[2]}} = \frac{\partial J}{\partial z^{[2]}} = a^{[2]} - y \frac{\partial J}{\partial a^{[1]}} = \frac{\partial J}{\partial z^{[2]}} \frac{\partial z^{[2]}}{\partial a^{[1]}} = \frac{\partial J}{\partial z^{[2]}} w^{[2]} = (a^{[2]} - y)w^{[2]} \frac{\partial J}{\partial z^{[1]}} = \frac{\partial J}{\partial a^{[1]}} \frac{\partial a^{[1]}}{\partial z^{[1]}} = \frac{\partial J}{\partial a^{[1]}} (1 - (a^{[1]})^2) \frac{\partial J}{\partial w^{[1]}} = \frac{\partial J}{\partial z^{[1]}} \frac{\partial z^{[1]}}{\partial w^{[1]}} = \frac{\partial J}{\partial z^{[1]}} x \frac{\partial J}{\partial b^{[1]}} = \frac{\partial J}{\partial z^{[1]}} \frac{\partial z^{[1]}}{\partial b^{[1]}} = \frac{\partial J}{\partial z^{[1]}} 所以在计算正向传播时，需要将中间计算得到的各个$a^{[i]}$的值保存下来， 因为在反向传播时，会用到它们。 在反向传播的计算中，一层一层从后向前计算梯度， 就可以利用链式法则计算出各个参数的梯度了。 参数更新： 这里先不引入高级的参数更新方式，就正常的更新， w^{[2]} = w^{[2]} - \alpha \frac{\partial J}{\partial w^{[2]}} b^{[2]} = b^{[2]} - \alpha \frac{\partial J}{\partial b^{[2]}} w^{[1]} = w^{[1]} - \alpha \frac{\partial J}{\partial w^{[1]}} b^{[1]} = b^{[1]} - \alpha \frac{\partial J}{\partial b^{[1]}} 更多层的神经网络简单的理解，深度神经网络就是层数比较多的神经网络， 所以基本的操作可以说和上面的两层神经网络是一样的， 下面按照编程题过一遍整个流程，首先是网络结构图， 图中就是一个深度神经网络的结构，它中间神经元的激活函数是ReLU， 最后一层是由sigmoid神经元组成，这一般适用于进行多分类。 由图中，可以看出整个神经网络的一个训练步骤： 初始化各层的权值$W$和$b$。 输入样本，进行前向传播，先进过一系列的ReLU激活函数， 最后再通过一个sigmoid，得到最后得输出。 对于得到的输出，计算它与样本的标签值的交叉熵，得到当前的损失。 将损失反向传播。 更新各个参数，继续从第2步开始。 注意在前向传播的过程中，要存下每一层的输出，因为在反向传播时需要用到它们。 各层参数的维度： 明白各层参数的维度，对于理解神经网络的代码很重要，这里写出它们的矩阵维度。 首先，假设一个输入样本是一个12288长度的行向量，它是由一幅64*64*3的图片展开而成的， 同时假设输入共有209个样本。 同时需要假设如下的一些东西： 网络一共有$L$层。 样本数$m = 209$。 样本维度$n = 12288$。 第$i$层的神经元数量有$n^{[i]}$个。 最后一层就是输出层，按上面的定义，它神经元的个数就是$n^{[L]}$个。 从输入开始，假设每个样本是一个列向量，那么输入就是一行输入样本， X = \begin{bmatrix} x^{[1]} & x^{[2]} & ... & x^{[m-1]} & x^{[m]} \end{bmatrix}其中$x^{[i]}$就是一个样本，它是一个列向量， x^{[i]} = \begin{bmatrix} x_1^{[i]} \\ x_2^{[i]} \\ ... \\ x_{n-1}^{[i]} \\ x_n^{[i]} \end{bmatrix}所以输入$X$的维度是，$X \in R^{n \times m}$。 为了方便线性运算，也就是之间$WX + b$来计算，这里把每个神经元定义为一个行向量， 并且它的长度等于它上一层的神经元数。 W^{[i](j)} = \begin{bmatrix} w_{1}^{[i](j)} & w_{2}^{[i](j)} & ... & w_{n^{[i-1]}-1}^{[i](j)} & w_{n^{[i-1]}}^{[i](j)} \end{bmatrix}其中$W^{[i](j)}$就表示第$i$层的第$j$个神经元。那么对于$W^{[i]}$，它就等于， W^{[i]} = \begin{bmatrix} W_1^{[i]} \\ W_2^{[i]} \\ ... \\ W_{n^{[i]}-1}^{[i]} \\ W_{n^{[i]}}^{[i]} \end{bmatrix}所以第$i$层的神经元的权值矩阵$W^{[i]}$的维度为，$W^{[i]} \in R^{n^{[i]} \times n^{[i-1]}}$，另外， z^{[i]} = W^{[i]}a^{[i-1]} + b \in R^{n^{[i]} \times m} a^{[i]} = g(z^{[i]}) \in R^{n^{[i]} \times m} 将它们的维度总结如下，当然它们所对应的导数和它们的维度一致， X \in R^{n \times m} z^{[i]} \in R^{n^{[i]} \times m} a^{[i]} \in R^{n^{[i]} \times m} W^{[i]} \in R^{n^{[i]} \times n^{[i-1]}} b^{[i]} \in R^{n^{[i]} \times 1} 前向传播： 只需要注意保存中间变量。 反向传播： 结合对两层神经网络的描述，其实反向传播的计算也很简单， 就从后向前计算导数就好了。 Hyperparameter： 超参数，就是一些需要在训练前就确定好的参数，并且可以认为一旦训练开始， 就不能再改变它们的值了，在神经网络中，超参数控制着$W$和$b$的最终取值， 可以认为它们决定了网络的最终性能。 例如下面的一些参数， 学习速率$\alpha$。 迭代次数$t$。 网络层数$L$。 某一层的神经元数$n^{[1]},n^{[2]},…$ 激活函数的选择。 对于这样一些的参数的选择，通常情况都得按着经验来，现在的很多研究也都是围绕着如何调参来展开的。 一般情况只能一个一个的去试，看哪一组参数最合适，所以这是一个很耗时间的过程。 总结第一个专项的课程，主要还是在于引入深度神经网络，讲了最为重要的前向传播和反向传播， 理解好前向与反向传播，对于神经网络的学习至关重要。]]></content>
      <categories>
        <category>coursera</category>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>study</tag>
        <tag>mechine learning</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode 71.Simplify Path]]></title>
    <url>%2F2017%2F08%2F21%2F2017-08-21-simplfiy-path%2F</url>
    <content type="text"><![CDATA[71.Simplify Path题目： Given an absolute path for a file (Unix-style), simplify it. For example,path = &quot;/home/&quot;, =&gt; &quot;/home&quot;path = &quot;/a/./../../c/&quot;, =&gt; &quot;/c&quot; 另外题目需要注意的是当出现&quot;/../&quot;时，返回&quot;/&quot;,多个&quot;/&quot;看成一个就行。 理解： 输入一个路径，将路径简单化，也就是去掉其中的&quot;..&quot;,&quot;.&quot; 以及多余的&quot;/&quot;,另外路径的最后不要加上&quot;/&quot;。 代码： 12345678910111213141516171819202122232425class Solution &#123; public String simplifyPath(String path) &#123; String[] strs = path.split("/"); String[] res = new String[strs.length]; int idx = 0; for ( String str : strs ) &#123; if ( str.length() == 0 || str.equals(".") ) &#123; continue; &#125; else if ( str.equals("..") ) &#123; if ( idx != 0 ) &#123; idx--; &#125; &#125; else &#123; res[idx++] = str; &#125; &#125; if ( idx == 0 ) return "/"; StringBuilder res_str = new StringBuilder(); for ( int i = 0; i &lt; idx; i++ ) &#123; res_str.append("/").append(res[i]); &#125; return res_str.toString(); &#125;&#125; 复杂度：O(n)，运行时间7ms。 这里有一个值得注意的地方，最开始，我是这样写的： 1String[] strs = path.split("/+"); 之后改成了， 1String[] strs = path.split("/"); 瞬间从15ms击败25%，变成了击败95%。所以split的多字符匹配还是要慢得多。]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>study</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[朴素贝叶斯+垃圾邮件分类器]]></title>
    <url>%2F2017%2F08%2F21%2F2017-08-21-spam-classifier%2F</url>
    <content type="text"><![CDATA[Naive Bayes 朴素贝叶斯关于吴恩达的cs229课程的一份笔记，关于朴素贝叶斯分类器在垃圾邮件分类上的应用。 spam classifier 垃圾邮件分类器样本: 过去时间收到的所有邮件，共$m$封。 步骤： 1.首先建立词典$X$，词典中共有$n$个词。 2.通过样本估计概率， P(X_i = 1|y = 1)=\Phi_{i|y=1}其中$X_i \in {0,1}$，表示词典中的第$i$个词是否出现，$X_i = 1$表示出现。 P(X_i = 1|y = 0)=\Phi_{i|y=0}上面的$y \in {0,1}$，并且$y=1$表示邮件为垃圾邮件。令 P(y=1)=\Phi_y然后利用极大似然法估计概率值。 3.通过上面得到 $P(X|y)$ 与 $P(y)$的概率， 对于新的输入$X$， 就可以用$P(y|X)$就可判断$y$。 改进这里考虑词的出现次数，对上面的分类器进行改进。 这个模型称为：Multinomial Event Model。 步骤： 1.建立词典，词典的词的总数为$V$。 2.对于一封邮件，将其转化为向量X，X_i的表示第i个词在词典中的位置。 例如，邮件开头为A NIPS ...，其中A在词典的第1个， NIPS在词典的35000个，则X_1 = 1, \ X_2 = 35000。 3.通过样本估计概率， P(X_j = k | y = 1)= \Phi_{k|y=1}其中$k \in {0,1,…,n}$，对$X_j=k$表示邮件中的第$j$个词在词典中的位置为$k$。 P(X_j = k | y = 0)= \Phi_{k|y=0}同样的，上面$y \in {0,1}$，对于$y=1$，它就表示邮件为垃圾邮件。令： P(y = 1)= \Phi_y同样的，利用极大似然法估计概率值。 4.通过上面得到$P(X|y)$与$P(y)$的概率， 对于新的输入$X$， 就可以用$P(y | X)$来判断$y$。]]></content>
      <categories>
        <category>cs229</category>
      </categories>
      <tags>
        <tag>mechine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[购买域名并绑定博客原地址]]></title>
    <url>%2F2017%2F08%2F20%2F2017-08-20-add-domain-name%2F</url>
    <content type="text"><![CDATA[其实我也不知道为什么要买一个域名，对我自己来说没什么卵用， 但是这样很爽，虽然挺贵的… 购买域名这里我的域名是在godaddy 上购买的， 就买一个.com的域名就行，另外还买了隐私保护，不管有没有什么大用， 反正已经买了。 这里我买了5年的，主要买的时候使用优惠码，直接搜索引擎搜索“godaddy 优惠码”就行， 隐私服务可买可不买，买了之后，购买信息就变了， 域名花费300￥，隐私保护200￥，一共5年，就当我自己买了个移动硬盘… 域名绑定先操作github，在github的工程下面建一个CNAME文件，加入一行域名，我的longrm.com。 github会自动按这个域名来转。 或者这里也可以不建这个文件，进入repository的setting里面，custom一下也行， 第二步就是去godaddy下面改域名的设置，参考： Configuring a Godaddy domain name with github pages Setting up an apex domain 简单总结参考里面的内容，更改下图两条变成所示就行了: 将A的值改成192.30.252.153。 将CNAME的www那一条改成用户名.github.io，这里我的就是isadamu.github.io。 最后浏览器输入买的域名，longrm.com，大功告成：]]></content>
      <categories>
        <category>guide</category>
      </categories>
      <tags>
        <tag>guide</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建jekyll博客的一些回顾。]]></title>
    <url>%2F2017%2F08%2F19%2F2017-08-19-build-blog-review%2F</url>
    <content type="text"><![CDATA[其实简单回顾，这一切并没有多复杂…所需要的东西也不多。 环境以及工具运行环境：windows 10博客部署位置：github博客引擎：jekyll 需要下载安装的工具： ruby github desktop 流程梳理从最开始梳理整个流程： 拥有一个github账号。 在账户下建立 用户名.github.io 的repository。 决定使用jekyll。 寻找一个自己喜欢的jekyll模板。 fork或者git这个模板到本地。 本地安装jekyll。 按照模板的介绍，结合jekyll调整模板配置。 上传模板到用户名.github.io，结束。 创建github账号github 建立repository一定要建立名字叫做用户名.github.io的repository， 这样github就会在这个repository下面启动github page 。 建好后： 决定是否使用jekyll目前据自己研究，主要就是jekyll和hexo两种方式， 好像hexo更好用？ 但是无所谓了，先用jekyll感受下。 寻找jekyll模板首先，当然是参考知乎了:有哪些简洁明快的 Jekyll 模板？ 第二，主要还是去比较官方的找了 jekyllthemes jekyllthemes.io 最后选择了NexT 主要是简洁，但是功能却一个不少。 另外，这是一个Hexo上的模板，作者Yitao Yao搬过来的， 今天看了看知乎才知道这是Hexo上最火的模板…大众审美。 下载这个模板到本地首先去到模板的github上的地址，然后复制它的git地址。 然后用github客户端clone到本地。 这样模板就得到了，甚至上传模板到用户名.github.io，就已经可以了。 但是为了获得“修改配置以及写博客时可以本地预览效果，觉得好就上传，不好就不上传”的效果， 需要安装jekyll来进行本地预览。 安装jekylljekyll一般用ruby来安装，所以首先安装ruby 。 下载下来后，一路按next就可以了。 打开命令行，输入 ruby -v, 123$ ruby -vruby 2.4.1p111 (2017-03-22 revision 58053) [x64-mingw32] 出现版本信息，那么ruby安装就成功了。 由于国内与国外网络之间的一些隔阂，这里需要换一下ruby的源， 否则待会安装jekyll可能很慢或者卡住。 国内源 ， 以前国内源域名http://ruby.taobao.org/ ， 但是现在改到https://gems.ruby-china.org/ 了。 12345$ gem sources --add https://gems.ruby-china.org/ --remove https://rubygems.org/$ gem sources -lhttps://gems.ruby-china.org# 确保只有 gems.ruby-china.org 先用ruby安装Bundler， 12345678$ gem install bundlerFetching: bundler-1.15.3.gem (100%)Successfully installed bundler-1.15.3Parsing documentation for bundler-1.15.3Installing ri documentation for bundler-1.15.3Done installing documentation for bundler after 6 seconds1 gem installed 安装好Bundler之后，这时候就需要更改命令行位置到执行目录到模板文件夹。 这里可以先打开模板文件夹，按住shift键，在文件夹空白处右键， 就会有一个类似在此处打开CMD的选项。然后执行命令： 1234567$ bundle install...Thank you for installing html-pipeline!You must bundle Filter gem dependencies.See html-pipeline README.md for more details.https://github.com/jch/html-pipeline#dependencies 注意这条命令实际上是去安装了文件夹下的Gemfile文件下的记录的文件， 更重要的是在文件里面有一条source &#39;https://rubygems.org&#39;， 这样它又把源换成了国外，将它注释掉，换成source &#39;https://gems.ruby-china.org/&#39; 。 这样，所需的所有环境就都搭好了，只要保证所有软件都是最新版本的，应该不会出现问题。 下一步就是同样在模板文件夹下，运行， 1$ bundle exec jekyll serve --watch 没有问题的话，本地服务器就运行起来了， 进入浏览器，打开网址 localhost:4000，就能预览到博客的效果了。 注意这里命令后面加了--watch，这使得你更改博客时，预览效果也会实时改变。 调整模板配置每个模板都会有它自己的配置说明，这里NexT有很完善的NexT使用说明 ，毕竟用的人多。 这里配置遇到问题就是侧边栏只能在右边，可能是因为移植的问题，反正没太大影响，无所谓。 上传模板首先，使用github客户端clone下用户名.github.io这个repository，方法和上面一样， 当然，如果它没有文件不知道能不能clone，试试https://github.com/用户名/用户名.github.io.git 这个url来clone。 clone下来之后，复制模板文件夹下的所有文件到用户名.github.io文件夹下。 查看github客户端，它会检测到改动。这时先在本地commit一下这个改动， 在箭头处输入改动的标题，想更多描述也行。 然后在1处本地确定这次改动，在2处点击即可将本地的改动push到github上。 这样，在浏览器打开用户名.github.io这个地址，就可以看到自己的博客了！！！ 写博客写博客就在本地用户名.github.io这个文件夹下的_posts下面写Markdown。 想加图片就在用户名.github.io这个文件夹建立一个images文件夹，图片放到里面， 在Markdown里面用{去掉{site.url}}/images/my.png这个链接就行。 然后仍然可以在用户名.github.io这个文件夹下用bundle exec jekyll serve --watch命令， 来进行本地预览localhost:4000。 觉得可以之后，就用github客户端push就ok了。]]></content>
      <categories>
        <category>guide</category>
        <category>jekyll</category>
      </categories>
      <tags>
        <tag>guide</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Everflow 阅读报告]]></title>
    <url>%2F2017%2F08%2F18%2F2017-08-18-everflow%2F</url>
    <content type="text"><![CDATA[Everflow 阅读报告文献名：Packet-Level Telemetry in Large Datacenter Networks （Zhu Y, Kang N, Cao J, et al. Packet-level telemetry in large datacenter networks[C]//ACM SIGCOMM Computer Communication Review. ACM, 2015, 45(4): 479-491.） 作者：Yibo Zhu、Nanxi Kang、Jiaxin Cao、Albert Greenberg、Guohan Lu、Ratul Mahajan、Dave Maltz、Lihua Yuan、Ming Zhang、Ben Y. Zhao、Haitao Zheng 关键字：Datacenter network; failure detection; probe 引言：在复杂网络中去排查故障是一件很困难的事，要做到及时的解决故障，需要做到： 在大规模的网络流量中识别出受影响的报文。 穿过多个网络组件去追踪它们。 分析流量轨迹中的故障模式。 测试并确定潜在的故障原因。 Everflow 就是一个大型网络中基于报文级别的遥测系统，使用商用交换机所具有的“match and mirror”功能来追踪特定的报文。它将抓取到的报文送到分析服务器，并且发出“guided probes”来测试并确定潜在的故障。 它的特点在于它是报文级别的，粒度细，并且随着数据中心实时运行，对当前的一小部分报文进行追踪，可以实时检测到故障。另外当操作员发现某些故障时，也可以对Everflow进行配置，注入探针报文去定位这些故障。 对于Silent packet drop、Silent blackhole、Inflated end-to-end latency、Loops from buggy middlebox routing、Load imbalance和Protocol bugs等一些问题，传统的工具是很难检测到故障或者很难定位到故障的，但是这些问题Everflow都可以发现，并且能够将这些问题定位到某个路由器或者某条链路。 另外，它所需占用的资源在数据中心尺度来看是很小的，基本可以不予考虑。 Everflow架构： Everflow主要由四个部分组成：controller、analyzer、storage和reshuffler。在这之上，多个Everflow application与Controller进行交互，利用Everflow提供的信息来诊断网络故障。 Reshuffler：Reshuffler负责将追踪的报文进行镜像，然后送到Analyzer。其中主要的问题是数据中心的流量巨大，不可能追踪所有的报文，只能有选择的进行追踪。需要使用下面一些措施。 在交换机上进行匹配和镜像。商用的数据中心交换机可以预定义匹配规则， 并对匹配报文执行确定的操作，且不改变原报文的转发行为。 Everflow利用这一点来减少追踪的负载， 用三种匹配规则来处理DCN（DataCenter Network）的通常故障。 第一，由于数据中心流的大小分布十分不均匀， 所以以TCP报文的SYN、FIN和RST部分来进行匹配， 随机在n个踪迹中选择一个踪迹进行追踪。 第二，某一些问题可能需要追踪特定应用的特定端口的报文， 或者两个服务器之间的报文， 所以这里在这样的报文中加入一个特殊的“debug bit”， 交换机将对这样的报文进行追踪。第三，对一些协议报文（例如BGP、PFC和RDMA）， 它们在网络中是属于很小的一部分，但是又十分重要， 所以Everflow会追踪所有这些的协议报文。 Switch-based reshuffler。我们需要一个低花费的策略来reshuffle这些追踪流量。为了实现这个策略，首先在HMux（hardware Mux）中定义一个VIP（virtual IP），配置所有交换机将踪迹报文转发到VIP，当一个踪迹报文到达HMux，HMux依据报文的五元组将报文重定向到一个DIP（direct IP，它就对应一个分析服务器）。这样就保证了同一五元组的报文只会被重定向到相同的DIP。 Analyzers：分析器。由分布式服务器组组成，每台服务器处理一部分“tracing traffic”。 每一个分析器保持着两个状态：追踪报文和计数器。 追踪报文。分析器保持着一个报文踪迹表，表中保存了同一个报文（由五元组和IPID定义）的镜像链。它保存原报文的一份全拷贝，一系列的逐跳信息，逐跳信息包括报文被镜像的交换机IP地址、时间戳、TTL、源MAC地址和DSCP/ECN。当在一秒以内不再有新的报文，就认为一条踪迹结束。 在一条结束的报文踪迹中，分析器会检查它是否有循环或丢包问题。当相同的设备在一条踪迹中出现多次时，就认为出现了循环。当一条踪迹的最后一跳与所期望的最后一跳不同时，就认为发生了丢包。 由于需要追踪的报文的数量十分巨大，为了减小存储压力，每一个分析器只会将 异常行为、设置了调试位（例guided probes）或者协议相关报文（例PFC和BGP）的踪迹信息写入存储单元。对剩下的踪迹，分析器会将它们以类型的区别聚合到计数列表，并每隔10秒写入存储单元。最后controller还会将每个分析器的计数合并到一起。 链路负载计数器。对每条链路，分析器会计算出链路的总负载（报文数、比特数和flow数）。另外它还可以对指定前缀的流量或指定的内部流量来进行更细粒度的统计，这个操作可以通过controller来动态增减。 延迟计数器。分析器通过“guided probes”来计算每一条链路的延迟。 镜像报文丢包计数。镜像报文也有可能会发生丢包，如下图5所示，当踪迹包含S2却不包含S1时，明显表示从S1镜像出来的报文发生了丢失。在实际部署中镜像报文丢失率很低，通过将镜像报文避开拥塞链路就能很好的解决这个问题。 Storage：这里使用了SCOPE数据库，它是一个可扩展的分布式数据处理系统。数据用表的形式进行存储。这里将报文踪迹按行进行存储，每一行存储一条报文踪迹。 Controller APIs：Everflow applications通过一些API来与控制器进行交互，通过这些API，应用可以查询报文踪迹、添加细粒度的负载计数器、触发guided probes以及选定踪迹添加调试bit。 它有GetTrace()、GetCounter()、AddCounter()、RemoveCounter()、Probe()以及EnableDbg()、DisableDbg() 这样一些操作函数。 guided probing。由于故障的发生可能有多种原因，被动的追踪报文可能不足以确定这些的原因，这时就需要重新注入报文来判断故障。这种能够在任意交换机上注入任意报文的能力，就叫做guided probing。 如上图（a），虽然追踪到了这个丢包，但是我们不能确定这个丢包是随机的还是持续的，通过guided probing，我们将报文p的复制注入到S2（如图b），确定丢包是随机的还是持续的。更进一步，可以设置探针报文为不同的五元组，测试丢包是随机的，还是针对某个特定的五元组。 如上图（a），当使用被动追踪中去测量链路延迟时（也就是正常的追踪），可能由于追踪报文的两次返回所经过的路径不同，造成测量不正确。guided probing不仅可以注入报文，还可以指定报文经过的路径，通过这一特点，设置探针报文经过S1 -&gt; S2 -&gt; S1，这样报文就会经过S1两次，这两次的时间间隔就是链路延迟的两倍。 故障排查：Latency profiler。当两个服务器之间的延迟变得很高时，为了去查询问题的原因，首先，对两个服务器之间的TCP SYN报文的debug bit进行标志，追踪这些报文，就可以路径上所经过的网络设备。然后，注入guided probes去测量逐跳延迟。有了这些信息，就能快速定位到发生问题的网络设备。 Packet drop debugger。发生丢包的原因可能有很多，例如拥塞、软件bug或者配置错误，这就需要调试器对发生丢包的踪迹进行检查。对于一个遭到丢弃的报文p，通过最后抓取到p的交换机Sn，可以推断出p所期望的下一跳。这样，调试器就会注入guided probes到Sn，确定丢包是否是持续的，确定丢包是否有固定模式（例如特定的五元组）。 Loop debugger。循环在数据中心网络中并不常见，但是它一旦出现，就将消耗大量的资源。当一个循环被检测到时（踪迹上相同设备出现两次），首先调试器注入guided probes去测试循环是不是持续的，如果循环是持续的，它会将循环路径上的设备进行上报，于是操作员就可以禁用设备上的端口来打破循环。在这期间调试器会持续注入guided probes，直到循环消失。 ECMP profiler。在数据中心网络中，交换机会用ECMP（等价路由）来切分流量到下一跳。但可能由于不好的哈希函数或者路由问题，会造成链路拥塞。对每一个交换机，ECMP profiler将会它的链路总负载。当检测到不均衡的负载划分时，它会启动更加细粒度的负载计数器来排查，检测这样的不均衡的负载划分是对所有的流还是特定前缀的流。这些信息可以帮助操作员快速的检测并定位故障。 RoCEv2-based RDMA debugger。RoCEv2-based RDMA（Remote DirectMemoryAccess）是一个新兴协议，它对网络的延迟要求非常的高。在数据中心网络中，有时由于NIC的一些软件bug，使得RDMA达不到理想的性能，但是NIC是由第三方厂商所提供的，不能得到其中的代码，于是就需要用调试器来处理这个问题。调试器追踪关于RDMA的所有控制报文，这些信息不仅能够观察到RDMA流的实际行为，也能调试第三方的代码问题。 （文章中第七部分提到了一些这些问题在实际中的一些处理的例子）]]></content>
      <categories>
        <category>paper</category>
        <category>network</category>
      </categories>
      <tags>
        <tag>study</tag>
        <tag>read</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode 15.3Sum]]></title>
    <url>%2F2017%2F08%2F18%2F2017-08-18-3Sum%2F</url>
    <content type="text"><![CDATA[15.3Sum Description: Given an array S of n integers, are there elements a, b, c in S such that a + b + c = 0? Find all unique triplets in the array which gives the sum of zero. Note: The solution set must not contain duplicate triplets. 题目理解：输入一个数组和一个目标值，求3个数之和等于目标值。返回所有这些3个数的组合，但是不能有重复的组合。 1234567891011121314151617181920212223242526272829public class Solution &#123; public List&lt;List&lt;Integer&gt;&gt; threeSum(int[] nums) &#123; List&lt;List&lt;Integer&gt;&gt; result = new ArrayList&lt;&gt;(); Arrays.sort(nums); int target = 0, m = nums.length; for ( int i = 0, bound = m - 2 ; i &lt; bound &amp;&amp; nums[i] &lt;= target ; ) &#123; int l = i + 1, r = m - 1, sum = target - nums[i]; while ( l &lt; r ) &#123; int temp_sum = nums[l] + nums[r]; if ( temp_sum &lt; sum ) l++; else if ( temp_sum &gt; sum ) r--; else &#123; List&lt;Integer&gt; item = new ArrayList&lt;&gt;(); item.add(nums[i]); item.add(nums[l]); item.add(nums[r]); result.add(item); l++; while ( l &lt; r &amp;&amp; nums[l] == nums[l-1] ) l++; r--; while ( l &lt; r &amp;&amp; nums[r] == nums[r+1] ) r--; &#125; &#125; i++; while ( i &lt; bound &amp;&amp; nums[i] == nums[i-1] ) i++; &#125; return result; &#125; &#125; 首先将数组进行排序，外层循环从左向右，内部就是一个Two_Sum，这里注意要跳过重复的值。 复杂度n^2，运行时间69ms。]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>study</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客bug记录]]></title>
    <url>%2F2017%2F08%2F18%2F2017-08-18-blog-bugs%2F</url>
    <content type="text"><![CDATA[2017-08-18 公式符号导致搜索功能无法使用（jekyll）本来以为弄好了博客，结果发现搜索功能不能使用了，重新来了一遍，控制变量法。终于发现问题是因为一个公式内的符号&amp;，造成的问题。 去掉公式内的&amp;符号后，一切恢复正常… 2017-08-20 image标签与photos标签（jekyll）试了一下在文件头加入这两个标签，想试一试加上封面的效果， 结果发现image标签我根本用不了，但是photos标签可以用， 不过需要注意的是photos标签后面内链直接source/images/...， 不再需要前面那个site.url。 但是用了photos标签后，文章内部会有一个加载失败的图片，看来是用不了了。 hexo Error: fatal: bad config line 1 in file .git/config（hexo）今天在部署博客时，突然出现了错误， 1hexo Error: fatal: bad config line 1 in file .git/config 完全不明白为什么，又没有改过配置文件。 搜索了发现是.deploy_git/.git/config文件里面的配置出现了问题，不是hexo的问题，而是git的问题。 这里的config文件里面的内容不知道为什么被修改了，变成了乱码。 删除了config文件，找了一个idea下面的一个同步工程的config文件，但是还是报错。 最后找了一个同步之前的config文件，才总于解决了问题。 里面的配置为： 12345678910[core] repositoryformatversion = 0 filemode = false bare = false logallrefupdates = true symlinks = false ignorecase = true[branch &quot;master&quot;] remote = git@github.com:isadamu/isadamu.github.io.git merge = refs/heads/master]]></content>
      <categories>
        <category>guide</category>
        <category>jekyll</category>
      </categories>
      <tags>
        <tag>guide</tag>
        <tag>bug</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode 18.4Sum]]></title>
    <url>%2F2017%2F08%2F18%2F2017-08-18-4Sum%2F</url>
    <content type="text"><![CDATA[18.4Sum Description: Given an array S of n integers, are there elements a, b, c, and d in S such that a + b + c + d = target? Find all unique quadruplets in the array which gives the sum of target. Note: The solution set must not contain duplicate quadruplets. example: For example, given array S = [1, 0, -1, 0, -2, 2], and target = 0. A solution set is: [ [-1, 0, 0, 1], [-2, -1, 1, 2], [-2, 0, 0, 2] ] 题目理解：与3Sum一个意思。 12345678910111213141516171819202122232425262728293031323334public class Solution &#123; public List&lt;List&lt;Integer&gt;&gt; fourSum(int[] nums, int target) &#123; List&lt;List&lt;Integer&gt;&gt; result = new ArrayList&lt;&gt;(); Arrays.sort(nums); int m = nums.length; for ( int idx1 = 0, bound1 = m - 3 ; idx1 &lt; bound1 &amp;&amp; nums[idx1] * 4 &lt;= target ; ) &#123; int target3 = target - nums[idx1]; for ( int i = idx1 + 1, bound = m - 2 ; i &lt; bound &amp;&amp; nums[i] * 3 &lt;= target3; ) &#123; int l = i + 1, r = m - 1, sum = target3 - nums[i]; if ( sum &lt;= nums[r] * 2 &amp;&amp; sum &gt;= nums[l] * 2 ) &#123; while ( l &lt; r ) &#123; int temp_sum = nums[l] + nums[r]; if ( temp_sum &lt; sum ) l++; else if ( temp_sum &gt; sum ) r--; else &#123; List&lt;Integer&gt; item = new ArrayList&lt;&gt;(); item.add(nums[idx1]);item.add(nums[i]);item.add(nums[l]);item.add(nums[r]); result.add(item); l++; while ( l &lt; r &amp;&amp; nums[l] == nums[l-1] ) l++; r--; while ( l &lt; r &amp;&amp; nums[r] == nums[r+1] ) r--; &#125; &#125; &#125; i++; while ( i &lt; bound &amp;&amp; nums[i] == nums[i-1] ) i++; &#125; idx1++; while ( idx1 &lt; bound1 &amp;&amp; nums[idx1] == nums[idx1-1] ) idx1++; &#125; return result; &#125;&#125; 内部包装了一个3Sum，但是其中一些中断的条件很重要，能够提升不少性能。 复杂度：n^3, 运行时间：28ms。 代码2： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114public class Solution &#123; public List&lt;List&lt;Integer&gt;&gt; fourSum(int[] nums, int target) &#123; List&lt;List&lt;Integer&gt;&gt; result = new ArrayList&lt;&gt;(); if ( nums.length &lt; 4 ) return result; Arrays.sort(nums); int[] ynums = new int[nums.length]; int repeat = 1, idx = 1; ynums[0] = nums[0]; for ( int i = 1; i &lt; nums.length; i++ ) &#123; if ( nums[i] == nums[i-1] ) &#123; repeat++; &#125; else &#123; repeat = 1; &#125; if ( repeat &lt; 4 ) &#123; ynums[idx++] = nums[i]; &#125; else if ( repeat == 4 &amp;&amp; nums[i]*4 == target ) &#123; myAdd( result, nums[i], nums[i], nums[i], nums[i]); &#125; &#125; int m = idx, ts_i = 0; int[][] two_sum = new int[m*(m-1)/2][3]; for ( int i = 0; i &lt; m - 1; i++ ) &#123; repeat = 1; while ( i &lt; m - 1 &amp;&amp; ynums[i + 1] == ynums[i] ) &#123; i++; repeat++; &#125; if ( repeat == 2 ) &#123; two_sum[ts_i][0] = ynums[i] * 2; two_sum[ts_i][1] = i - 1; two_sum[ts_i][2] = i; ts_i++; &#125; else if ( repeat == 3 ) &#123; two_sum[ts_i][0] = ynums[i] * 2; two_sum[ts_i][1] = i - 1; two_sum[ts_i][2] = i - 1; ts_i++; &#125; for ( int j = i + 1; j &lt; m; ) &#123; two_sum[ts_i][0] = ynums[i] + ynums[j]; two_sum[ts_i][1] = i; two_sum[ts_i][2] = j; ts_i++; j++; while ( j &lt; m &amp;&amp; ynums[j] == ynums[j-1] ) &#123; j++; &#125; &#125; &#125; Arrays.sort(two_sum, 0, ts_i, new MyCompare()); int l = 0, r = ts_i - 1; while ( l &lt; r ) &#123; int sum = two_sum[l][0] + two_sum[r][0]; if ( sum &lt; target ) &#123; l++; &#125; else if ( sum &gt; target ) &#123; r--; &#125; else &#123; if ( two_sum[l][2] &lt; two_sum[r][1] ) &#123; myAdd( result, ynums[two_sum[l][1]], ynums[two_sum[l][2]], ynums[two_sum[r][1]], ynums[two_sum[r][2]]); &#125; int ll = l + 1, rr = r - 1; while ( ll &lt; r &amp;&amp; two_sum[ll][0] == two_sum[ll-1][0] ) &#123; if ( two_sum[ll][2] &lt; two_sum[r][1] ) &#123; myAdd( result, ynums[two_sum[ll][1]], ynums[two_sum[ll][2]], ynums[two_sum[r][1]], ynums[two_sum[r][2]]); &#125; ll++; &#125; while ( ll &lt; rr &amp;&amp; two_sum[rr][0] == two_sum[rr+1][0] ) &#123; if ( two_sum[rr][1] &gt; two_sum[l][2] ) &#123; myAdd( result, ynums[two_sum[l][1]], ynums[two_sum[l][2]], ynums[two_sum[rr][1]], ynums[two_sum[rr][2]]); &#125; rr--; &#125; l++; r--; &#125; &#125; return result; &#125; private void myAdd( List&lt;List&lt;Integer&gt;&gt; result, int a1, int a2, int b1, int b2) &#123; List&lt;Integer&gt; item = new ArrayList&lt;&gt;(); item.add(a1); item.add(a2); item.add(b1); item.add(b2); result.add(item); &#125; private class MyCompare implements Comparator&lt;int[]&gt; &#123; public int compare(int[] o1, int[] o2) &#123; if ( o1[0] &gt; o2[0] ) &#123; return 1; &#125; else if ( o1[0] &lt; o2[0] ) &#123; return -1; &#125; else &#123; if ( o1[1] &lt; o2[1] ) &#123; return -1; &#125; else if ( o1[1] &gt; o2[1] ) &#123; return 1; &#125; else &#123; if ( o1[2] &lt; o2[2] ) &#123; return -1; &#125; else &#123; return 1; &#125; &#125; &#125; &#125; &#125;&#125; 贼复杂，想法到是很简单，就是先计算两两数的和，然后就变成了类似2Sum的东西，但是其中的多种重复情况，使得编写程序十分复杂。 并且代码还可以优化，但是懒得想了，就不该开始这个代码(；′⌒`)。 复杂度：n^2log(n)(个人简单估计是这样的，但是常数因子很大，而且占用空间很多)。 运行时间：85ms。]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>study</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的第一篇博客 for jekyll]]></title>
    <url>%2F2017%2F08%2F18%2F2017-08-18-my-first-blog%2F</url>
    <content type="text"><![CDATA[写博客主题说明书 写博客直接在_post文件夹里面写，直接新建md文件即可。 图片放入到source/images里面。 这里文件名格式是：时间+题目。 其中文件名中的空格用”-“来代替，不能用中文名，否则报错找不到路径。 写博客时，可以使用 bundle exec jekyll serve --watch 命令来运行jekyll。 然后打开 127.0.0.1:4000 来查看实时效果。 写完之后，使用github客户端进行push即可。 之后便可打开我的博客 https://isadamu.github.io/ 查看了。当然，可能刷新要几分钟左右。 博客头部常用的关键字：--- title: // 文章题目 date: // 文章日期 description: // 对这篇文章的简单描述 categories: // 对文章的分类，细分，多添加，ps:只能英文 tags: // 文章的标签，广义，多添加，ps:只能英文 link: // 对文章标题加上超链接 image: // 相当于加上一个图片封面 photos: // 多添加，感觉与上面类似？ type: // 没看出来作用。。。 --- 需要注意的是category、tag不能包含中文。 代码高亮不高亮代码就直接缩进四个空格， public static void main(String[] args) { System.out.println(&quot;高亮前&quot;); } 或者， `\`` public static void main(String[] args) { System.out.println(&quot;高亮前&quot;); } `\`` 高亮代码语法(主要去掉”\“)： `\``java public static void main(String[] args) { System.out.println(&quot;高亮后&quot;); } `\`` 123public static void main(String[] args) &#123; System.out.println("高亮后");&#125; 公式12# 行间公式：$$Y=W^&#123;T&#125;X+b$$ Y=W^{T}X+b1行内公式：$ Y=W^&#123;T&#125;X+b $ 行内$ Y=W^{T}X+b $公式 图片插入图片，内链， 1![image](\&#123;\&#123;site.url\&#125;\&#125;/source/images/head/head_bigfan.png) 去掉site.url旁边的4个’\‘。 当然这里也可以用外链， 1![image](https://img3.doubanio.com/view/photo/photo/public/p2022836553.webp) 插入图片大小的调整： 12&lt;div align="center"&gt;&lt;img src="&#123;&#123;site.url&#125;&#125;/source/images/head/head_bigfan.png"style="zoom:50%" title="大范" /&gt;&lt;/div&gt; 也可以用， 12&lt;div align="center"&gt;&lt;img src="&#123;&#123;site.url&#125;&#125;/source/images/head/head_bigfan.png"width="200" height="200" title="大范" /&gt;&lt;/div&gt; 语法Markdown语法 MathJax基本语法 在线公式编辑器，快速知道语法 如果想要换行，在一行的后面空两个空格就行，例： 12这一行后没有两个空格所以不能换行 这一行后没有两个空格 所以不能换行 12这一行后有两个空格 所以能换行 这一行后有两个空格所以能换行]]></content>
      <categories>
        <category>guide</category>
        <category>jekyll</category>
      </categories>
      <tags>
        <tag>guide</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode 1.Two Sum]]></title>
    <url>%2F2017%2F08%2F18%2F2017-08-18-two-sum%2F</url>
    <content type="text"><![CDATA[1 Two Sum题目描述：Given an array of integers, return indices of the two numbers such that they add up to a specific target. You may assume that each input would have exactly one solution, and you may not use the same element twice. 题目理解：输入一个数组和一个目标值，若数组中两个值之和等于目标值，返回它们的下标，每个数只能使用一次。 代码1： 123456789101112131415public class Solution &#123; public int[] twoSum(int[] nums, int target) &#123; Map&lt;Integer,Integer&gt; diffs = new HashMap&lt;&gt;(); Integer i = null, index = null; for (i = 0; i &lt; nums.length ; i++) &#123; if ( ( index = diffs.get( nums[i] ) ) != null ) &#123; break; &#125; else &#123; diffs.put( target - nums[i] , i); &#125; &#125; int[] indexs = &#123;index,i&#125;; return indexs; &#125;&#125; 重头开始遍历，查看当前数字是否在差值中，在其中就返回，否则计算差值，加入map。 复杂度：nlog(n)，运行时间10ms。 代码2： 12345678910111213141516171819202122232425262728public class Solution &#123; public int[] twoSum(int[] nums, int target) &#123; int[][] inums = new int[nums.length][2]; for ( int i = 0; i &lt; nums.length; i++ ) &#123; inums[i][0] = nums[i]; inums[i][1] = i; &#125; Arrays.sort(inums, new Comparator&lt;int[]&gt;() &#123; public int compare( int[] o1, int[] o2 ) &#123; return o1[0] - o2[0]; &#125; &#125;); int[] res = &#123;-1,-1&#125;; int l = 0, r = inums.length - 1; while ( l &lt; r ) &#123; int sum = inums[l][0] + inums[r][0]; if ( sum &lt; target ) &#123; l++; &#125; else if ( sum &gt; target ) &#123; r--; &#125; else &#123; res[0] = inums[l][1]; res[1] = inums[r][1]; return res; &#125; &#125; return res; &#125;&#125; 将原始数据排序，然后进行左右逼近，得到结果。 复杂度nlog(n),运行时间12ms。]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>study</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode 16.3Sum Cloest]]></title>
    <url>%2F2017%2F08%2F18%2F2017-08-18-3Sum-Closest%2F</url>
    <content type="text"><![CDATA[16.3Sum Cloest Description: Given an array S of n integers, find three integers in S such that the sum is closest to a given number, target. Return the sum of the three integers. You may assume that each input would have exactly one solution. example For example, given array S = {-1 2 1 -4}, and target = 1. The sum that is closest to the target is 2. (-1 + 2 + 1 = 2). 题目理解：输入一个数组和一个目标值，求数组中三个数的和，返回和与目标值之间的最小差值。 1234567891011121314151617181920212223public class Solution &#123; public int threeSumClosest(int[] nums, int target) &#123; int dis = Integer.MAX_VALUE, result = -1, m = nums.length; Arrays.sort(nums); for ( int i = 0, bound = m - 2 ; i &lt; bound &amp;&amp; dis != 0 &amp;&amp; nums[i] - target &lt; dis; ) &#123; int l = i + 1, r = m - 1; while ( l &lt; r ) &#123; int sum = nums[i] + nums[l] + nums[r]; int temp_dis = sum - target; temp_dis = temp_dis &gt; 0 ? temp_dis : -temp_dis; if ( temp_dis &lt; dis ) &#123; dis = temp_dis; result = sum; &#125; if ( sum - target &lt; 0 ) l++; else r--; &#125; i++; while ( i &lt; bound &amp;&amp; nums[i] == nums[i-1] ) i++; &#125; return result; &#125;&#125; 方法和3Sum几乎一样，只是多进行一下距离的比较。 复杂度 n^2, 运行时间 18ms.]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>study</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cs231n Assignment 1]]></title>
    <url>%2F2017%2F07%2F16%2F2017-07-16-cs231n-assignment1%2F</url>
    <content type="text"><![CDATA[cs231n : assignment-1 笔记作业内容： knn svm softmax two_layer_net features 作业所使用数据集 ：本次作业所有算法的数据集均使用CIFAR-10，它是一个包含不同物体图片的数据集，图片大小为32*32*3。 123# Load the raw CIFAR-10 data.cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir) 数据集中图片示例： knn (k-Nearest Neighbor)knn 也就是k近邻，它的思想很简单，也就是将训练样本中与测试样本距离最近 k 个样本看作标签的预测。当然它也可以用来做聚类，思想方法与分类一样。 距离的计算一般使用 L1 distance 或 L2 distance d_1(I_1, I_2) = \sum_P{|I_1^P - I_2^P |}d_1(I_1, I_2) = \sqrt{\sum_P{(I_1^P - I_2^P)^2}}knn 分类器训练步骤： 训练阶段：只需要将训练数据存储下来即可。 测试阶段：输入测试样本，计算它与所有训练样本之间的两两距离，取出前k个最近的训练样本，它们中最多的标记就作为测试样本的标记。 knn 需要的训练参数只有 k ，这类参数也叫做交叉验证参数(cross-validated)，也就是我们在对模型进行交叉验证时，需要调整的参数。 从 knn 的训练过程可以看出，knn 基本没有训练时间的消耗，但是它在进行样本分类时，由于需要计算与所有训练样本间的距离，所有它的样本分类很耗时间（有一些方法可以应用来减小计算量）。 jupter notebook :由于原始的数据太多，首先对样本进行下采样，否则算法将会非常慢。 123456789101112131415# Subsample the data for more efficient code execution in this exercisenum_training = 5000mask = list(range(num_training))X_train = X_train[mask]y_train = y_train[mask]num_test = 500mask = list(range(num_test))X_test = X_test[mask]y_test = y_test[mask]# Reshape the image data into rowsX_train = np.reshape(X_train, (X_train.shape[0], -1))X_test = np.reshape(X_test, (X_test.shape[0], -1))print(X_train.shape, X_test.shape) (5000, 3072) (500, 3072) 训练部分代码：12345# Create a kNN classifier instance. # Remember that training a kNN classifier is a noop: # the Classifier simply remembers the data and does no further processing classifier = KNearestNeighbor()classifier.train(X_train, y_train) 123def trian(self, X, y): self.X_train = X self.y_train = y 很明显，训练部分就是将训练数据存下来了而已，没有进行任何操作。 测试部分（使用 L2 distance）：12345678910111213141516# Let's compare how fast the implementations aredef time_function(f, *args): import time tic = time.time() f(*args) toc = time.time() return toc - tictwo_loop_time = time_function(classifier.compute_distances_two_loops, X_test)print('Two loop version took %f seconds' % two_loop_time)one_loop_time = time_function(classifier.compute_distances_one_loop, X_test)print('One loop version took %f seconds' % one_loop_time)no_loop_time = time_function(classifier.compute_distances_no_loops, X_test)print('No loop version took %f seconds' % no_loop_time) 1234# two loopfor i in xrange(num_test): for j in xrange(num_train): dists[i][j] = np.sqrt(np.sum(np.square(self.X_train[j,:] - X[i,:]))) 1234# one loopfor i in xrange(num_test): temp = np.sqrt(np.sum(np.square(self.X_train - X[i,:]), axis = 1)) dists[i,:] = temp.T 1234567# no loop# 这里需要将 L2 的公式给写开，得到三个和式，分别计算出三部分，# 就可以计算出距离，也就避免了显式的循环x_square = np.sum(np.square(X), axis=1, keepdims=True)xtr_square = np.sum(np.square(self.X_train), axis=1, keepdims=True)two_x_y = 2*X.dot(self.X_train.T)dists = np.sqrt((x_square + xtr_square.T) - two_x_y) Two loop version took 20.160266 secondsOne loop version took 34.393572 secondsNo loop version took 0.665433 seconds 测试部分要求实现二重循环、一重循环和无循环三种计算距离的方式，从结果可以看出有无循环在计算时间上的巨大差异。证明在实现算法时，通过矩阵运算来直接计算的重要性。 交叉验证部分 (Cross-validation) :由于我们可以选择不同的 k 值，所以可以使用交叉验证来测试不同k值下，knn 在数据集中的准确率，于是可以选择出最佳的 k 值。 对每一个k值，不能只进行一次测试，误差可能导致结果选择不准确，所以使用5折交叉验证。将训练数据分成五等份（随机划分），每次使用一份作为验证集，其余的作为训练集，进行五次计算，平均值作为其最终的准确率。 1234567891011121314151617181920212223num_folds = 5k_choices = [1, 3, 5, 8, 10, 12, 15, 20, 50, 100]for k in k_choices: this_k_cross = [] for j in range(num_folds): xtf_copy = X_train_folds.copy() ytf_copy = y_train_folds.copy() del(xtf_copy[j]) del(ytf_copy[j]) xtf_train = np.vstack(xtf_copy) ytf_train = np.hstack(ytf_copy) xtf_val = X_train_folds[j] ytf_val = y_train_folds[j] classifier.train(xtf_train, ytf_train) cross_idx = num_folds - 1 if cross_idx == j : cross_idx = 0 y_pred = classifier.predict(xtf_val, k) num_correct = np.sum(ytf_val == y_pred) accuracy = float(num_correct) / each_num this_k_cross.append(accuracy) k_to_accuracies[k] = this_k_cross 画出的图如下：\ 1234567891011# 由图中看出最优的k值在10左右，这里选取10best_k = 10classifier = KNearestNeighbor()classifier.train(X_train, y_train)y_test_pred = classifier.predict(X_test, k=best_k)# Compute and display the accuracynum_correct = np.sum(y_test_pred == y_test)accuracy = float(num_correct) / num_testprint('Got %d / %d correct =&gt; accuracy: %f' % (num_correct, num_test, accuracy)) Got 141 / 500 correct =&gt; accuracy: 0.282000 得到在测试集上的准确率为28.2%。 knn小结 ：knn 算法十分简单，主要涉及的就是距离的计算，所以它不能应对较为复杂的分类任务。\ 最后得到的准确率30%不到，效果很不好，还要注意到本来就能有10%的准确率，因为一共有10个分类，所以随机划分的准确率就是10%。 svm (Support Vector Machine)SVM，也就是支持向量机，是一个很常用的算法，也是一个很有效的算法，在分类任务中经常会使用到它。\ 具体SVM的一些性质暂时先不管，因为它拥有很多的形式，也有很多的推导，加上它还有核函数的trick，所以算法实际上有很多的内容。但在这里就简单的使用即可。 初始定义 : f(x_i,W,b) = Wx_i+bs_j = f(x_i, W)_jL_i = \sum_{j \neq y_i}{max(0, s_j - s_{y_i} + \Delta)}其中 Li 就是损失函数，首先这里我们判断一个检测样本的划分是看 f 函数得出的值最大的那一维，所以这里损失函数表达的是其它类的得分与正确类的得分之间的差值，并且差距至少为 Delta。这个损失函数一般称为 hinge loss。 加入Regularization ： 如果只用上面的损失函数来进行计算，它对权值 W 的取值大小是不敏感的，也就是 W 取值很大或很小时都能得到相同的结果，加入正则化，可以迫使 W 取较小的值，这里当然也有一定防止过拟合的作用。 正则函数定义： R(W) = \frac 12\sum_k{\sum_l{W_{k,l}^2}}L = \frac 1N \sum_i{L_i} + \lambda R(W)正则化后的损失函数就是上面的 L , 在交叉验证中，Delta 的选取其实并不十分重要（一般等于1.0），因为算法总是能够适应过来，更为重要的是正则化强度 lambda 的选取。 jupyter notebook :数据预处理 ： 这里数据集选取的大小就不像 knn 这么保守， 12345# As a sanity check, print out the shapes of the dataprint('Training data shape: ', X_train.shape)print('Validation data shape: ', X_val.shape)print('Test data shape: ', X_test.shape)print('dev data shape: ', X_dev.shape) Training data shape: (49000, 3072)Validation data shape: (1000, 3072)Test data shape: (1000, 3072)dev data shape: (500, 3072) 这里将训练数据划分为训练本分，交叉验证部分、测试部分和快速检测部分四个部分，其中训练部分数据远远大于剩下的部分，与通常所说的70%、20%、10% 不同，也许在数据较多的时候，只要保证验证集和测试集数据能够正确检验模型即可。其中的快速检测部分用来检验代码的正常性，首先利用它来证明算法的编写没有问题，然后再开始正常的表演。 1234567891011121314151617# first: compute the image mean based on the training datamean_image = np.mean(X_train, axis=0)# second: subtract the mean image from train and test dataX_train -= mean_imageX_val -= mean_imageX_test -= mean_imageX_dev -= mean_image# third: append the bias dimension of ones (i.e. bias trick) so that our SVM# only has to worry about optimizing a single weight matrix W.X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])print(X_train.shape, X_val.shape, X_test.shape, X_dev.shape) (49000, 3073) (1000, 3073) (1000, 3073) (500, 3073) 这里首先进行了数据的零均值化，第二步使用了加入 bias 的 tirck ，也就是在训练数据中增加一维数据，值全部为1，这样我们就可以将 bias 并入权重里面，于是在计算时就无需再单独去考虑 bias。 损失函数与梯度 ： 123456789101112131415161718192021def svm_loss_naive(W, X, y, reg): num_classes = W.shape[1] num_train = X.shape[0] loss = 0.0 for i in xrange(num_train): scores = X[i].dot(W) correct_class_score = scores[y[i]] for j in xrange(num_classes): if j == y[i]: continue margin = scores[j] - correct_class_score + 1 # note delta = 1 if margin &gt; 0: loss += margin dW[:,j] += X[i] dW[:,y[i]] += -X[i] loss /= num_train loss += 0.5 * reg * np.sum(W * W) dW /= num_train dW += reg * W 首先是简单粗暴的循环版本，注意到在 max 函数的梯度计算时，对于为0的部分不计算梯度，只有非零的时候才去计算。在非零时，有两组权值加入了运算，正确分类的权值和当前分类的权值，所以这两组权值都会求得梯度。如下所示： max(0, W_j^Tx_i - W_{y_i}^Tx_i + \Delta) > 0l = W_j^Tx_i - W_{y_i}^Tx_i + \Delta\frac {\partial l}{\partial W_j} = x_i\frac {\partial l}{\partial W_{y_i}} = -x_i上面的式子，就对应了上面代码中 margin &gt; 0 的部分，调整了 dW 的第 j 列和 y{i} 列。 第二步是尝试去除显式的循环： 12345678910111213# computer lossm = X.shape[0]scores = X.dot(W)right_scores = scores[range(m), y].reshape(m, 1)temp_loss = np.maximum(0, scores - right_scores + 1)temp_loss[range(m), y] = 0loss = np.sum(temp_loss)/m + 0.5 * reg * np.sum(W * W)# computer gradienttemp_loss[temp_loss &gt; 0] = 1row_sum = np.sum(temp_loss, axis = 1)temp_loss[range(m), y] = -row_sumdW += np.dot(X.T, temp_loss)/m + reg * W loss 的向量化计算并不复杂，将得分中的正确得分置零，再求和即可。 对 dW 的向量化就不这么好想（这里想了很久才做出来）， 对于 temp_loss 中的每一行，就代表了一个样本 x[i] 与 W 矩阵相乘，再与 0 取 maximum 所得到的取值。 单看 temp_loss 一行，在求取梯度之后，一个 j列 不为零的值，导致 dW 的 j 列需要加上一个 x[i] , y[i] 列则需要减去 x[i] , 所以 row_sum 就计算了需要调整的次数。 最后利用矩阵乘法的 trick ，注意乘出来的维度要与 dW 一致。 12345678910# compare time_use tic = time.time()_, grad_naive = svm_loss_naive(W, X_dev, y_dev, 0.000005)toc = time.time()print('Naive loss and gradient: computed in %fs' % (toc - tic))tic = time.time()_, grad_vectorized = svm_loss_vectorized(W, X_dev, y_dev, 0.000005)toc = time.time()print('Vectorized loss and gradient: computed in %fs' % (toc - tic)) Naive loss and gradient: computed in 0.097084sVectorized loss and gradient: computed in 0.004936s 很明显，向量化之后效率大大提升。 训练部分（随机梯度下降） ： 通过上面的验证，保证了代码的正确性，开始使用训练集进行训练。 这里所说的是使用随机梯度下降（SGD, Stochastic Gradient Descent），但事实上这里其实使用的是 mini-batch 每次随机选出部分样本，进行迭代与权重的调整。 123456789def train(self, X, y, learning_rate=1e-3, reg=1e-5, num_iters=100, batch_size=200, verbose=False): for it in xrange(num_iters): idxs = np.random.choice(num_train, batch_size, replace = True) X_batch = X[idxs,:] y_batch = y[idxs] loss, grad = self.loss(X_batch, y_batch, reg) loss_history.append(loss) self.W -= learning_rate * grad 123svm = LinearSVM()loss_hist = svm.train(X_train, y_train, learning_rate=1e-7, reg=2.5e4, num_iters=1500, verbose=True) 画出 loss 随迭代变化的图： 这里的曲线十分光滑，不知道为什么，感觉在 mini-batch 下应该会有起伏才对 12345# Accuracy in training and validation sety_train_pred = svm.predict(X_train)print('training accuracy: %f' % (np.mean(y_train == y_train_pred), ))y_val_pred = svm.predict(X_val)print('validation accuracy: %f' % (np.mean(y_val == y_val_pred), )) training accuracy: 0.376714validation accuracy: 0.377000 交叉验证 ： 12345678910111213141516learning_rates = [1e-7, 1e-6, 5e-5]regularization_strengths = [1e5, 2.5e4, 5e4, 1e3]num_iters = 3000for lr in learning_rates: for rs in regularization_strengths: svm = LinearSVM() svm.train(X_train, y_train, lr, rs, num_iters) y_train_pred = svm.predict(X_train) y_val_pred = svm.predict(X_val) ytr_acc = np.mean(y_train == y_train_pred) yval_acc = np.mean(y_val == y_val_pred) results[(lr,rs)] = (ytr_acc, yval_acc) if yval_acc &gt; best_val : best_val = yval_acc best_svm = svm best validation accuracy achieved during cross-validation: 0.394000 这部分首先参数设置很诡异啊，学习步长非常小，正则强度又非常之大，计算时在学习步长为 5e-5 时会出现数值爆炸的情况。 取得在交叉验证集中的准确率为 39.4%。 1234# Evaluate the best svm on test sety_test_pred = best_svm.predict(X_test)test_accuracy = np.mean(y_test == y_test_pred)print('linear SVM on raw pixels final test set accuracy: %f' % test_accuracy) linear SVM on raw pixels final test set accuracy: 0.370000 取得在测试集中的准确率为 37.0%，比 knn 只好 10% 左右。 将权值可视化，如下图所示： 从图像中可以看出图中大概的形状以及背景颜色与它所要区分的类别很相近。个人认为由于首先零均值化之后，数据中有正有负，然后这里 SVM 只涉及到乘法，当权值与样本数据接近时，得到的得分就会比较高，也是因为这里太简单了，所以导致效果并不是很好。 SVM小结 ：这里使用了多分类的SVM，可以看出效果并不很好，事实上，可以使用 one-vs-all 的方式或者使用核函数来进行分类，相信会得到更好的结果。 softmax classifiersoftmax classifier 也是一种常用的分类器，它其实长得很像 Logistic Regression ，下面是它的损失函数的定义： L_i = -log \left(\frac {e^{f_{y_i}}}{\sum_j{e^{f_j}}}\right)f(x_i;W) = Wx_i其中 W 为权值，xi 为第 i 个样本。可以把它写开成为等价形式， L_i = -f_{y_i} + log \sum_j{e^{f_j}}这样的损失函数称为交叉熵损失（cross-entropy loss），其中， f_j(z) = \frac {e^{z_j}}{\sum_k{e^{z_k}}}就是 softmax function。从直观来说，softmax function 把得分转化为了概率，然后用类似熵的式子来使得正确分类的概率最大化（因为要最小化损失函数）。 在实际计算中，如果不对分数进行处理就直接带入损失函数，由于其中涉及到 e 的幂次，可能导致计算上数值过大造成问题，所以需要对其做一些处理，如下， \frac {e^{f_{y_i}}}{\sum_j{e^{f_j}}} \rightarrow \frac {Ce^{f_{y_i}}}{C\sum_j{e^{f_j}}} = \frac {e^{f_{y_i} + logC}}{\sum_j{e^{f_j + logC}}}这样就相当于要所有得分共同加上一个固定值 logC ，并且这对计算的结果不会产生影响。于是，通常情况下我们取， logC = -\max_jf_j这样就不会带来计算上溢出的问题，同时也不会影响到计算结果，所以也不用花费额外的精力去关注它。 加入正则化（与 SVM 一样），写出最终的损失函数如下， L = \sum_i^m -log\left(\frac {e^{f_{y_i} + logC}}{\sum_j{e^{f_j + logC}}}\right) + \lambda R(W)jupyter notebook :数据预处理 ： 123456789101112131415161718192021222324252627282930# Load the raw CIFAR-10 dataX_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)# Preprocessing: reshape the image data into rowsX_train = np.reshape(X_train, (X_train.shape[0], -1))X_val = np.reshape(X_val, (X_val.shape[0], -1))X_test = np.reshape(X_test, (X_test.shape[0], -1))X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))# Normalize the data: subtract the mean imagemean_image = np.mean(X_train, axis = 0)X_train -= mean_imageX_val -= mean_imageX_test -= mean_imageX_dev -= mean_image# add bias dimension and transform into columnsX_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])print('Train data shape: ', X_train.shape)print('Train labels shape: ', y_train.shape)print('Validation data shape: ', X_val.shape)print('Validation labels shape: ', y_val.shape)print('Test data shape: ', X_test.shape)print('Test labels shape: ', y_test.shape)print('dev data shape: ', X_dev.shape)print('dev labels shape: ', y_dev.shape) Train data shape: (49000, 3073)Train labels shape: (49000,)Validation data shape: (1000, 3073)Validation labels shape: (1000,)Test data shape: (1000, 3073)Test labels shape: (1000,)dev data shape: (500, 3073)dev labels shape: (500,) 这里使用的数据同样是 CIFAR10 ， 代码首先读入数据，并将每一幅图片都转化为行向量。 将数据划分为四部分，和 SVM 时操作一样。 零均值化。 使用 bias trick，加入一列 1 向量。 损失函数和梯度 ： 首先实现循环版本的代码， 12345678910111213141516m = X.shape[0]c = W.shape[1]for i in xrange(m) : score = X[i].dot(W) score_mean = score - np.max(score) score_exp = np.exp(score_mean) sum_se = np.sum(score_exp) loss += (np.log(sum_se) - score_mean[y[i]]) for j in xrange(c): dW[:,j] += (score_exp[j] / sum_se) * X[i] if j == y[i]: dW[:,j] -= X[i]loss /= mloss += 0.5 * reg * np.sum(W * W)dW /= m dW += reg * W 循环中，将计算分为了每个样本分别计算，利用了损失函数的变形来编写，其中， L_i = -f_{y_i} + log\sum_j{e^{f_j}} \frac {\partial L_i}{\partial W_j} = \frac {\partial L_i}{\partial f_j} \frac {\partial f_j}{\partial W_j} = \begin{cases} \frac {e^{f_j}}{\sum_j{e^{f_j}}} x_i && ,j \neq y_i \\ \frac {e^{f_j}}{\sum_j{e^{f_j}}} x_i - x_i && ,j = y_i \end{cases}循环中就体现了上面的式子，在最后再加入正则化即可。 1234567# Generate a random softmax weight matrix and use it to compute the loss.W = np.random.randn(3073, 10) * 0.0001loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)# As a rough sanity check, our loss should be something close to -log(0.1).print('loss: %f' % loss)print('sanity check: %f' % (-np.log(0.1))) loss: 2.367492sanity check: 2.302585 上面这一段使用了随机的初始权值来对 loss 进行了计算，然后与 -log(0.1) 进行了比较，在期望下两个值之间应该十分接近，因为在随机情况下，每一个样本在 softmax 之后它得每一类划分的概率应该都是 0.1，在取熵之后就是接近 -log(0.1)。 梯度检测 ： 这部分留到最后在统一说，通常在算法实现时，我们要先检查自己的梯度计算是否有问题，以保证在正常训练时不会出现问题。 向量化 ： 123456789101112131415161718# lossm = X.shape[0]scores = X.dot(W)scores_mean = scores - np.max(scores, axis = 1).reshape(m, 1)scores_exp = np.exp(scores_mean)sum_row = np.sum(scores_exp, 1)loss += -np.sum(np.log(scores_exp[range(m), y]/sum_row))# gradientcoef = scores_exp / sum_row.reshape(m,1)coef[np.arange(m), y] -= 1dW = X.T.dot(coef)# regularizationloss /= mloss += 0.5 * reg * np.sum(W * W)dW /= mdW += reg * W 在经历过 SVM 的向量化之后，这里的向量化就和它有一定类似，只不过 SVM 时，系数为 1 ，在这里系数是概率，外加正确值减去一个 -1，然后同样使用矩阵乘法所得结果需要与 dW 的规模一致的 trick，得到代码的机构。 123456789tic = time.time()loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)toc = time.time()print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))tic = time.time()loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)toc = time.time()print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic)) naive loss: 2.349745e+00 computed in 0.113950svectorized loss: 2.349745e+00 computed in 0.011255s 两次计算的结果一致，向量化的速度是非向量化的 10 倍。 交叉验证 ： 123456789101112131415161718learning_rates = [1e-7, 5e-7]regularization_strengths = [2.5e4, 5e4]it_num = 1500for lr in learning_rates : for rs in regularization_strengths : softmax = Softmax() softmax.train(X_train, y_train, lr, rs, it_num) y_train_pred = softmax.predict(X_train) y_val_pred = softmax.predict(X_val) ytr_acc = np.mean(y_train == y_train_pred) yval_acc = np.mean(y_val == y_val_pred) results[(lr,rs)] = (ytr_acc, yval_acc) if yval_acc &gt; best_val : best_val = yval_acc best_softmax = print('best validation accuracy achieved during cross-validation: %f' % best_val) best validation accuracy achieved during cross-validation: 0.372000 得到交叉验证集中的最高准确率为 37.2%。 1234# Evaluate the best softmax on test sety_test_pred = best_softmax.predict(X_test)test_accuracy = np.mean(y_test == y_test_pred)print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, )) softmax on raw pixels final test set accuracy: 0.369000 得到在测试集中的准确率为 36.9%。 将权值可视化之后得到图如下， 得到的图像与之前的 SVM 几乎是一样的，因为在实际上它们所观察的，都是在 W*xi 中的最大取值，它们之间的学习步长以及正则化强度的取值大小几乎也是一样的，所以最终得到的准确率几乎也一样。 softmax小结 : 从 softmax 的函数形式就可以看出，它适合的是多分类的场景，但它在 CIFAR10 上的表现并不好，说明在这种过于复杂的任务中，它很难轻易的表现的很好，可能还需要对数据进行一些特征提取的工作，来提升它的性能。 Two_Layer_Neural_Network这里要求实现一个简单的二层神经网络（也可以称作一个隐藏层的神经网络），神经网络的东西单独写，先写题。 如图所示就类似于题目要求的神经网络，输入层 + 隐藏层 + 输出层，完成多分类任务。更为具体的，题目要求网络结构为， input - fully connected layer - ReLU - fully connected layer - softmax。 jupyter notebook :前向传播（Forward Pass） ： 123# Forward Passh = np.maximum(0, X.dot(W1) + b1)scores = h.dot(W2) + b2 前向传播十分简单，两个矩阵乘法就能搞定。 损失函数(loss function) : 12345678# lossscores_mean = scores - np.max(scores, 1, keepdims=True)scores_exp = np.exp(scores_mean)sc_sum = np.sum(scores_exp, 1, keepdims=True)y_pred = scores_exp / sc_sumloss = -np.sum(np.log(y_pred[range(N), y]))loss /= Nloss += reg * (np.sum(W1 * W1) + np.sum(W2 * W2)) 注意这里的损失函数是 softmax 。 反向传播(Backward Pass) : 1234567891011# Backward Passdscores = y_pred.copy()dscores[range(N), y] -= 1dscores /= Ngrads['W2'] = h.T.dot(dscores) + 2 * reg * W2grads['b2'] = np.sum(dscores, axis = 0)h_grad = dscores.dot(W2.T)h_grad[h &lt;= 0] = 0grads['W1'] = X.T.dot(h_grad) + 2 * reg * W1grads['b1'] = np.sum(h_grad, axis = 0) 这里写了很多遍也不对，对照别人的代码才知道这里的误差起始直接用的就是 y_pred ，也就是说正确类的误差为 1 - fj ，错误类的误差为 fj - 0，也就是说我们期望正确类的概率为 1 ，其余为 0， 不用先对 softmax 求导再传播。另外这里的误差还要除以样本数 N，没想明白？？？。 另外反向传播需要用到前向传播的中间值，这里就是 h, y_pred 。 反向传播在书写代码时，利用矩阵乘法的 trick 可以迅速梳理出写法。 第一次传播就直接回传 第二次传播是 hinge function ，它是个不连续的函数，我们处理它的反向传播时，将前向传播时为 0 的值，不再反向传播即可。 加上正则化 训练部分 ： 12345678910111213# mini-batchidx = np.random.choice(num_train, batch_size, replace=True)X_batch = X[idx]y_batch = y[idx]# loss, gradsloss, grads = self.loss(X_batch, y=y_batch, reg=reg)# weight updateself.params['W1'] -= learning_rate * grads['W1']self.params['b1'] -= learning_rate * grads['b1']self.params['W2'] -= learning_rate * grads['W2']self.params['b2'] -= learning_rate * grads['b2'] 使用mini-batch的方法来训练，权值更新十分简单，训练过程如下图， 图像中 loss 的变化是一直在跳变的，但是它总体的趋势是在下降，这就是在训练中使用 batch model 以外的训练方法中会看到的场景，因为每次我们沿梯度下降时，寻找的是本次使用样本的梯度下降方向，而不是全局的下降方向。 1234567891011121314input_size = 32 * 32 * 3hidden_size = 50num_classes = 10net = TwoLayerNet(input_size, hidden_size, num_classes)# Train the networkstats = net.train(X_train, y_train, X_val, y_val, num_iters=1000, batch_size=200, learning_rate=1e-4, learning_rate_decay=0.95, reg=0.25, verbose=True)# Predict on the validation setval_acc = (net.predict(X_val) == y_val).mean()print('Validation accuracy: ', val_acc) Validation accuracy: 0.287 在上面定义了一个隐藏层为 50 个神经元的一个神经网络，这里只迭代 1000 次，得到一个粗略的 Validation accuracy 为 28.7% ，感觉很差。所以下面要进行交叉验证来找到一个好的参数，神经网络时常干的就是这么一个调参的工作。 先看当前训练得到权值的一个长相， 只有车的长相比较明显。 交叉验证 ： 123456789101112131415161718192021# cross-validationhidden_size = [ 100, 200, 300, 400, 500 ] learning_rate = [ 1e-3, 1e-4]reg = [ 0.15, 0.25, 0.45]for hs in hidden_size: for lr in learning_rate: for re in reg: net = TwoLayerNet(input_size, hs, num_classes) # Train the network stats = net.train(X_train, y_train, X_val, y_val, num_iters=4000, batch_size=400, learning_rate=lr, learning_rate_decay=0.99, reg=re, verbose=True) # Predict on the validation set val_acc = (net.predict(X_val) == y_val).mean() print('Validation accuracy: ', val_acc, ' hs = ', hs, ' lr = ', lr, ' re = ', re) if val_acc &gt; best_val: best_val = val_acc best_net = net Validation accuracy: 0.528 hs = 500 lr = 0.0005 re = 0.2 这里的训练过程非常慢，事实上我看效果不好的参数就主动停止了它。已经可以预见在网络更为复杂的时候，光用 cpu 来跑神经网络的困难所在了。 这里得到了 52.8% 的验证集准确率。 将这个复杂的网络权值进行可视化， 图中还是可以依稀辨识出形状的。 12test_acc = (best_net.predict(X_test) == y_test).mean()print('Test accuracy: ', test_acc) Test accuracy: 0.546 得到最后在测试集上的准确率为 54.6% ，实际上我在测试时跑出的最高准确率为 56% ，再调整了一下参数，效果反而差了一点(；′⌒`) 。 two_layer_net 小结 ： 对于神经网络，还有很多神奇的操作现在还不明白，但是很明显它在这种复杂问题上的分类能力是要强于其它的分类方法的，起码在这里比 SVM、Softmax 强 20% 。 features :这一部分使用了原始图像的方向梯度直方图(HOG,Histogram of Oriented Gradient)的特征来进行训练，具体它是怎么完成的现在并不关心，总之它提取了特征，不再使用图像的原始特征，然后能够提高算法的准确率。 特征与数据预处理 ： 1234567891011121314151617181920212223num_color_bins = 10 # Number of bins in the color histogramfeature_fns = [hog_feature, lambda img: color_histogram_hsv(img, nbin=num_color_bins)]X_train_feats = extract_features(X_train, feature_fns, verbose=True)X_val_feats = extract_features(X_val, feature_fns)X_test_feats = extract_features(X_test, feature_fns)# Preprocessing: Subtract the mean featuremean_feat = np.mean(X_train_feats, axis=0, keepdims=True)X_train_feats -= mean_featX_val_feats -= mean_featX_test_feats -= mean_feat# Preprocessing: Divide by standard deviation. This ensures that each feature# has roughly the same scale.std_feat = np.std(X_train_feats, axis=0, keepdims=True)X_train_feats /= std_featX_val_feats /= std_featX_test_feats /= std_feat# Preprocessing: Add a bias dimensionX_train_feats = np.hstack([X_train_feats, np.ones((X_train_feats.shape[0], 1))])X_val_feats = np.hstack([X_val_feats, np.ones((X_val_feats.shape[0], 1))])X_test_feats = np.hstack([X_test_feats, np.ones((X_test_feats.shape[0], 1))]) 上面的过程完成了以下操作： 特征提取 零均值化 单位化方差 加入偏置 此时数据的维度发生了很大的变化， 1print(X_train_feats.shape) (49000, 155) 数据直接由 3072 维，变到了 154 维（不考虑偏置维），算法复杂度也会显著降低。 训练SVM ： 12345678910111213141516171819# cross-validationlearning_rates = [1e-9, 1e-8, 1e-7]regularization_strengths = [5e4, 5e5, 5e6]num_iters = 3000for lr in learning_rates: for rs in regularization_strengths: svm = LinearSVM() svm.train(X_train_feats, y_train, lr, rs, num_iters) y_train_pred = svm.predict(X_train_feats) y_val_pred = svm.predict(X_val_feats) ytr_acc = np.mean(y_train == y_train_pred) yval_acc = np.mean(y_val == y_val_pred) results[(lr,rs)] = (ytr_acc, yval_acc) if yval_acc &gt; best_val : best_val = yval_acc best_svm = svm print('best validation accuracy achieved during cross-validation: %f' % best_val) best validation accuracy achieved during cross-validation: 0.425000 1234# Evaluate your trained SVM on the test sety_test_pred = best_svm.predict(X_test_feats)test_accuracy = np.mean(y_test == y_test_pred)print(test_accuracy) 0.424 得到测试集准确率为 42.4%，比之前提升了 5% 左右。 来观察一下它的误分类的长相， 恩，没什么规律可言O__O “… 训练神经网络 ： 1234567891011121314151617181920212223# cross-validationhidden_dim = 400hidden_size = [ hidden_dim, ] learning_rate = [0.2, 0.25, 0.30 ]reg = [ 0.00005, 0.00001, 0.000005 ]for hs in hidden_size: for lr in learning_rate: for re in reg: net = TwoLayerNet(input_dim, hs, num_classes) # Train the network stats = net.train(X_train_feats, y_train, X_val_feats, y_val, num_iters=4000, batch_size=400, learning_rate=lr, learning_rate_decay=0.98, reg=re, verbose=False) # Predict on the validation set val_acc = (net.predict(X_val_feats) == y_val).mean() print('Validation accuracy: ', val_acc, ' hs = ', hs, ' lr = ', lr, ' re = ', re) if val_acc &gt; best_val: best_val = val_acc best_net = net Validation accuracy: 0.63 hs = 400 lr = 0.2 re = 5e-05Validation accuracy: 0.587 hs = 400 lr = 0.2 re = 1e-05Validation accuracy: 0.58 hs = 400 lr = 0.2 re = 5e-06Validation accuracy: 0.583 hs = 400 lr = 0.25 re = 5e-05Validation accuracy: 0.582 hs = 400 lr = 0.25 re = 1e-05Validation accuracy: 0.587 hs = 400 lr = 0.25 re = 5e-06Validation accuracy: 0.593 hs = 400 lr = 0.3 re = 5e-05Validation accuracy: 0.576 hs = 400 lr = 0.3 re = 1e-05Validation accuracy: 0.588 hs = 400 lr = 0.3 re = 5e-06 12test_acc = (net.predict(X_test_feats) == y_test).mean()print(test_acc) 0.592 这里得到测试集准确率为 59.2% ，比之前提高了 4% ，虽然提升不多，但是模型复杂度也降低了，所以训练速度比之前快了很多。 特征提取小结： 在没有广泛应用神经网络的时期，特征提取和数据的预处理是机器学习中非常重要的一部分，甚至是主要的一部分，神经网络的广泛应用，其实就将特征提取这个部分给简单化了，模型只带特征提取的能力。 具体的很多特征提取的方法目前还不知道，随着课程推进再慢慢学习。 第一部分的笔记就先到这里，这算是我的第一次笔记！！！]]></content>
      <categories>
        <category>cs231n</category>
      </categories>
      <tags>
        <tag>study</tag>
        <tag>mechine learning</tag>
        <tag>homework</tag>
      </tags>
  </entry>
</search>
